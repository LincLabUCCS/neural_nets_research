{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/oliviawatkins/Documents/Schoolwork/NN/neural_nets_research\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets_library import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    Contains the two learnable parts of the model in four independent, fully connected layers.\n",
    "    First the initial values for the registers and instruction registers and second the \n",
    "    parameters that computes the required distributions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 first_arg = None, \n",
    "                 second_arg = None, \n",
    "                 output = None, \n",
    "                 instruction = None, \n",
    "                 initial_registers = None, \n",
    "                 stop_threshold = .99, \n",
    "                 blur = 3, \n",
    "                 correctness_weight = .25, \n",
    "                 halting_weight = .25, \n",
    "                 confidence_weight = .25, \n",
    "                 efficiency_weight = .25,\n",
    "                 t_max = 100):\n",
    "        #TODO: Read over ANC paper, check if there are more reasonable default initial values.\n",
    "        \"\"\"\n",
    "        Initialize a bunch of constants and pass in matrices defining a program.\n",
    "        \n",
    "        :param first_arg: Matrix with the 1st register argument for each timestep stored in the columns (1xRxM)\n",
    "        :param second_arg: Matrix with the 2nd register argument for each timestep stored in the columns (1xRxM)\n",
    "        :param output: Matrix with the output register for each timestep stored in the columns (1xRxM)\n",
    "        :param instruction: Matrix with the instruction for each timestep stored in the columns (1xNxM)\n",
    "        :param initial_registers: Matrix where each row is a distribution over the value in one register (1xRxM)\n",
    "        :param stop_threshold: The stop probability threshold at which the controller should stop running\n",
    "        :param blur: The factor our one-hot vectors are be multiplied by before they're softmaxed to add blur\n",
    "        :param correctness_weight: Weight given to the correctness component of the loss function\n",
    "        :param halting_weight: Weight given to the halting component of the loss function\n",
    "        :param confidence_weight: Weight given to the confidence component of the loss function\n",
    "        :param efficiency_weight: Weight given to the efficiency component of the loss function\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Controller, self).__init__()\n",
    "        \n",
    "        # Initialize dimension constants\n",
    "        B, R, M = initial_registers.size()\n",
    "        self.M = M\n",
    "        self.R = R\n",
    "        \n",
    "        # Initialize loss function weights\n",
    "        # In the ANC paper, these scalars are called, alpha, beta, gamma, and delta\n",
    "        self.correctness_weight = correctness_weight\n",
    "        self.halting_weight = halting_weight\n",
    "        self.confidence_weight = confidence_weight\n",
    "        self.efficiency_weight = efficiency_weight\n",
    "        \n",
    "        # And yet more initialized constants... yeah, there are a bunch, I know.\n",
    "        self.t_max = t_max\n",
    "        self.blur_factor = blur\n",
    "        self.stop_threshold = stop_threshold\n",
    "        \n",
    "        # Blur matrices - i.e. give each different operation/argument/register value some nonzero probability\n",
    "        if blur is not None:\n",
    "            first_arg = self.blur(first_arg, blur, 1)\n",
    "            second_arg = self.blur(second_arg, blur, 1)\n",
    "            output = self.blur(output, blur, 1)\n",
    "            instruction = self.blur(instruction, blur, 1)\n",
    "            initial_registers = self.blur(initial_registers, blur, 2)\n",
    "            \n",
    "            # Initialize parameters.  These are the things that are going to be optimized. \n",
    "            self.first_arg = nn.Parameter(first_arg.data)\n",
    "            self.second_arg = nn.Parameter(second_arg.data)\n",
    "            self.output = nn.Parameter(output.data)\n",
    "            self.instruction = nn.Parameter(instruction.data) \n",
    "            self.registers = nn.Parameter(initial_registers.data)\n",
    "        else:\n",
    "            # Initialize parameters.  These are the things that are going to be optimized. \n",
    "            self.first_arg = nn.Parameter(first_arg)\n",
    "            self.second_arg = nn.Parameter(second_arg)\n",
    "            self.output = nn.Parameter(output)\n",
    "            self.instruction = nn.Parameter(instruction) \n",
    "            self.registers = nn.Parameter(initial_registers)\n",
    "        \n",
    "        # Machine initialization\n",
    "        self.machine = Machine(B, M, R)\n",
    "    \n",
    "    def blur(self, matrix, scale_factor, dimension):\n",
    "        \"\"\"\n",
    "        Takes a matrix, each row (or column) of which is a one-hot vector.\n",
    "        Multiply each 1 by a constant and then softmax it, which \n",
    "        effectively \"blurs\" the matrix a little bit.\n",
    "        \n",
    "        :param matrix: Matrix to blur\n",
    "        :param scale_factor: Constant to multiply the matrix by before it's softmaxed\n",
    "        :param dimension: Dimension to softmax over\n",
    "        \n",
    "        :return: Blurred matrix\n",
    "        \"\"\"\n",
    "        matrix = scale_factor * matrix\n",
    "        softmax = nn.Softmax(dimension)\n",
    "        return softmax(Variable(matrix))    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def forward(self, input, train):\n",
    "        \"\"\"\n",
    "        Runs the controller on a certain input memory matrix.\n",
    "        It either returns the loss or the output memory.\n",
    "        \n",
    "        :param input: A three-tuple of three MxM matrices: (memory matrix, output_memory, output_mask)\n",
    "        \n",
    "        :return: If train is true, return the loss. Otherwise, return the output matrix\n",
    "        \"\"\"\n",
    "        # Program's initial memory\n",
    "        self.memory = input[0]\n",
    "        # Desired output memory\n",
    "        self.output_memory = Variable(input[1])\n",
    "        # Mask with 1's in the rows of the output memory matrix which actually contain the answer.\n",
    "        self.output_mask = Variable(input[2])\n",
    "        \n",
    "#         print(\"STARTING MEM\", self.memory)\n",
    "        \n",
    "        # Initialize instruction regiser (1xMx1)\n",
    "        self.register_buffer('IR', torch.zeros(1, M, 1))\n",
    "        self.IR[0, 0, 0] = 1\n",
    "        \n",
    "        # Blur memory and instruction register\n",
    "        if self.blur_factor is not None:\n",
    "            self.memory = self.blur(self.memory, self.blur_factor, 2) \n",
    "            IR = self.blur(self.IR, self.blur_factor, 1)\n",
    "        else:\n",
    "            IR = Variable(self.IR)\n",
    "            self.memory = Variable(self.memory)\n",
    "        \n",
    "        efficiency_loss = 0\n",
    "        confidence_loss = 0\n",
    "        self.stop_probability = 0\n",
    "        \n",
    "        # Copy registers so we aren't using the values from the previous iteration.\n",
    "        registers = self.registers\n",
    "        \n",
    "        # loss initialization\n",
    "        self.confidence = 0\n",
    "        self.efficiency = 0\n",
    "        self.halting = 0\n",
    "        self.correctness = 0\n",
    "        \n",
    "        print(\"MEM\", self.memory)\n",
    "        \n",
    "        t = 0 \n",
    "        # Run the program, one timestep at a time, until the program terminates or whe time out\n",
    "        while t < self.t_max and self.stop_probability < self.stop_threshold: \n",
    "            a = torch.bmm(self.first_arg, IR)\n",
    "            b = torch.bmm(self.second_arg, IR)\n",
    "            o = torch.bmm(self.output, IR)\n",
    "            e = torch.bmm(self.instruction, IR)\n",
    "            \n",
    "            print(\"OUTPUT\", self.output)\n",
    "            print(\"IR\", IR)\n",
    "            \n",
    "            # Update memory, registers, and IR after machine operation\n",
    "            self.old_stop_probability = self.stop_probability\n",
    "            self.memory, registers, IR, new_stop_prob = self.machine(e, a, b, o, self.memory, registers, IR) \n",
    "            print(\"MEM\", self.memory)\n",
    "            self.stop_probability += new_stop_prob[0]\n",
    "            \n",
    "            # If we're training, calculate loss\n",
    "            if train:\n",
    "                self.timestep_loss(t)\n",
    "            \n",
    "            t += 1\n",
    "            \n",
    "        print(\"DESIRED OUTPUT\", self.output_memory)\n",
    "        print(\"FINAL MEMORY\", self.memory)\n",
    "        \n",
    "        # If we're training, return loss.  Otherwise return memory.\n",
    "        if train:\n",
    "            self.final_loss(t)\n",
    "            total_loss  = self.total_loss()\n",
    "            print(\"LOSS COMPONENTS\", self.correctness, self.efficiency, self.correctness, self.halting)\n",
    "            return (self.memory, total_loss)\n",
    "        else:\n",
    "            return (self.memory, None)\n",
    "        \n",
    "        \n",
    "    def timestep_loss(self, t):\n",
    "        \"\"\"\n",
    "        @ Rakia @ Aditya feel free to use this function definition or not.  My main thought was that this would \n",
    "        compute the types of loss which get updated every timestep.\n",
    "        \"\"\"\n",
    "        # Confidence Loss \n",
    "        mem_diff = self.output_memory - self.memory\n",
    "        correctness = torch.sum((self.output_mask) * mem_diff * mem_diff)\n",
    "        self.confidence += (self.stop_probability - self.old_stop_probability) * correctness\n",
    "        \n",
    "        # Efficiency Loss\n",
    "        if t < self.t_max and self.stop_probability < self.stop_threshold: # don't add efficiency loss on the last timestep\n",
    "            self.efficiency += (1 - self.stop_probability)\n",
    "    \n",
    "    def final_loss(self, t):\n",
    "        \"\"\"\n",
    "        @ Rakia @ Aditya feel free to use this function definition or not.  My main thought was that this would \n",
    "        compute the types of loss which get updated every timestep.\n",
    "        \"\"\"\n",
    "        # Correctness loss\n",
    "        mem_diff = self.output_memory - self.memory\n",
    "        self.correctness = torch.sum((self.output_mask) * mem_diff * mem_diff)\n",
    "\n",
    "        # Halting loss\n",
    "        if t == self.t_max:\n",
    "            self.halting = (1 - self.stop_probability)\n",
    "   \n",
    "\n",
    "    def total_loss(self):\n",
    "        \"\"\" compute four diferent loss functions and return a weighted average of the four measuring correctness, \n",
    "        halting, efficiency, and confidence\"\"\"\n",
    "        return  (self.correctness*self.correctness_weight) + (self.confidence_weight*self.confidence) + (self.halting_weight*self.halting) + (self.efficiency_weight*self.efficiency) \n",
    "        \n",
    "      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(nn.Module):\n",
    "    \"\"\"\n",
    "    Parent class for our binary operations\n",
    "    \"\"\"\n",
    "    def __init__(self, M):\n",
    "        \"\"\"\n",
    "        Initialize the memory length (needed so we can mod our answer in case it exceeds the range 0-M-1)\n",
    "        Also calculate the output matrix for the operation\n",
    "        \n",
    "        :param M: Memory length\n",
    "        \"\"\"\n",
    "        super(Operation, self).__init__()\n",
    "        self.M = M\n",
    "        \n",
    "        # Create a MxMxM matrix where the (i,j,k) cell is 1 iff operation(i,j) = k.\n",
    "        self.outputs = torch.zeros(M, M, M)\n",
    "        for i in range(M):\n",
    "            for j in range(M):\n",
    "                val = self.compute(i, j)\n",
    "                self.outputs[val][i][j] = 1\n",
    "                \n",
    "        self.outputs = Variable(self.outputs)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        \"\"\" \n",
    "        Perform the binary operation.  The arguments may or may not be used.\n",
    "        \n",
    "        :param x: First argument\n",
    "        :param y: Second argument\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        :return: The output matrix\n",
    "        \"\"\"\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Operation):\n",
    "\n",
    "    def __init__(self, M):\n",
    "        super(Add, self).__init__(M)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        return (x + y) % self.M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stop(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Stop, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jump(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Jump, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0 # Actual jump happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decrement(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Decrement, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x - 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Increment(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Increment, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x + 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Max, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return max(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Min(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Min, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return min(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Read, self).__init__(M)\n",
    "        # Leave output matrix blank since we're gonna do the reading elsewhere\n",
    "        self.outputs = torch.zeros(M, M, M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return 0 # Actual reading happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtract(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Subtract, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return (x - y) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Write(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Write, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return 0 # Actual write happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zero(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Zero, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine(nn.Module):\n",
    "    \"\"\"\n",
    "    The Machine executes assembly instructions passed to it by the Controller.\n",
    "    It updates the given memory, registers, and instruction pointer.\n",
    "    The Machine doesn't have any learnable parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, B, M, R):\n",
    "        \"\"\"\n",
    "        Initializes dimensions, operations, and counters\n",
    "        \n",
    "        :param B: Batch size (meant to be 1)\n",
    "        :param M: Memory length.  Integer values also take on values 0-M-1.  M is also the program length.\n",
    "        :param R: Number of registers\n",
    "        \"\"\"\n",
    "        super(Machine, self).__init__()\n",
    "        \n",
    "        # Store parameters as class variables\n",
    "        self.R = R # Number of registers\n",
    "        self.M = M # Memory length (also largest number)\n",
    "        self.B = B # Batch size\n",
    "        \n",
    "        # Start off with 0 probability of stopping\n",
    "        self.stop_probability = torch.zeros(B)\n",
    "        \n",
    "        # List of ops (must be in same order as the original ANC paper so compilation works right)\n",
    "        self.ops = [ \n",
    "            Stop(M),\n",
    "            Zero(M),\n",
    "            Increment(M),\n",
    "            Add(M),\n",
    "            Subtract(M),\n",
    "            Decrement(M),\n",
    "            Min(M),\n",
    "            Max(M),\n",
    "            Read(M),\n",
    "            Write(M),\n",
    "            Jump(M)\n",
    "        ]\n",
    "        \n",
    "        # Number of instructions\n",
    "        self.N = len(self.ops)\n",
    "        \n",
    "        # Create a 4D matrix composed of the output matrices of each of the ops\n",
    "        self.outputs = Variable(torch.zeros(self.N, M, M, M))\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            op = self.ops[i]\n",
    "            self.outputs[i] = op()\n",
    "            \n",
    "        # Add an extra batch dimension\n",
    "        self.outputs = torch.unsqueeze(self.outputs, 0)\n",
    "        self.outputs = self.outputs.expand(B, -1, -1, -1, -1)\n",
    "        \n",
    "        # Keep track of ops which will be handled specially\n",
    "        self.jump_index = 10\n",
    "        self.stop_index = 0\n",
    "        self.write_index = 9\n",
    "        self.read_index = 8\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, e, a, b, o, memory, registers, IR):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run the Machine for one timestep (corresponding to the execution of one line of Assembly).\n",
    "        The first four parameter names correspond to the vector names used in the original ANC paper\n",
    "        \n",
    "        :param e: Probability distribution over the instruction being executed (BxMx1)\n",
    "        :param a: Probability distribution over the first argument register (length BxRx1)\n",
    "        :param b: Probability distribution over the second argument register (length BxRx1)\n",
    "        :param o: Probability distribution over the first argument register (length BxRx1)\n",
    "        :param memory: Memory matrix (size BxMxM)\n",
    "        :param registers: Register matrix (size BxRxM)\n",
    "        :param IR: Instruction Register (length BxM)\n",
    "        \n",
    "        :return: The memory, registers, and instruction register after the timestep\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO: Get rid of all of this\n",
    "        op_names = [ \n",
    "            \"Stop(M)\",\n",
    "            \"Zero(M)\",\n",
    "            \"Increment(M\",\n",
    "            \"Add(M)\",\n",
    "            \"Subtract(M\",\n",
    "            \"Decrement(M)\",\n",
    "            \"Min(M)\",\n",
    "            \"Max(M)\",\n",
    "            \"Read(M)\",\n",
    "            \"Write(M)\",\n",
    "            \"Jump(M)\"\n",
    "        ]\n",
    "        e_temp = e\n",
    "        e_temp = e_temp.squeeze()\n",
    "        num, index = torch.max(e_temp, 0)\n",
    "        print(\"RUNNING OP\", op_names[index.data[0]], \" WITH PROBABILITY\", num)\n",
    "        \n",
    "        # Dimensions B x 1 x R -> B x 1 x R\n",
    "        a = torch.transpose(a, 1, 2)\n",
    "        b = torch.transpose(b, 1, 2)\n",
    "        \n",
    "        # Calculate distributions over the two argument values by multiplying each \n",
    "        # register by the probability that register is being used.\n",
    "        arg1 = torch.bmm(a, registers)\n",
    "        arg2 = torch.bmm(b, registers)\n",
    "        \n",
    "        # Multiply the output matrix by the arg1 and arg2 vectors to take into account\n",
    "        # Before we do this, we're going to have to do a bunch of dimension squishing.\n",
    "        \n",
    "        # arg1_long dimensions: B x 1 x M --> B x 1 x 1 x 1 x M\n",
    "        arg1_long = torch.unsqueeze(arg1, 1)\n",
    "        arg1_long = torch.unsqueeze(arg1_long, 1)\n",
    "        \n",
    "        outputs_x_arg1 = torch.matmul(arg1_long, self.outputs)\n",
    "        \n",
    "        # outputs_x_arg1 dimensions: B x N x M x 1 x M -> B x N x M x M\n",
    "        outputs_x_arg1 = torch.squeeze(outputs_x_arg1, 3)\n",
    "        \n",
    "        # arg2_long dimensions: B x 1 x M --> B x 1 x M x 1\n",
    "        arg2_long = torch.unsqueeze(arg2, 3)\n",
    "        \n",
    "        outputs_x_args = torch.matmul(outputs_x_arg1, arg2_long)\n",
    "        \n",
    "        # outputs_x_args dimensions: B x N x M x 1 -> B x N x M\n",
    "        outputs_x_args = torch.squeeze(outputs_x_args, 3)\n",
    "        \n",
    "        # e dimensions B x N x 1 -> B x 1 x N\n",
    "        e = torch.transpose(e, 1, 2)\n",
    "        \n",
    "        # read_vec dimensions B x 1 -> B x 1 x 1\n",
    "        read_vec =  e[:, :, self.read_index]\n",
    "        read_vec = read_vec.unsqueeze(1)\n",
    "        \n",
    "        # Length Bx1xM vector over the output of the operation\n",
    "        out_vec = torch.matmul(e, outputs_x_args)\n",
    "        \n",
    "        # Deal with memory reads separately\n",
    "        out_vec = out_vec + read_vec * torch.matmul(arg1, memory)        \n",
    "        \n",
    "        # Update our memory, registers, instruction register, and stopping probability\n",
    "        memory = self.writeMemory(e, o, memory, arg1, arg2)\n",
    "        registers = self.writeRegisters(out_vec, o, registers)\n",
    "        IR = self.updateIR(e, IR, arg1, arg2)\n",
    "        stop_prob = self.getStop(e)\n",
    "        \n",
    "        return(memory, registers, IR, stop_prob)\n",
    "        \n",
    "        \n",
    "    def writeRegisters(self, out, o, registers):\n",
    "        \"\"\"\n",
    "        Write the result of our operation to our registers.\n",
    "        \n",
    "        :param out: Probability distribution over the output value (Bx1xM)\n",
    "        :param o: Probability distribution over the output register (BxRx1)\n",
    "        :param Registers: register matrix (BxRxM)\n",
    "        \n",
    "        :return: The updated registers (BxRxM)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Multiply probability of writing to each output register by the distribution over the value we're writing there.\n",
    "        new_register_vals = torch.matmul(o, out)\n",
    "        \n",
    "        # Multiply each original register cell by the probabilty of not writing to that register\n",
    "        old_register_vals = (1-o).expand(self.B, self.R, self.M) * registers\n",
    "        \n",
    "        # Take a weighted sum over the old and new register values\n",
    "        registers =  new_register_vals + old_register_vals\n",
    "        \n",
    "        return registers\n",
    "    \n",
    "    def updateIR(self, e, IR, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the instruction register\n",
    "        \n",
    "        :param e: Distribution over the current instruction (BxNx1)\n",
    "        :param IR: Instruction register (length BxMx1)\n",
    "        :param arg1: Distribution over the first argument value (length BxMx1)\n",
    "        :param arg2: Distribution over the second argument value (length BxMx1)\n",
    "        \n",
    "        :return: The updated instruction register (BxMx1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dimensions B x 1 x M -> B x M x 1\n",
    "        arg2 = arg2.transpose(1, 2)\n",
    "        \n",
    "        # Probability that we're on the jump instruction\n",
    "        jump_probability = e[:, :, self.jump_index]\n",
    "        \n",
    "        # Probability that the first argument is 0\n",
    "        is_zero = arg1[:, :, 0]\n",
    "        \n",
    "        # Slicing lost a dimension.  Let's add it back\n",
    "        jump_probability = torch.unsqueeze(jump_probability, 1)\n",
    "        is_zero = torch.unsqueeze(is_zero, 1)\n",
    "        \n",
    "        # If we're not jumping, just shift IR by one slot\n",
    "        wraparound = IR[:, -1]\n",
    "        normal_instructions = IR[:, :-1]\n",
    "        \n",
    "        # For whatever reason, when you chop off one row/column, that dimension disappears.  Add it back.\n",
    "        wraparound = wraparound.unsqueeze(1)\n",
    "        IR_no_jump = torch.cat([wraparound, normal_instructions], 1)\n",
    "        \n",
    "        # If we are on a jump instruction, check whether the argument's 0.\n",
    "        # If it is, jump to the location specified by arg2.  Otherwise, increment like normal.\n",
    "        IR_jump = arg2 * is_zero + (1 - is_zero) * IR_no_jump\n",
    "        \n",
    "        # Take a weighted sum of the instruction register with and without jumping\n",
    "        IR = IR_no_jump * (1 - jump_probability) + IR_jump * jump_probability\n",
    "        \n",
    "        return IR\n",
    "    \n",
    "    def writeMemory(self, e, o, mem_orig, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the memory\n",
    "        \n",
    "        :param e: Distribution over the current instruction (B x1xM)\n",
    "        :param mem_orig: Current memory matrix (BxMxM)\n",
    "        :param arg1: Distribution over the first argument value (Bx1xM)\n",
    "        :param arg2: Distribution over the second argument value (Bx1xM)\n",
    "        \n",
    "        :return: The updated memory matrix (BxMxM)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Probability that we're on the write instruction\n",
    "        write_probability = e[:,:, self.write_index]\n",
    "        \n",
    "        # write_probability dimensions: Bx1 -> B x 1 x 1\n",
    "        write_probability = torch.unsqueeze(write_probability, 1)\n",
    "        \n",
    "        # arg1 dimensions: B x 1 x M -> B x M x 1\n",
    "        arg1 = torch.transpose(arg1, 1, 2)\n",
    "        \n",
    "        # If we are on a write instruction, write the value arg2 in register arg1. Otherwise, leave memory as is.\n",
    "        mem_changed = torch.bmm(arg1, arg2)\n",
    "        mem_unchanged = mem_orig * (1-arg1).expand(-1, -1, self.M)\n",
    "        mem_write = mem_changed + mem_unchanged\n",
    "        \n",
    "        # Take a weighted sum over the new memory and old memory\n",
    "        memory = mem_orig * (1 - write_probability) + mem_write * write_probability\n",
    "\n",
    "        return memory\n",
    "        \n",
    "    def getStop(self, e):\n",
    "        \"\"\"\n",
    "        Obtain the probability that we will stop at this timestep based on the probability that we are running the STOP op.\n",
    "        \n",
    "        :param e: distribution over the current instruction (length Bx1xM)\n",
    "        \n",
    "        :return: probability representing whether the controller should stop.\n",
    "        \"\"\"\n",
    "        return e[:, :, self.stop_index].data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hotify(vec, length, dimension):\n",
    "    \"\"\"\n",
    "    Turn a tensor of integers into a matrix of one-hot vectors.\n",
    "    \n",
    "    :param vec: The vector to be converted.\n",
    "    :param length: One dimension of the matrix (the other is the length of vec)\n",
    "    :param dimension: Which dimension stores the elements of vec.  If 0, they're stored in the rows.  If 1, the columns.\n",
    "    \n",
    "    :return A matrix of one-hot vectors, each row or column corresponding to one element of vec\n",
    "    \"\"\"\n",
    "    x = vec.size()[0]\n",
    "    if dimension == 0:\n",
    "        binary_vec = torch.zeros(x, length)\n",
    "        for i in range(x):\n",
    "            binary_vec[i][vec[i]] = 1\n",
    "        return binary_vec\n",
    "    elif dimension == 1:\n",
    "        binary_vec = torch.zeros(length, x)\n",
    "        for i in range(x):\n",
    "            binary_vec[vec[i]][i] = 1\n",
    "        return binary_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition task\n",
    "# Generate this by running the instructions here (but with the addition program file): https://github.com/aditya-khant/neural-assembly-compiler\n",
    "# Then get rid of the .cuda in each of the tensors since we (or at least I) don't have cuda\n",
    "init_registers = torch.IntTensor([6,2,0,1,0,0]) # Length R, should be RxM\n",
    "first_arg = torch.IntTensor([4,3,3,3,4,2,2,5]) # Length M, should be RxM\n",
    "second_arg = torch.IntTensor([5,5,0,5,5,1,4,5]) # Length M, should be RxM\n",
    "target = torch.IntTensor([4,3,5,3,4,5,5,5]) # Length M, should be RxM\n",
    "instruction = torch.IntTensor([8,8,10,5,2,10,9,0]) # Length M, should be NxM\n",
    "\n",
    "# Get dimensions we'll need\n",
    "M = first_arg.size()[0]\n",
    "R = init_registers.size()[0]\n",
    "N = 11\n",
    "\n",
    "# Turn the given tensors into matrices of one-hot vectors.\n",
    "init_registers = one_hotify(init_registers, M, 0)\n",
    "first_arg = one_hotify(first_arg, R, 1)\n",
    "second_arg = one_hotify(second_arg, R, 1)\n",
    "target = one_hotify(target, R, 1)\n",
    "instruction = one_hotify(instruction, N, 1)\n",
    "\n",
    "# Add a fake first batch\n",
    "init_registers = init_registers.unsqueeze(0)\n",
    "first_arg = first_arg.unsqueeze(0)\n",
    "second_arg = second_arg.unsqueeze(0)\n",
    "target = target.unsqueeze(0)\n",
    "instruction = instruction.unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# An example starting memory.  In this case, we're adding 3+4 and expect 7.\n",
    "initial_memory = torch.IntTensor([3,4,0,0,0,0,0,0])\n",
    "initial_memory = one_hotify(initial_memory, M, 0)\n",
    "output_memory = torch.zeros(M, M)\n",
    "output_memory[0][7] = 1\n",
    "\n",
    "# Output mask has ones in the rows of the memory matrix where the answer will be stored.\n",
    "output_mask = torch.zeros(M, M)\n",
    "output_mask[0] = torch.ones(M)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the addition task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            # An example starting memory.  In this case, we're adding 3+4 and expect 7.\n",
    "            first_addend = random.randint(0, M-1)\n",
    "            second_addend = random.randint(0, M-1)\n",
    "            initial_memory = torch.zeros(M, M)\n",
    "#             initial_memory[0][first_addend] = 1\n",
    "#             initial_memory[1][second_addend] = 1\n",
    "            initial_memory[0][3] = 1 #TODO: Fix this!\n",
    "            initial_memory[1][3] = 1\n",
    "\n",
    "            \n",
    "            output_memory = torch.zeros(M, M)\n",
    "#             output_memory[0][(first_addend + second_addend) % M] = 1\n",
    "            output_memory[0][6] = 1\n",
    "\n",
    "            # Output mask has ones in the rows of the memory matrix where the answer will be stored.\n",
    "            output_mask = torch.zeros(M, M)\n",
    "            output_mask[0] = torch.ones(M)\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "LR is set to 0.001\n",
      "MEM Variable containing:\n",
      "(0 ,.,.) = \n",
      "   0   0   0   1   0   0   0   0\n",
      "   0   0   0   1   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "OUTPUT Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0096 -0.0056  0.0106  0.0185  0.0021 -0.0008 -0.0322 -0.0103\n",
      " -0.0188 -0.0175  0.0121 -0.0081 -0.0107  0.0135 -0.0162  0.0173\n",
      " -0.0155 -0.0070  0.0069 -0.0184 -0.0043 -0.0013 -0.0144 -0.0367\n",
      "  0.0107  1.0132 -0.0163  0.9737  0.0309 -0.0256  0.0297 -0.0213\n",
      "  0.9585  0.0103  0.0249  0.0105  0.9473  0.0144 -0.0243  0.0463\n",
      "  0.0028  0.0425  0.9760  0.0434  0.0484  1.0033  0.9854  0.9431\n",
      "[torch.FloatTensor of size 1x6x8]\n",
      "\n",
      "IR Variable containing:\n",
      "(0 ,.,.) = \n",
      "   1\n",
      "   0\n",
      "   0\n",
      "   0\n",
      "   0\n",
      "   0\n",
      "   0\n",
      "   0\n",
      "[torch.FloatTensor of size 1x8x1]\n",
      "\n",
      "RUNNING OP Read(M)  WITH PROBABILITY Variable containing:\n",
      " 0.9836\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "MEM Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 5 \n",
      "  -3.2986e-03 -3.7983e-05  4.7256e-05  1.0034e+00  1.1528e-04  8.2191e-05\n",
      "  1.7322e-05  1.9947e-07 -2.4816e-07  9.9998e-01 -6.0537e-07 -4.3162e-07\n",
      "  5.1353e-05  5.9133e-07 -7.3569e-07 -1.8322e-06 -1.7947e-06 -1.2796e-06\n",
      "  9.2814e-06  1.0688e-07 -1.3297e-07 -3.3114e-07 -3.2436e-07 -2.3127e-07\n",
      "  5.1690e-06  5.9522e-08 -7.4052e-08 -1.8442e-07 -1.8065e-07 -1.2880e-07\n",
      "  3.6551e-05  4.2089e-07 -5.2363e-07 -1.3040e-06 -1.2774e-06 -9.1074e-07\n",
      "  7.6064e-05  8.7588e-07 -1.0897e-06 -2.7138e-06 -2.6583e-06 -1.8953e-06\n",
      "  2.1842e-05  2.5152e-07 -3.1292e-07 -7.7929e-07 -7.6335e-07 -5.4425e-07\n",
      "\n",
      "Columns 6 to 7 \n",
      "   1.2740e-04  8.0515e-05\n",
      " -6.6901e-07 -4.2281e-07\n",
      " -1.9833e-06 -1.2535e-06\n",
      " -3.5846e-07 -2.2655e-07\n",
      " -1.9963e-07 -1.2617e-07\n",
      " -1.4116e-06 -8.9216e-07\n",
      " -2.9377e-06 -1.8566e-06\n",
      " -8.4359e-07 -5.3315e-07\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "OUTPUT Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0096 -0.0056  0.0106  0.0185  0.0021 -0.0008 -0.0322 -0.0103\n",
      " -0.0188 -0.0175  0.0121 -0.0081 -0.0107  0.0135 -0.0162  0.0173\n",
      " -0.0155 -0.0070  0.0069 -0.0184 -0.0043 -0.0013 -0.0144 -0.0367\n",
      "  0.0107  1.0132 -0.0163  0.9737  0.0309 -0.0256  0.0297 -0.0213\n",
      "  0.9585  0.0103  0.0249  0.0105  0.9473  0.0144 -0.0243  0.0463\n",
      "  0.0028  0.0425  0.9760  0.0434  0.0484  1.0033  0.9854  0.9431\n",
      "[torch.FloatTensor of size 1x6x8]\n",
      "\n",
      "IR Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0210\n",
      "  1.0209\n",
      "  0.0003\n",
      "  0.0007\n",
      "  0.0007\n",
      "  0.0005\n",
      "  0.0008\n",
      "  0.0005\n",
      "[torch.FloatTensor of size 1x8x1]\n",
      "\n",
      "RUNNING OP Read(M)  WITH PROBABILITY Variable containing:\n",
      " 0.9880\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "MEM Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 5 \n",
      "  -3.1522e-03 -4.5154e-05  3.6023e-05  1.0033e+00  1.1032e-04  7.8636e-05\n",
      " -5.5595e-03  2.7472e-04  4.2912e-04  1.0066e+00  1.8822e-04  1.3504e-04\n",
      "  4.1269e-04 -1.7196e-05 -2.8557e-05 -2.1623e-05 -1.4029e-05 -1.0057e-05\n",
      "  3.1768e-04 -1.5074e-05 -2.3877e-05 -1.7222e-05 -1.0766e-05 -7.7225e-06\n",
      "  1.8942e-04 -9.0098e-06 -1.4259e-05 -1.0276e-05 -6.4191e-06 -4.6043e-06\n",
      "  2.2833e-04 -9.0196e-06 -1.5289e-05 -1.1808e-05 -7.7709e-06 -5.5692e-06\n",
      "  3.8100e-04 -1.4136e-05 -2.4568e-05 -1.9416e-05 -1.2983e-05 -9.3024e-06\n",
      "  1.9462e-04 -8.2534e-06 -1.3615e-05 -1.0242e-05 -6.6135e-06 -4.7412e-06\n",
      "\n",
      "Columns 6 to 7 \n",
      "   1.1669e-04  7.7036e-05\n",
      "  4.0813e-04  1.3212e-04\n",
      " -2.8471e-05 -9.8413e-06\n",
      " -2.2965e-05 -7.5561e-06\n",
      " -1.3706e-05 -4.5051e-06\n",
      " -1.5470e-05 -5.4501e-06\n",
      " -2.5291e-05 -9.1039e-06\n",
      " -1.3509e-05 -4.6395e-06\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "OUTPUT Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0096 -0.0056  0.0106  0.0185  0.0021 -0.0008 -0.0322 -0.0103\n",
      " -0.0188 -0.0175  0.0121 -0.0081 -0.0107  0.0135 -0.0162  0.0173\n",
      " -0.0155 -0.0070  0.0069 -0.0184 -0.0043 -0.0013 -0.0144 -0.0367\n",
      "  0.0107  1.0132 -0.0163  0.9737  0.0309 -0.0256  0.0297 -0.0213\n",
      "  0.9585  0.0103  0.0249  0.0105  0.9473  0.0144 -0.0243  0.0463\n",
      "  0.0028  0.0425  0.9760  0.0434  0.0484  1.0033  0.9854  0.9431\n",
      "[torch.FloatTensor of size 1x6x8]\n",
      "\n",
      "IR Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0006\n",
      " -0.0210\n",
      "  1.0208\n",
      "  0.0003\n",
      "  0.0007\n",
      "  0.0007\n",
      "  0.0005\n",
      "  0.0008\n",
      "[torch.FloatTensor of size 1x8x1]\n",
      "\n",
      "RUNNING OP Jump(M)  WITH PROBABILITY Variable containing:\n",
      " 1.0027\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "MEM Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 5 \n",
      "  -3.1549e-03 -6.3184e-05  5.6233e-05  1.0025e+00  1.0616e-04  1.0895e-04\n",
      " -5.5611e-03  2.3613e-04  4.7115e-04  1.0049e+00  1.7930e-04  1.9895e-04\n",
      "  4.1331e-04 -1.5116e-05 -3.0891e-05 -2.2757e-05 -1.3561e-05 -1.3558e-05\n",
      "  4.3065e-04  3.6780e-04 -4.5349e-04 -2.2578e-04  7.5569e-05 -6.5209e-04\n",
      "  1.9008e-04 -6.7099e-06 -1.6838e-05 -1.1528e-05 -5.9002e-06 -8.4737e-06\n",
      "  2.2899e-04 -6.7518e-06 -1.7832e-05 -1.3042e-05 -7.2593e-06 -9.3846e-06\n",
      "  3.8270e-04 -8.4307e-06 -3.0970e-05 -2.2524e-05 -1.1697e-05 -1.8904e-05\n",
      "  1.9543e-04 -5.4527e-06 -1.6756e-05 -1.1767e-05 -5.9816e-06 -9.4529e-06\n",
      "\n",
      "Columns 6 to 7 \n",
      "   8.7240e-04  5.8752e-05\n",
      "  2.0032e-03  9.3428e-05\n",
      " -1.1556e-04 -7.7423e-06\n",
      " -1.6052e-02  3.7881e-04\n",
      " -1.0997e-04 -2.1846e-06\n",
      " -1.1039e-04 -3.1620e-06\n",
      " -2.6412e-04 -3.3476e-06\n",
      " -1.3073e-04 -1.8137e-06\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "OUTPUT Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0096 -0.0056  0.0106  0.0185  0.0021 -0.0008 -0.0322 -0.0103\n",
      " -0.0188 -0.0175  0.0121 -0.0081 -0.0107  0.0135 -0.0162  0.0173\n",
      " -0.0155 -0.0070  0.0069 -0.0184 -0.0043 -0.0013 -0.0144 -0.0367\n",
      "  0.0107  1.0132 -0.0163  0.9737  0.0309 -0.0256  0.0297 -0.0213\n",
      "  0.9585  0.0103  0.0249  0.0105  0.9473  0.0144 -0.0243  0.0463\n",
      "  0.0028  0.0425  0.9760  0.0434  0.0484  1.0033  0.9854  0.9431\n",
      "[torch.FloatTensor of size 1x6x8]\n",
      "\n",
      "IR Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0011\n",
      "  0.0017\n",
      " -0.0232\n",
      "  1.0669\n",
      "  0.0006\n",
      " -0.0010\n",
      " -0.0437\n",
      "  0.0016\n",
      "[torch.FloatTensor of size 1x8x1]\n",
      "\n",
      "RUNNING OP Decrement(M)  WITH PROBABILITY Variable containing:\n",
      " 1.0648\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "MEM Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 5 \n",
      "   7.3826e-03 -2.3834e-04 -1.4423e-04  9.9038e-01  1.5304e-04  5.1147e-05\n",
      "  2.9721e-03  9.1847e-05  3.0534e-04  9.9514e-01  2.1648e-04  1.5141e-04\n",
      "  1.3188e-04 -1.0405e-05 -2.5543e-05 -1.1167e-05 -1.4855e-05 -1.2046e-05\n",
      " -8.3811e-02  1.8139e-03  1.1078e-03  3.2243e-03 -3.0357e-04 -2.5931e-04\n",
      " -2.7248e-04  1.0363e-06 -8.0434e-06  7.5218e-06 -8.0233e-06 -5.9870e-06\n",
      " -3.0658e-04  2.2173e-06 -7.6493e-06  9.0135e-06 -9.7184e-06 -6.5060e-06\n",
      " -1.3784e-03  2.1064e-05  2.4946e-06  4.9995e-05 -1.9793e-05 -9.4549e-06\n",
      " -3.4357e-04  3.5744e-06 -6.5072e-06  1.0430e-05 -8.4556e-06 -6.5560e-06\n",
      "\n",
      "Columns 6 to 7 \n",
      "   5.1485e-04  9.4007e-05\n",
      "  1.7037e-03  1.2158e-04\n",
      " -1.0628e-04 -8.7084e-06\n",
      " -1.4766e-02  1.2579e-04\n",
      " -9.4717e-05 -3.7692e-06\n",
      " -9.2730e-05 -4.9974e-06\n",
      " -2.0635e-04 -9.3840e-06\n",
      " -1.1297e-04 -3.6600e-06\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "OUTPUT Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0096 -0.0056  0.0106  0.0185  0.0021 -0.0008 -0.0322 -0.0103\n",
      " -0.0188 -0.0175  0.0121 -0.0081 -0.0107  0.0135 -0.0162  0.0173\n",
      " -0.0155 -0.0070  0.0069 -0.0184 -0.0043 -0.0013 -0.0144 -0.0367\n",
      "  0.0107  1.0132 -0.0163  0.9737  0.0309 -0.0256  0.0297 -0.0213\n",
      "  0.9585  0.0103  0.0249  0.0105  0.9473  0.0144 -0.0243  0.0463\n",
      "  0.0028  0.0425  0.9760  0.0434  0.0484  1.0033  0.9854  0.9431\n",
      "[torch.FloatTensor of size 1x6x8]\n",
      "\n",
      "IR Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0060\n",
      "  0.0011\n",
      "  0.0016\n",
      " -0.0232\n",
      "  1.0617\n",
      "  0.0005\n",
      " -0.0011\n",
      " -0.0435\n",
      "[torch.FloatTensor of size 1x8x1]\n",
      "\n",
      "RUNNING OP Increment(M  WITH PROBABILITY Variable containing:\n",
      " 1.0346\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "MEM Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 5 \n",
      "   5.1485e-03 -2.0396e-04  6.5102e-05  9.9333e-01  1.4042e-04  6.5347e-05\n",
      "  2.4376e-03  1.0025e-04  3.5544e-04  9.9584e-01  2.1352e-04  1.5486e-04\n",
      "  6.9867e-05 -9.4413e-06 -1.9777e-05 -7.6851e-06 -1.5215e-05 -1.1661e-05\n",
      " -1.1154e-01  2.2612e-03  3.4747e-03  4.7344e-03 -4.5826e-04 -1.1128e-04\n",
      " -2.3043e-04  3.8279e-07 -1.1952e-05  5.1608e-06 -7.7796e-06 -6.2487e-06\n",
      " -2.2653e-04  9.7318e-07 -1.5091e-05  4.5189e-06 -9.2544e-06 -7.0040e-06\n",
      " -8.2710e-04  1.2494e-05 -4.8692e-05  1.9055e-05 -1.6595e-05 -1.2878e-05\n",
      " -5.7858e-04  7.2272e-06  1.5339e-05  2.3625e-05 -9.8174e-06 -5.0939e-06\n",
      "\n",
      "Columns 6 to 7 \n",
      "   6.3265e-04  7.6877e-05\n",
      "  1.7326e-03  1.1752e-04\n",
      " -1.0309e-04 -9.1876e-06\n",
      " -1.3940e-02 -6.3636e-05\n",
      " -9.6881e-05 -3.4447e-06\n",
      " -9.6849e-05 -4.3796e-06\n",
      " -2.3460e-04 -5.1319e-06\n",
      " -1.0089e-04 -5.4733e-06\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "OUTPUT Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0096 -0.0056  0.0106  0.0185  0.0021 -0.0008 -0.0322 -0.0103\n",
      " -0.0188 -0.0175  0.0121 -0.0081 -0.0107  0.0135 -0.0162  0.0173\n",
      " -0.0155 -0.0070  0.0069 -0.0184 -0.0043 -0.0013 -0.0144 -0.0367\n",
      "  0.0107  1.0132 -0.0163  0.9737  0.0309 -0.0256  0.0297 -0.0213\n",
      "  0.9585  0.0103  0.0249  0.0105  0.9473  0.0144 -0.0243  0.0463\n",
      "  0.0028  0.0425  0.9760  0.0434  0.0484  1.0033  0.9854  0.9431\n",
      "[torch.FloatTensor of size 1x6x8]\n",
      "\n",
      "IR Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0425\n",
      "  0.0060\n",
      "  0.0010\n",
      "  0.0015\n",
      " -0.0232\n",
      "  1.0605\n",
      "  0.0005\n",
      " -0.0011\n",
      "[torch.FloatTensor of size 1x8x1]\n",
      "\n",
      "RUNNING OP Jump(M)  WITH PROBABILITY Variable containing:\n",
      " 1.0722\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "MEM Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 5 \n",
      "   5.3977e-03 -5.6305e-04  3.1502e-02  9.6383e-01 -6.6263e-04 -4.5001e-04\n",
      "  2.4372e-03  1.0069e-04  3.1805e-04  9.9588e-01  2.1448e-04  1.5548e-04\n",
      "  6.4704e-05 -4.6867e-06 -4.2963e-04  2.2333e-06 -4.8050e-06 -4.9714e-06\n",
      " -1.1163e-01  2.2727e-03  2.6325e-03  4.7585e-03 -4.3714e-04 -9.7571e-05\n",
      " -2.3330e-04  2.9694e-06 -2.3475e-04  1.0555e-05 -2.1191e-06 -2.6113e-06\n",
      " -2.2778e-04  2.0994e-06 -1.1210e-04  6.8675e-06 -6.7900e-06 -5.4203e-06\n",
      " -8.2973e-04  1.4771e-05 -2.4462e-04  2.3801e-05 -1.1619e-05 -9.6807e-06\n",
      " -5.7621e-04  5.1427e-06  1.9479e-04  1.9278e-05 -1.4376e-05 -8.0238e-06\n",
      "\n",
      "Columns 6 to 7 \n",
      "   1.9156e-03  1.3289e-04\n",
      "  1.7311e-03  1.1745e-04\n",
      " -1.2009e-04 -9.9502e-06\n",
      " -1.3986e-02 -6.5250e-05\n",
      " -1.0612e-04 -3.8580e-06\n",
      " -1.0087e-04 -4.5597e-06\n",
      " -2.4275e-04 -5.4957e-06\n",
      " -9.3441e-05 -5.1400e-06\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "OUTPUT Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0096 -0.0056  0.0106  0.0185  0.0021 -0.0008 -0.0322 -0.0103\n",
      " -0.0188 -0.0175  0.0121 -0.0081 -0.0107  0.0135 -0.0162  0.0173\n",
      " -0.0155 -0.0070  0.0069 -0.0184 -0.0043 -0.0013 -0.0144 -0.0367\n",
      "  0.0107  1.0132 -0.0163  0.9737  0.0309 -0.0256  0.0297 -0.0213\n",
      "  0.9585  0.0103  0.0249  0.0105  0.9473  0.0144 -0.0243  0.0463\n",
      "  0.0028  0.0425  0.9760  0.0434  0.0484  1.0033  0.9854  0.9431\n",
      "[torch.FloatTensor of size 1x6x8]\n",
      "\n",
      "IR Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0170\n",
      " -0.0062\n",
      "  1.3200\n",
      " -0.0322\n",
      " -0.0339\n",
      " -0.0166\n",
      " -0.1741\n",
      "  0.0023\n",
      "[torch.FloatTensor of size 1x8x1]\n",
      "\n",
      "RUNNING OP Jump(M)  WITH PROBABILITY Variable containing:\n",
      " 1.2787\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "MEM Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 5 \n",
      "   9.4990e-04 -4.5819e-03  3.1232e-02  8.5925e-01 -1.1969e-02  5.0352e-03\n",
      "  2.5130e-03  1.7586e-04  2.6011e-04  9.9786e-01  4.2398e-04  5.5919e-05\n",
      "  8.3246e-03  8.7210e-03 -7.3490e-03 -2.6206e-03  2.4330e-02 -1.1631e-02\n",
      " -1.1127e-01  2.0839e-03  2.7589e-03  4.7892e-03 -9.3120e-04  1.3990e-04\n",
      " -3.6045e-04 -1.3252e-04 -1.2804e-04  5.1244e-05 -3.7991e-04  1.7787e-04\n",
      " -5.3503e-05  1.8778e-04 -2.5773e-04 -4.8914e-05  5.1095e-04 -2.5278e-04\n",
      " -7.4665e-04  1.0487e-04 -3.1558e-04 -3.2151e-06  2.3950e-04 -1.2968e-04\n",
      " -1.5928e-03 -1.0890e-03  1.0438e-03  3.4758e-04 -3.0647e-03  1.4495e-03\n",
      "\n",
      "Columns 6 to 7 \n",
      "   1.3510e-01 -3.1890e-03\n",
      " -7.1632e-04  1.7845e-04\n",
      " -2.8543e-01  7.0606e-03\n",
      " -8.1029e-03 -2.0914e-04\n",
      "  4.3232e-03 -1.1365e-04\n",
      " -6.1711e-03  1.4590e-04\n",
      " -3.1875e-03  6.7484e-05\n",
      "  3.5673e-02 -8.9164e-04\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "OUTPUT Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0096 -0.0056  0.0106  0.0185  0.0021 -0.0008 -0.0322 -0.0103\n",
      " -0.0188 -0.0175  0.0121 -0.0081 -0.0107  0.0135 -0.0162  0.0173\n",
      " -0.0155 -0.0070  0.0069 -0.0184 -0.0043 -0.0013 -0.0144 -0.0367\n",
      "  0.0107  1.0132 -0.0163  0.9737  0.0309 -0.0256  0.0297 -0.0213\n",
      "  0.9585  0.0103  0.0249  0.0105  0.9473  0.0144 -0.0243  0.0463\n",
      "  0.0028  0.0425  0.9760  0.0434  0.0484  1.0033  0.9854  0.9431\n",
      "[torch.FloatTensor of size 1x6x8]\n",
      "\n",
      "IR Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0292\n",
      "  0.0558\n",
      " -0.0314\n",
      "  2.2593\n",
      "  0.0191\n",
      " -0.0938\n",
      " -0.9004\n",
      " -0.2774\n",
      "[torch.FloatTensor of size 1x8x1]\n",
      "\n",
      "RUNNING OP Decrement(M)  WITH PROBABILITY Variable containing:\n",
      " 2.2250\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "MEM Variable containing:\n",
      "(0 ,.,.) = \n",
      "  2.1273 -0.3064 -0.3940 -0.9708 -1.0059  0.0141 -0.1249  0.1839\n",
      " -0.0491  0.0077  0.0092  1.0489  0.0252 -0.0004 -0.0008 -0.0042\n",
      " -2.5670  0.4080  0.4205  0.2032  1.3215 -0.0618 -0.9567 -0.1970\n",
      " -0.0981  0.0003  0.0007  0.0038 -0.0066  0.0002 -0.0080  0.0008\n",
      "  0.0301 -0.0046 -0.0053 -0.0024 -0.0149  0.0004  0.0042  0.0025\n",
      " -0.0554  0.0083  0.0092  0.0045  0.0270 -0.0008 -0.0065 -0.0046\n",
      " -0.0332  0.0048  0.0052  0.0026  0.0157 -0.0004 -0.0033 -0.0027\n",
      "  0.3468 -0.0515 -0.0590 -0.0282 -0.1683  0.0041  0.0245  0.0290\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "OUTPUT Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0096 -0.0056  0.0106  0.0185  0.0021 -0.0008 -0.0322 -0.0103\n",
      " -0.0188 -0.0175  0.0121 -0.0081 -0.0107  0.0135 -0.0162  0.0173\n",
      " -0.0155 -0.0070  0.0069 -0.0184 -0.0043 -0.0013 -0.0144 -0.0367\n",
      "  0.0107  1.0132 -0.0163  0.9737  0.0309 -0.0256  0.0297 -0.0213\n",
      "  0.9585  0.0103  0.0249  0.0105  0.9473  0.0144 -0.0243  0.0463\n",
      "  0.0028  0.0425  0.9760  0.0434  0.0484  1.0033  0.9854  0.9431\n",
      "[torch.FloatTensor of size 1x6x8]\n",
      "\n",
      "IR Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.1334\n",
      " -0.0274\n",
      " -0.0171\n",
      " -0.0489\n",
      "  1.4305\n",
      "  0.0163\n",
      " -0.0658\n",
      " -0.6047\n",
      "[torch.FloatTensor of size 1x8x1]\n",
      "\n",
      "RUNNING OP Increment(M  WITH PROBABILITY Variable containing:\n",
      " 1.3798\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "MEM Variable containing:\n",
      "(0 ,.,.) = \n",
      "  2.0127 -0.4010 -0.3644 -0.8834 -0.9212  0.0118 -0.1028  0.2644\n",
      " -0.1375  0.1400  0.0165  1.1513  0.0344  0.0007 -0.0125 -0.1090\n",
      " -2.6289  0.4397  0.4293  0.2070  1.3466 -0.0627 -0.9762 -0.2199\n",
      " -0.0997  0.0025  0.0008  0.0038 -0.0065  0.0003 -0.0082 -0.0009\n",
      " -0.0497  0.1251  0.0005 -0.0022 -0.0097  0.0016 -0.0069 -0.1006\n",
      " -0.0560  0.0092  0.0093  0.0045  0.0271 -0.0007 -0.0066 -0.0053\n",
      " -0.0208 -0.0139  0.0042  0.0025  0.0145 -0.0006 -0.0016  0.0122\n",
      "  0.3848 -0.1479 -0.0595 -0.0265 -0.1610  0.0029  0.0316  0.1064\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "OUTPUT Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0096 -0.0056  0.0106  0.0185  0.0021 -0.0008 -0.0322 -0.0103\n",
      " -0.0188 -0.0175  0.0121 -0.0081 -0.0107  0.0135 -0.0162  0.0173\n",
      " -0.0155 -0.0070  0.0069 -0.0184 -0.0043 -0.0013 -0.0144 -0.0367\n",
      "  0.0107  1.0132 -0.0163  0.9737  0.0309 -0.0256  0.0297 -0.0213\n",
      "  0.9585  0.0103  0.0249  0.0105  0.9473  0.0144 -0.0243  0.0463\n",
      "  0.0028  0.0425  0.9760  0.0434  0.0484  1.0033  0.9854  0.9431\n",
      "[torch.FloatTensor of size 1x6x8]\n",
      "\n",
      "IR Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.6168\n",
      "  0.1457\n",
      " -0.0271\n",
      " -0.0172\n",
      " -0.0487\n",
      "  1.4424\n",
      "  0.0154\n",
      " -0.0752\n",
      "[torch.FloatTensor of size 1x8x1]\n",
      "\n",
      "RUNNING OP Jump(M)  WITH PROBABILITY Variable containing:\n",
      " 1.4417\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "MEM Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 5 \n",
      "   1.6439e+00 -2.1050e-01 -1.2484e-01 -7.7510e-01 -8.1224e-01  3.0624e-02\n",
      " -1.9148e-01  1.9733e-01  1.0413e-01  1.0946e+00  3.3757e-02  9.7125e-03\n",
      " -2.6893e+00  4.0594e-01  3.7580e-01  2.1333e-01  1.4019e+00 -7.2674e-02\n",
      " -8.3637e-02 -1.5945e-02 -2.4532e-02  3.0158e-03 -6.9646e-03 -2.3263e-03\n",
      " -5.2595e-02  1.2795e-01  4.8628e-03 -2.0459e-03 -9.5902e-03  2.0375e-03\n",
      " -2.5242e-02 -2.4719e-02 -3.7288e-02  3.0950e-03  2.7182e-02 -5.5595e-03\n",
      " -1.5279e-02 -1.9947e-02 -3.8996e-03  2.2869e-03  1.4509e-02 -1.4113e-03\n",
      "  4.1131e-01 -1.7159e-01 -8.9495e-02 -2.7843e-02 -1.6410e-01 -1.8270e-05\n",
      "\n",
      "Columns 6 to 7 \n",
      "  -1.0747e-01  1.0028e-01\n",
      " -1.9281e-02 -1.6324e-01\n",
      " -1.0109e+00 -1.8077e-01\n",
      " -6.2161e-03  1.6189e-02\n",
      " -7.2764e-03 -1.0328e-01\n",
      " -2.8277e-03  2.6232e-02\n",
      " -9.0577e-04  1.7773e-02\n",
      "  3.4544e-02  1.2781e-01\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "DESIRED OUTPUT Variable containing:\n",
      "(0 ,.,.) = \n",
      "   0   0   0   0   0   0   1   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "FINAL MEMORY Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 5 \n",
      "   1.6439e+00 -2.1050e-01 -1.2484e-01 -7.7510e-01 -8.1224e-01  3.0624e-02\n",
      " -1.9148e-01  1.9733e-01  1.0413e-01  1.0946e+00  3.3757e-02  9.7125e-03\n",
      " -2.6893e+00  4.0594e-01  3.7580e-01  2.1333e-01  1.4019e+00 -7.2674e-02\n",
      " -8.3637e-02 -1.5945e-02 -2.4532e-02  3.0158e-03 -6.9646e-03 -2.3263e-03\n",
      " -5.2595e-02  1.2795e-01  4.8628e-03 -2.0459e-03 -9.5902e-03  2.0375e-03\n",
      " -2.5242e-02 -2.4719e-02 -3.7288e-02  3.0950e-03  2.7182e-02 -5.5595e-03\n",
      " -1.5279e-02 -1.9947e-02 -3.8996e-03  2.2869e-03  1.4509e-02 -1.4113e-03\n",
      "  4.1131e-01 -1.7159e-01 -8.9495e-02 -2.7843e-02 -1.6410e-01 -1.8270e-05\n",
      "\n",
      "Columns 6 to 7 \n",
      "  -1.0747e-01  1.0028e-01\n",
      " -1.9281e-02 -1.6324e-01\n",
      " -1.0109e+00 -1.8077e-01\n",
      " -6.2161e-03  1.6189e-02\n",
      " -7.2764e-03 -1.0328e-01\n",
      " -2.8277e-03  2.6232e-02\n",
      " -9.0577e-04  1.7773e-02\n",
      "  3.4544e-02  1.2781e-01\n",
      "[torch.FloatTensor of size 1x8x8]\n",
      "\n",
      "LOSS COMPONENTS Variable containing:\n",
      " 5.2602\n",
      "[torch.FloatTensor of size 1]\n",
      " 12.941134801832959 Variable containing:\n",
      " 5.2602\n",
      "[torch.FloatTensor of size 1]\n",
      " 2.12487483874429\n",
      "\n",
      "Training complete in 0m 0s\n",
      "Best loss: 0.826237\n"
     ]
    }
   ],
   "source": [
    "M = 8 # Don't change this (as long as we're using the add-task)\n",
    "num_examples = 1\n",
    "\n",
    "dataset = AddTaskDataset(M, num_examples)\n",
    "data_loader = data.DataLoader(dataset, batch_size = 1) # Don't change this batch size.  You have been warned.\n",
    "\n",
    "def anc_validation_criterion(output, label):\n",
    "    target_memory = label[1]\n",
    "    target_mask = label[2]\n",
    "    \n",
    "    output = output.data * target_mask\n",
    "    target_memory = target_memory * target_mask\n",
    "    _, target_indices = torch.max(target_memory, 2)\n",
    "    _, output_indices = torch.max(output, 2)\n",
    "    return torch.equal(output_indices, target_indices)\n",
    "\n",
    "# Initialize our controller\n",
    "controller = Controller(first_arg = first_arg, \n",
    "                        second_arg = second_arg, \n",
    "                        output = target, \n",
    "                        instruction = instruction, \n",
    "                        initial_registers = init_registers, \n",
    "                        stop_threshold = .9, \n",
    "                        blur = None, \n",
    "                        correctness_weight = .35, \n",
    "                        halting_weight = .3, \n",
    "                        confidence_weight = .3, \n",
    "                        efficiency_weight = .05, \n",
    "                        t_max = 10) \n",
    "\n",
    "# Learning rate is a tunable hyperparameter\n",
    "# The paper didn't mention which one they used\n",
    "optimizer = optim.Adam(controller.parameters(), lr = 0.001)\n",
    "\n",
    "plot_every = 10\n",
    "\n",
    "best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "    controller, \n",
    "    data_loader,  \n",
    "    optimizer, \n",
    "    num_epochs = 1, \n",
    "    print_every = 10, \n",
    "    plot_every = plot_every, \n",
    "    deep_copy_desired = False, \n",
    "    validation_criterion = anc_validation_criterion, \n",
    "    forward_train = True, \n",
    "    batch_size = 5) # In the paper, they used batch sizes of 1 or 5\n",
    "    \n",
    "    #kangaroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114476278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x * plot_every for x in range(len(train_plot_losses))], train_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114abf3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x * plot_every for x in range(len(validation_plot_losses))], validation_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "    0     1     0     0     0     0     0     0\n",
      "    0     0     1     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 8x8]\n",
      ", \n",
      "    0     0     0     1     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 8x8]\n",
      ", \n",
      "    1     1     1     1     1     1     1     1\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 8x8]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(dataset.input_list[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
