{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/oliviawatkins/Documents/Schoolwork/NN/neural_nets_research\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets_library import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    Contains the two learnable parts of the model in four independent, fully connected layers.\n",
    "    First the initial values for the registers and instruction registers and second the \n",
    "    parameters that computes the required distributions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 first_arg = None, \n",
    "                 second_arg = None, \n",
    "                 output = None, \n",
    "                 instruction = None, \n",
    "                 initial_registers = None, \n",
    "                 stop_threshold = .99, \n",
    "                 blur = 3, \n",
    "                 multiplier = 50,\n",
    "                 correctness_weight = .25, \n",
    "                 halting_weight = .25, \n",
    "                 confidence_weight = .25, \n",
    "                 efficiency_weight = .25,\n",
    "                 t_max = 100):\n",
    "        #TODO: Read over ANC paper, check if there are more reasonable default initial values.\n",
    "        \"\"\"\n",
    "        Initialize a bunch of constants and pass in matrices defining a program.\n",
    "        \n",
    "        :param first_arg: Matrix with the 1st register argument for each timestep stored in the columns (1xRxM)\n",
    "        :param second_arg: Matrix with the 2nd register argument for each timestep stored in the columns (1xRxM)\n",
    "        :param output: Matrix with the output register for each timestep stored in the columns (1xRxM)\n",
    "        :param instruction: Matrix with the instruction for each timestep stored in the columns (1xNxM)\n",
    "        :param initial_registers: Matrix where each row is a distribution over the value in one register (1xRxM)\n",
    "        :param stop_threshold: The stop probability threshold at which the controller should stop running\n",
    "        :param blur: The factor our one-hot vectors are be multiplied by before they're softmaxed to add blur\n",
    "        :param correctness_weight: Weight given to the correctness component of the loss function\n",
    "        :param halting_weight: Weight given to the halting component of the loss function\n",
    "        :param confidence_weight: Weight given to the confidence component of the loss function\n",
    "        :param efficiency_weight: Weight given to the efficiency component of the loss function\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Controller, self).__init__()\n",
    "        \n",
    "        # Initialize dimension constants\n",
    "        B, R, M = initial_registers.size()\n",
    "        self.M = M\n",
    "        self.R = R\n",
    "        \n",
    "        # Initialize loss function weights\n",
    "        # In the ANC paper, these scalars are called, alpha, beta, gamma, and delta\n",
    "        self.correctness_weight = correctness_weight\n",
    "        self.halting_weight = halting_weight\n",
    "        self.confidence_weight = confidence_weight\n",
    "        self.efficiency_weight = efficiency_weight\n",
    "        \n",
    "        \n",
    "        # And yet more initialized constants... yeah, there are a bunch, I know.\n",
    "        self.multiplier = multiplier\n",
    "        self.t_max = t_max\n",
    "        self.blur_factor = blur\n",
    "        self.stop_threshold = stop_threshold\n",
    "        \n",
    "        # Blur matrices - i.e. give each different operation/argument/register value some nonzero probability\n",
    "        if blur is not None:\n",
    "            first_arg = self.blur(first_arg, blur, 1)\n",
    "            second_arg = self.blur(second_arg, blur, 1)\n",
    "            output = self.blur(output, blur, 1)\n",
    "            instruction = self.blur(instruction, blur, 1)\n",
    "            initial_registers = self.blur(initial_registers, blur, 2)\n",
    "            \n",
    "            # Initialize parameters.  These are the things that are going to be optimized. \n",
    "            self.first_arg = nn.Parameter(first_arg.data)\n",
    "            self.second_arg = nn.Parameter(second_arg.data)\n",
    "            self.output = nn.Parameter(output.data)\n",
    "            self.instruction = nn.Parameter(instruction.data) \n",
    "            self.registers = nn.Parameter(initial_registers.data)\n",
    "        else:\n",
    "            # Initialize parameters.  These are the things that are going to be optimized. \n",
    "            self.first_arg = nn.Parameter(first_arg)\n",
    "            self.second_arg = nn.Parameter(second_arg)\n",
    "            self.output = nn.Parameter(output)\n",
    "            self.instruction = nn.Parameter(instruction) \n",
    "            self.registers = nn.Parameter(initial_registers)\n",
    "        \n",
    "        # Machine initialization\n",
    "        self.machine = Machine(B, M, R)\n",
    "    \n",
    "    def blur(self, matrix, scale_factor, dimension):\n",
    "        \"\"\"\n",
    "        Takes a matrix, each row (or column) of which is a one-hot vector.\n",
    "        Multiply each 1 by a constant and then softmax it, which \n",
    "        effectively \"blurs\" the matrix a little bit.\n",
    "        \n",
    "        :param matrix: Matrix to blur\n",
    "        :param scale_factor: Constant to multiply the matrix by before it's softmaxed\n",
    "        :param dimension: Dimension to softmax over\n",
    "        \n",
    "        :return: Blurred matrix\n",
    "        \"\"\"\n",
    "        matrix = scale_factor * matrix\n",
    "        softmax = nn.Softmax(dimension)\n",
    "        return softmax(Variable(matrix))    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def forward(self, input, train):\n",
    "        \"\"\"\n",
    "        Runs the controller on a certain input memory matrix.\n",
    "        It either returns the loss or the output memory.\n",
    "        \n",
    "        :param input: A three-tuple of three MxM matrices: (memory matrix, output_memory, output_mask)\n",
    "        \n",
    "        :return: If train is true, return the loss. Otherwise, return the output matrix\n",
    "        \"\"\"\n",
    "        # Program's initial memory\n",
    "        self.memory = input[0]\n",
    "        # Desired output memory\n",
    "        self.output_memory = Variable(input[1])\n",
    "        # Mask with 1's in the rows of the output memory matrix which actually contain the answer.\n",
    "        self.output_mask = Variable(input[2])\n",
    "    \n",
    "        \n",
    "        # Initialize instruction regiser (1xMx1)\n",
    "        self.register_buffer('IR', torch.DoubleTensor(1, M, 1).zero_())\n",
    "        self.IR[0, 0, 0] = 1\n",
    "        \n",
    "        # Blur memory and instruction register\n",
    "        if self.blur_factor is not None:\n",
    "            self.memory = self.blur(self.memory, self.blur_factor, 2) \n",
    "            IR = self.blur(self.IR, self.blur_factor, 1)\n",
    "        else:\n",
    "            IR = Variable(self.IR)\n",
    "            self.memory = Variable(self.memory)\n",
    "        \n",
    "        efficiency_loss = 0\n",
    "        confidence_loss = 0\n",
    "        self.stop_probability = 0\n",
    "        \n",
    "        # Copy registers so we aren't using the values from the previous iteration.\n",
    "        registers = self.registers\n",
    "        \n",
    "        # loss initialization\n",
    "        self.confidence = 0\n",
    "        self.efficiency = 0\n",
    "        self.halting = 0\n",
    "        self.correctness = 0\n",
    "        \n",
    "        t = 0 \n",
    "        # Run the program, one timestep at a time, until the program terminates or whe time out\n",
    "        while t < self.t_max and self.stop_probability < self.stop_threshold: \n",
    "            \n",
    "            softmax = nn.Softmax(1)\n",
    "            \n",
    "            a = softmax(torch.bmm(self.multiplier * self.first_arg, IR))\n",
    "            b = softmax(torch.bmm(self.multiplier * self.second_arg, IR))\n",
    "            o = softmax(torch.bmm(self.multiplier * self.output, IR))\n",
    "            e = softmax(torch.bmm(self.multiplier * self.instruction, IR))\n",
    "            \n",
    "            old_mem = self.memory\n",
    "            \n",
    "            # Update memory, registers, and IR after machine operation\n",
    "            self.old_stop_probability = self.stop_probability\n",
    "            self.memory, registers, IR, new_stop_prob = self.machine(e, a, b, o, self.memory, registers, IR) \n",
    "            self.stop_probability += new_stop_prob[0]\n",
    "            \n",
    "            # If we're training, calculate loss\n",
    "            if train:\n",
    "                self.timestep_loss(t)\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        # If we're training, return loss.  Otherwise return memory.\n",
    "        if train:\n",
    "            self.final_loss(t)\n",
    "            total_loss  = self.total_loss()\n",
    "            return (self.memory, total_loss)\n",
    "        else:\n",
    "            return (self.memory, None)\n",
    "        \n",
    "        \n",
    "    def timestep_loss(self, t):\n",
    "        \"\"\"\n",
    "        @ Rakia @ Aditya feel free to use this function definition or not.  My main thought was that this would \n",
    "        compute the types of loss which get updated every timestep.\n",
    "        \"\"\"\n",
    "        # Confidence Loss \n",
    "        mem_diff = self.output_memory - self.memory\n",
    "        correctness = torch.sum((self.output_mask) * mem_diff * mem_diff)\n",
    "        self.confidence += (self.stop_probability - self.old_stop_probability) * correctness\n",
    "        \n",
    "        # Efficiency Loss\n",
    "        if t < self.t_max and self.stop_probability < self.stop_threshold: # don't add efficiency loss on the last timestep\n",
    "            self.efficiency += (1 - self.stop_probability)\n",
    "            \n",
    "    \n",
    "    def final_loss(self, t):\n",
    "        \"\"\"\n",
    "        @ Rakia @ Aditya feel free to use this function definition or not.  My main thought was that this would \n",
    "        compute the types of loss which get updated every timestep.\n",
    "        \"\"\"\n",
    "        # Correctness loss\n",
    "        mem_diff = self.output_memory - self.memory\n",
    "        self.correctness = torch.sum((self.output_mask) * mem_diff * mem_diff)\n",
    "\n",
    "        # Halting loss\n",
    "        if t == self.t_max:\n",
    "            self.halting = (1 - self.stop_probability)\n",
    "   \n",
    "\n",
    "    def total_loss(self):\n",
    "        \"\"\" compute four diferent loss functions and return a weighted average of the four measuring correctness, \n",
    "        halting, efficiency, and confidence\"\"\"\n",
    "        return  (self.correctness*self.correctness_weight) + (self.confidence_weight*self.confidence) + (self.halting_weight*self.halting) + (self.efficiency_weight*self.efficiency) \n",
    "        \n",
    "      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(nn.Module):\n",
    "    \"\"\"\n",
    "    Parent class for our binary operations\n",
    "    \"\"\"\n",
    "    def __init__(self, M):\n",
    "        \"\"\"\n",
    "        Initialize the memory length (needed so we can mod our answer in case it exceeds the range 0-M-1)\n",
    "        Also calculate the output matrix for the operation\n",
    "        \n",
    "        :param M: Memory length\n",
    "        \"\"\"\n",
    "        super(Operation, self).__init__()\n",
    "        self.M = M\n",
    "        \n",
    "        # Create a MxMxM matrix where the (i,j,k) cell is 1 iff operation(i,j) = k.\n",
    "        self.outputs = torch.IntTensor(M, M, M).zero_()\n",
    "        for i in range(M):\n",
    "            for j in range(M):\n",
    "                val = self.compute(i, j)\n",
    "                self.outputs[val][i][j] = 1\n",
    "                \n",
    "        self.outputs = Variable(self.outputs)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        \"\"\" \n",
    "        Perform the binary operation.  The arguments may or may not be used.\n",
    "        \n",
    "        :param x: First argument\n",
    "        :param y: Second argument\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        :return: The output matrix\n",
    "        \"\"\"\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Operation):\n",
    "\n",
    "    def __init__(self, M):\n",
    "        super(Add, self).__init__(M)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        return (x + y) % self.M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stop(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Stop, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jump(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Jump, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0 # Actual jump happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decrement(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Decrement, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x - 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Increment(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Increment, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x + 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Max, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return max(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Min(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Min, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return min(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Read, self).__init__(M)\n",
    "        # Leave output matrix blank since we're gonna do the reading elsewhere\n",
    "        self.outputs = torch.DoubleTensor(M, M, M).zero_()\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return 0 # Actual reading happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtract(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Subtract, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return (x - y) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Write(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Write, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return 0 # Actual write happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zero(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Zero, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine(nn.Module):\n",
    "    \"\"\"\n",
    "    The Machine executes assembly instructions passed to it by the Controller.\n",
    "    It updates the given memory, registers, and instruction pointer.\n",
    "    The Machine doesn't have any learnable parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, B, M, R):\n",
    "        \"\"\"\n",
    "        Initializes dimensions, operations, and counters\n",
    "        \n",
    "        :param B: Batch size (meant to be 1)\n",
    "        :param M: Memory length.  Integer values also take on values 0-M-1.  M is also the program length.\n",
    "        :param R: Number of registers\n",
    "        \"\"\"\n",
    "        super(Machine, self).__init__()\n",
    "        \n",
    "        # Store parameters as class variables\n",
    "        self.R = R # Number of registers\n",
    "        self.M = M # Memory length (also largest number)\n",
    "        self.B = B # Batch size\n",
    "        \n",
    "        # Start off with 0 probability of stopping\n",
    "        self.stop_probability = 0 \n",
    "        \n",
    "        # List of ops (must be in same order as the original ANC paper so compilation works right)\n",
    "        self.ops = [ \n",
    "            Stop(M),\n",
    "            Zero(M),\n",
    "            Increment(M),\n",
    "            Add(M),\n",
    "            Subtract(M),\n",
    "            Decrement(M),\n",
    "            Min(M),\n",
    "            Max(M),\n",
    "            Read(M),\n",
    "            Write(M),\n",
    "            Jump(M)\n",
    "        ]\n",
    "        \n",
    "        # Number of instructions\n",
    "        self.N = len(self.ops)\n",
    "        \n",
    "        # Create a 4D matrix composed of the output matrices of each of the ops\n",
    "        self.outputs = Variable(torch.DoubleTensor(self.N, M, M, M)).zero_()\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            op = self.ops[i]\n",
    "            self.outputs[i] = op()\n",
    "            \n",
    "        # Add an extra batch dimension\n",
    "        self.outputs = torch.unsqueeze(self.outputs, 0)\n",
    "        self.outputs = self.outputs.expand(B, -1, -1, -1, -1)\n",
    "        \n",
    "        # Keep track of ops which will be handled specially\n",
    "        self.jump_index = 10\n",
    "        self.stop_index = 0\n",
    "        self.write_index = 9\n",
    "        self.read_index = 8\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, e, a, b, o, memory, registers, IR):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run the Machine for one timestep (corresponding to the execution of one line of Assembly).\n",
    "        The first four parameter names correspond to the vector names used in the original ANC paper\n",
    "        \n",
    "        :param e: Probability distribution over the instruction being executed (BxMx1)\n",
    "        :param a: Probability distribution over the first argument register (length BxRx1)\n",
    "        :param b: Probability distribution over the second argument register (length BxRx1)\n",
    "        :param o: Probability distribution over the first argument register (length BxRx1)\n",
    "        :param memory: Memory matrix (size BxMxM)\n",
    "        :param registers: Register matrix (size BxRxM)\n",
    "        :param IR: Instruction Register (length BxM)\n",
    "        \n",
    "        :return: The memory, registers, and instruction register after the timestep\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO: Get rid of all of this\n",
    "        op_names = [ \n",
    "            \"Stop(M)\",\n",
    "            \"Zero(M)\",\n",
    "            \"Increment(M\",\n",
    "            \"Add(M)\",\n",
    "            \"Subtract(M\",\n",
    "            \"Decrement(M)\",\n",
    "            \"Min(M)\",\n",
    "            \"Max(M)\",\n",
    "            \"Read(M)\",\n",
    "            \"Write(M)\",\n",
    "            \"Jump(M)\"\n",
    "        ]\n",
    "        e_temp = e\n",
    "        e_temp = e_temp.squeeze()\n",
    "        num, index = torch.max(e_temp, 0)\n",
    "        \n",
    "        # Dimensions B x 1 x R -> B x 1 x R\n",
    "        a = torch.transpose(a, 1, 2)\n",
    "        b = torch.transpose(b, 1, 2)\n",
    "        \n",
    "        # Calculate distributions over the two argument values by multiplying each \n",
    "        # register by the probability that register is being used.\n",
    "        arg1 = torch.bmm(a, registers)\n",
    "        arg2 = torch.bmm(b, registers)\n",
    "        \n",
    "        # Multiply the output matrix by the arg1 and arg2 vectors to take into account\n",
    "        # Before we do this, we're going to have to do a bunch of dimension squishing.\n",
    "        \n",
    "        # arg1_long dimensions: B x 1 x M --> B x 1 x 1 x 1 x M\n",
    "        arg1_long = torch.unsqueeze(arg1, 1)\n",
    "        arg1_long = torch.unsqueeze(arg1_long, 1)\n",
    "        \n",
    "        outputs_x_arg1 = torch.matmul(arg1_long, self.outputs)\n",
    "        \n",
    "        # outputs_x_arg1 dimensions: B x N x M x 1 x M -> B x N x M x M\n",
    "        outputs_x_arg1 = torch.squeeze(outputs_x_arg1, 3)\n",
    "        \n",
    "        # arg2_long dimensions: B x 1 x M --> B x 1 x M x 1\n",
    "        arg2_long = torch.unsqueeze(arg2, 3)\n",
    "        \n",
    "        outputs_x_args = torch.matmul(outputs_x_arg1, arg2_long)\n",
    "        \n",
    "        # outputs_x_args dimensions: B x N x M x 1 -> B x N x M\n",
    "        outputs_x_args = torch.squeeze(outputs_x_args, 3)\n",
    "        \n",
    "        # e dimensions B x N x 1 -> B x 1 x N\n",
    "        e = torch.transpose(e, 1, 2)\n",
    "        \n",
    "        # read_vec dimensions B x 1 -> B x 1 x 1\n",
    "        read_vec =  e[:, :, self.read_index]\n",
    "        read_vec = read_vec.unsqueeze(1)\n",
    "        \n",
    "        # Length Bx1xM vector over the output of the operation\n",
    "        out_vec = torch.matmul(e, outputs_x_args)\n",
    "        \n",
    "        # Deal with memory reads separately\n",
    "        out_vec = out_vec + read_vec * torch.matmul(arg1, memory)        \n",
    "        \n",
    "        # Update our memory, registers, instruction register, and stopping probability\n",
    "        mem_old = memory\n",
    "        memory = self.writeMemory(e, o, memory, arg1, arg2)\n",
    "        registers = self.writeRegisters(out_vec, o, registers)\n",
    "        IR = self.updateIR(e, IR, arg1, arg2)\n",
    "        stop_prob = self.getStop(e)\n",
    "        \n",
    "        return(memory, registers, IR, stop_prob)\n",
    "        \n",
    "        \n",
    "    def writeRegisters(self, out, o, registers):\n",
    "        \"\"\"\n",
    "        Write the result of our operation to our registers.\n",
    "        \n",
    "        :param out: Probability distribution over the output value (Bx1xM)\n",
    "        :param o: Probability distribution over the output register (BxRx1)\n",
    "        :param Registers: register matrix (BxRxM)\n",
    "        \n",
    "        :return: The updated registers (BxRxM)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Multiply probability of writing to each output register by the distribution over the value we're writing there.\n",
    "        new_register_vals = torch.matmul(o, out)\n",
    "        \n",
    "        # Multiply each original register cell by the probabilty of not writing to that register\n",
    "        old_register_vals = (1-o).expand(self.B, self.R, self.M) * registers\n",
    "        \n",
    "        # Take a weighted sum over the old and new register values\n",
    "        registers =  new_register_vals + old_register_vals\n",
    "        \n",
    "        return registers\n",
    "    \n",
    "    def updateIR(self, e, IR, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the instruction register\n",
    "        \n",
    "        :param e: Distribution over the current instruction (BxNx1)\n",
    "        :param IR: Instruction register (length BxMx1)\n",
    "        :param arg1: Distribution over the first argument value (length BxMx1)\n",
    "        :param arg2: Distribution over the second argument value (length BxMx1)\n",
    "        \n",
    "        :return: The updated instruction register (BxMx1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dimensions B x 1 x M -> B x M x 1\n",
    "        arg2 = arg2.transpose(1, 2)\n",
    "        \n",
    "        # Probability that we're on the jump instruction\n",
    "        jump_probability = e[:, :, self.jump_index]\n",
    "        \n",
    "        # Probability that the first argument is 0\n",
    "        is_zero = arg1[:, :, 0]\n",
    "        \n",
    "        # Slicing lost a dimension.  Let's add it back\n",
    "        jump_probability = torch.unsqueeze(jump_probability, 1)\n",
    "        is_zero = torch.unsqueeze(is_zero, 1)\n",
    "        \n",
    "        # If we're not jumping, just shift IR by one slot\n",
    "        wraparound = IR[:, -1]\n",
    "        normal_instructions = IR[:, :-1]\n",
    "        \n",
    "        # For whatever reason, when you chop off one row/column, that dimension disappears.  Add it back.\n",
    "        wraparound = wraparound.unsqueeze(1)\n",
    "        IR_no_jump = torch.cat([wraparound, normal_instructions], 1)\n",
    "        \n",
    "        # If we are on a jump instruction, check whether the argument's 0.\n",
    "        # If it is, jump to the location specified by arg2.  Otherwise, increment like normal.\n",
    "        IR_jump = arg2 * is_zero + (1 - is_zero) * IR_no_jump\n",
    "        \n",
    "        # Take a weighted sum of the instruction register with and without jumping\n",
    "        IR = IR_no_jump * (1 - jump_probability) + IR_jump * jump_probability\n",
    "        \n",
    "        return IR\n",
    "    \n",
    "    def writeMemory(self, e, o, mem_orig, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the memory\n",
    "        \n",
    "        :param e: Distribution over the current instruction (B x1xM)\n",
    "        :param mem_orig: Current memory matrix (BxMxM)\n",
    "        :param arg1: Distribution over the first argument value (Bx1xM)\n",
    "        :param arg2: Distribution over the second argument value (Bx1xM)\n",
    "        \n",
    "        :return: The updated memory matrix (BxMxM)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Probability that we're on the write instruction\n",
    "        write_probability = e[:,:, self.write_index]\n",
    "        \n",
    "        # write_probability dimensions: Bx1 -> B x 1 x 1\n",
    "        write_probability = torch.unsqueeze(write_probability, 1)\n",
    "        \n",
    "        # arg1 dimensions: B x 1 x M -> B x M x 1\n",
    "        arg1 = torch.transpose(arg1, 1, 2)\n",
    "        \n",
    "        # If we are on a write instruction, write the value arg2 in register arg1. Otherwise, leave memory as is.\n",
    "        mem_changed = torch.bmm(arg1, arg2)\n",
    "        mem_unchanged = mem_orig * (1-arg1).expand(-1, -1, self.M)\n",
    "        mem_write = mem_changed + mem_unchanged\n",
    "        \n",
    "        \n",
    "        # Take a weighted sum over the new memory and old memory\n",
    "        memory = mem_orig * (1 - write_probability) + mem_write * write_probability\n",
    "        return memory\n",
    "        \n",
    "    def getStop(self, e):\n",
    "        \"\"\"\n",
    "        Obtain the probability that we will stop at this timestep based on the probability that we are running the STOP op.\n",
    "        \n",
    "        :param e: distribution over the current instruction (length Bx1xM)\n",
    "        \n",
    "        :return: probability representing whether the controller should stop.\n",
    "        \"\"\"\n",
    "        return e[:, :, self.stop_index].data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hotify(vec, length, dimension):\n",
    "    \"\"\"\n",
    "    Turn a tensor of integers into a matrix of one-hot vectors.\n",
    "    \n",
    "    :param vec: The vector to be converted.\n",
    "    :param length: One dimension of the matrix (the other is the length of vec)\n",
    "    :param dimension: Which dimension stores the elements of vec.  If 0, they're stored in the rows.  If 1, the columns.\n",
    "    \n",
    "    :return A matrix of one-hot vectors, each row or column corresponding to one element of vec\n",
    "    \"\"\"\n",
    "    x = vec.size()[0]\n",
    "    if dimension == 0:\n",
    "        binary_vec = torch.DoubleTensor(x, length).zero_()\n",
    "        for i in range(x):\n",
    "            binary_vec[i][vec[i]] = 1\n",
    "        return binary_vec\n",
    "    elif dimension == 1:\n",
    "        binary_vec = torch.DoubleTensor(length, x).zero_()\n",
    "        for i in range(x):\n",
    "            binary_vec[vec[i]][i] = 1\n",
    "        return binary_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition task\n",
    "# Generate this by running the instructions here (but with the addition program file): https://github.com/aditya-khant/neural-assembly-compiler\n",
    "# Then get rid of the .cuda in each of the tensors since we (or at least I) don't have cuda\n",
    "init_registers = torch.IntTensor([6,2,0,1,0,0]) # Length R, should be RxM\n",
    "first_arg = torch.IntTensor([4,3,3,3,4,2,2,5]) # Length M, should be RxM\n",
    "second_arg = torch.IntTensor([5,5,0,5,5,1,4,5]) # Length M, should be RxM\n",
    "target = torch.IntTensor([4,3,5,3,4,5,5,5]) # Length M, should be RxM\n",
    "instruction = torch.IntTensor([8,8,10,5,2,10,9,0]) # Length M, should be NxM\n",
    "\n",
    "# Get dimensions we'll need\n",
    "M = first_arg.size()[0]\n",
    "R = init_registers.size()[0]\n",
    "N = 11\n",
    "\n",
    "# Turn the given tensors into matrices of one-hot vectors.\n",
    "init_registers = one_hotify(init_registers, M, 0)\n",
    "first_arg = one_hotify(first_arg, R, 1)\n",
    "second_arg = one_hotify(second_arg, R, 1)\n",
    "target = one_hotify(target, R, 1)\n",
    "instruction = one_hotify(instruction, N, 1)\n",
    "\n",
    "# Add a fake first batch\n",
    "init_registers = init_registers.unsqueeze(0)\n",
    "first_arg = first_arg.unsqueeze(0)\n",
    "second_arg = second_arg.unsqueeze(0)\n",
    "target = target.unsqueeze(0)\n",
    "instruction = instruction.unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# An example starting memory.  In this case, we're adding 3+4 and expect 7.\n",
    "initial_memory = torch.IntTensor([3,4,0,0,0,0,0,0])\n",
    "initial_memory = one_hotify(initial_memory, M, 0)\n",
    "output_memory = torch.DoubleTensor(M, M).zero_()\n",
    "output_memory[0][7] = 1\n",
    "\n",
    "# Output mask has ones in the rows of the memory matrix where the answer will be stored.\n",
    "output_mask = torch.DoubleTensor(M, M).zero_()\n",
    "output_mask[0] = torch.ones(M)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the addition task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            # An example starting memory.  In this case, we're adding 3+4 and expect 7.\n",
    "            first_addend = random.randint(0, M-1)\n",
    "            second_addend = random.randint(0, M-1)\n",
    "            initial_memory = torch.DoubleTensor(M, M).zero_()\n",
    "            initial_memory[0][first_addend] = 1\n",
    "            initial_memory[1][second_addend] = 1\n",
    "            for j in range(2, M):\n",
    "                initial_memory[j][0] = 1 #TODO: Un-for-loop this!\n",
    "\n",
    "            \n",
    "            output_memory = torch.DoubleTensor(M, M).zero_()\n",
    "            output_memory[0][(first_addend + second_addend) % M] = 1\n",
    "\n",
    "            # Output mask has ones in the rows of the memory matrix where the answer will be stored.\n",
    "            output_mask = torch.DoubleTensor(M, M).zero_()\n",
    "            output_mask[0] = torch.ones(M)\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "LR is set to 0.001\n",
      "Epoch Number: 0, Batch Number: 10, Training Loss: 1.5532\n",
      "Time so far is 0m 0s\n",
      "Epoch Number: 0, Batch Number: 20, Training Loss: 3.8509\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 0, Batch Number: 30, Training Loss: 1.0310\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 0, Batch Number: 40, Training Loss: 0.9294\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 0, Batch Number: 50, Training Loss: 0.9047\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 0, Batch Number: 60, Training Loss: 1.9193\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 0, Batch Number: 70, Training Loss: 0.7697\n",
      "Time so far is 0m 3s\n",
      "Epoch Number: 0, Batch Number: 80, Training Loss: 0.9337\n",
      "Time so far is 0m 3s\n",
      "Epoch Number: 0, Batch Number: 90, Training Loss: 0.8341\n",
      "Time so far is 0m 3s\n",
      "Epoch Number: 0, Batch Number: 100, Training Loss: 0.9485\n",
      "Time so far is 0m 4s\n",
      "\n",
      "Training complete in 0m 4s\n",
      "Best loss: 1.367453\n"
     ]
    }
   ],
   "source": [
    "M = 8 # Don't change this (as long as we're using the add-task)\n",
    "num_examples = 100\n",
    "\n",
    "dataset = AddTaskDataset(M, num_examples)\n",
    "data_loader = data.DataLoader(dataset, batch_size = 1) # Don't change this batch size.  You have been warned.\n",
    "\n",
    "def anc_validation_criterion(output, label):\n",
    "    target_memory = label[1]\n",
    "    target_mask = label[2]\n",
    "    \n",
    "    output = output.data * target_mask\n",
    "    target_memory = target_memory * target_mask\n",
    "    _, target_indices = torch.max(target_memory, 2)\n",
    "    _, output_indices = torch.max(output, 2)\n",
    "    \n",
    "    return 1 - torch.equal(output_indices, target_indices)\n",
    "\n",
    "# Initialize our controller\n",
    "controller = Controller(first_arg = first_arg, \n",
    "                        second_arg = second_arg, \n",
    "                        output = target, \n",
    "                        instruction = instruction, \n",
    "                        initial_registers = init_registers, \n",
    "                        stop_threshold = .9, \n",
    "                        blur = 5, \n",
    "                        multiplier = 10,\n",
    "                        correctness_weight = .75, \n",
    "                        halting_weight = .1, \n",
    "                        confidence_weight = .1, \n",
    "                        efficiency_weight = .05, \n",
    "                        t_max = 500) \n",
    "\n",
    "# Learning rate is a tunable hyperparameter\n",
    "# The paper didn't mention which one they used\n",
    "optimizer = optim.Adam(controller.parameters(), lr = 0.00001)\n",
    "\n",
    "plot_every = 10\n",
    "\n",
    "best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "    controller, \n",
    "    data_loader,  \n",
    "    optimizer, \n",
    "    num_epochs = 1, \n",
    "    print_every = 10, \n",
    "    plot_every = plot_every, \n",
    "    deep_copy_desired = False, \n",
    "    validation_criterion = anc_validation_criterion, \n",
    "    forward_train = True, \n",
    "    batch_size = 5) # In the paper, they used batch sizes of 1 or 5\n",
    "    \n",
    "    #kangaroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116530b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x * plot_every for x in range(len(train_plot_losses))], train_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116530048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x * plot_every for x in range(len(validation_plot_losses))], validation_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0155  0.0111  0.0133  0.0114  0.0138  0.0147  0.0036  0.0119\n",
      "  0.0155  0.0139  0.0140  0.0129  0.0139  0.0148  0.0051  0.0126\n",
      "  0.0049  0.0110  0.0149  0.0187  0.0141  0.9609  0.9793  0.0162\n",
      "  0.0124  0.9601  0.9596  0.9603  0.0138  0.0069 -0.0056 -0.0040\n",
      "  0.9667  0.0170  0.0158  0.0120  0.9600  0.0124 -0.0045 -0.0009\n",
      "  0.0144  0.0137  0.0144 -0.0055  0.0142  0.0144  0.0128  0.9711\n",
      "[torch.DoubleTensor of size 1x6x8]\n",
      "\n",
      "Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0134  0.0131  0.9746  0.0135  0.0148  0.0128  0.0121  0.0103\n",
      "  0.0042 -0.0002  0.0058  0.0138  0.0111  0.9598  0.0020  0.0108\n",
      "  0.0139  0.0116  0.0123  0.0140  0.0138  0.0129  0.0014  0.0136\n",
      " -0.0011 -0.0008 -0.0021  0.0117 -0.0019  0.0134 -0.0064 -0.0018\n",
      "  0.0126  0.0129 -0.0010  0.0185  0.0153  0.0088  0.9750  0.0181\n",
      "  0.9670  0.9608  0.0092  0.9557  0.9567  0.0136 -0.0040  0.9560\n",
      "[torch.DoubleTensor of size 1x6x8]\n",
      "\n",
      "Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0143  0.0055  0.0125 -0.0034  0.0126  0.0078  0.0022 -0.0011\n",
      "  0.0154  0.0133  0.0145  0.0130  0.0141  0.0151  0.0143  0.0138\n",
      "  0.0148  0.0124  0.0046  0.0140  0.0136  0.0003  0.0153  0.0140\n",
      "  0.0145  0.9627  0.0135  0.9611  0.0137  0.0138  0.0004  0.0008\n",
      "  0.9589  0.0129  0.0141  0.0133  0.9600  0.0136  0.0133 -0.0020\n",
      "  0.0090 -0.0008  0.9595  0.0112  0.0141  0.9601  0.9596  0.9737\n",
      "[torch.DoubleTensor of size 1x6x8]\n",
      "\n",
      "Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0148  0.0054  0.0043  0.0056  0.0132  0.0143 -0.0009  0.9425\n",
      "  0.0151  0.0053  0.0041  0.0057  0.0134  0.0145 -0.0013  0.0112\n",
      " -0.0010  0.0035 -0.0016 -0.0060  0.9297  0.0020 -0.0062 -0.0002\n",
      "  0.0151  0.0050  0.0041  0.0010  0.0137  0.0146 -0.0016  0.0113\n",
      "  0.0151  0.0050  0.0042  0.0010  0.0137  0.0146 -0.0016  0.0113\n",
      "  0.0131  0.0038  0.0005  0.9313  0.0137  0.0139 -0.0048 -0.0016\n",
      "  0.0151  0.0053  0.0041  0.0057  0.0134  0.0145 -0.0013  0.0112\n",
      "  0.0151  0.0050  0.0041  0.0010  0.0137  0.0146 -0.0016  0.0113\n",
      "  0.9355  0.9409  0.0122  0.0120  0.0135  0.0125  0.0106  0.0124\n",
      "  0.0161  0.0044  0.0140  0.0177  0.0135  0.0174  0.9481  0.0135\n",
      "  0.0052  0.0006  0.9357  0.0049  0.0131  0.9294 -0.0013  0.0075\n",
      "[torch.DoubleTensor of size 1x11x8]\n",
      "\n",
      "Parameter containing:\n",
      "(0 ,.,.) = \n",
      "  0.0067  0.0067  0.0067  0.0067  0.0067  0.0067  0.9553  0.0067\n",
      " -0.0005 -0.0001  0.9475 -0.0012 -0.0000  0.0022 -0.0006 -0.0014\n",
      "  0.9472 -0.0004 -0.0004 -0.0004 -0.0004 -0.0004 -0.0004 -0.0004\n",
      "  0.0067  0.9553  0.0067  0.0067  0.0067  0.0067  0.0067  0.0067\n",
      "  0.9474 -0.0021 -0.0008 -0.0008 -0.0008 -0.0008 -0.0008 -0.0008\n",
      "  0.9482 -0.0003 -0.0003 -0.0003 -0.0003 -0.0003 -0.0003 -0.0003\n",
      "[torch.DoubleTensor of size 1x6x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(controller.first_arg)\n",
    "print(controller.second_arg)\n",
    "print(controller.output)\n",
    "print(controller.instruction)\n",
    "print(controller.registers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
