{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/oliviawatkins/Documents/Schoolwork/NN/neural_nets_research\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets_library import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkProb(vec, dim, name):\n",
    "    # Get rid of batch dim\n",
    "    sums = torch.sum(vec, dim)\n",
    "    prob = torch.max(torch.abs(sums - 1)).data[0]\n",
    "    if not prob < .002:\n",
    "        print(\"BAD PROB\", prob, name, vec)\n",
    "    return prob < .002\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    Contains the two learnable parts of the model in four independent, fully connected layers.\n",
    "    First the initial values for the registers and instruction registers and second the \n",
    "    parameters that computes the required distributions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 first_arg = None, \n",
    "                 second_arg = None, \n",
    "                 output = None, \n",
    "                 instruction = None, \n",
    "                 initial_registers = None, \n",
    "                 stop_threshold = 1, \n",
    "                 multiplier = 5,\n",
    "                 correctness_weight = .2, \n",
    "                 halting_weight = .2, \n",
    "                 confidence_weight = .2, \n",
    "                 efficiency_weight = .4,\n",
    "                 t_max = 75):\n",
    "        \"\"\"\n",
    "        Initialize a bunch of constants and pass in matrices defining a program.\n",
    "        \n",
    "        :param first_arg: Matrix with the 1st register argument for each timestep stored in the columns (RxM)\n",
    "        :param second_arg: Matrix with the 2nd register argument for each timestep stored in the columns (RxM)\n",
    "        :param output: Matrix with the output register for each timestep stored in the columns (RxM)\n",
    "        :param instruction: Matrix with the instruction for each timestep stored in the columns (NxM)\n",
    "        :param initial_registers: Matrix where each row is a distribution over the value in one register (RxM)\n",
    "        :param stop_threshold: The stop probability threshold at which the controller should stop running\n",
    "        :param multiplier: The factor our one-hot vectors are be multiplied by before they're softmaxed to add blur\n",
    "        :param correctness_weight: Weight given to the correctness component of the loss function\n",
    "        :param halting_weight: Weight given to the halting component of the loss function\n",
    "        :param confidence_weight: Weight given to the confidence component of the loss function\n",
    "        :param efficiency_weight: Weight given to the efficiency component of the loss function\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Controller, self).__init__()\n",
    "        \n",
    "        # Initialize dimension constants\n",
    "        R, M = initial_registers.size()\n",
    "        self.M = M\n",
    "        self.R = R\n",
    "        self.times = []\n",
    "        \n",
    "        # Initialize loss function weights\n",
    "        # In the ANC paper, these scalars are called, alpha, beta, gamma, and delta\n",
    "        self.correctness_weight = correctness_weight\n",
    "        self.halting_weight = halting_weight\n",
    "        self.confidence_weight = confidence_weight\n",
    "        self.efficiency_weight = efficiency_weight\n",
    "        \n",
    "        # And yet more initialized constants... yeah, there are a bunch, I know.\n",
    "        self.t_max = t_max\n",
    "        self.stop_threshold = stop_threshold\n",
    "        self.multiplier = multiplier\n",
    "\n",
    "        # Initialize parameters.  These are the things that are going to be optimized. \n",
    "        self.first_arg = nn.Parameter(multiplier * first_arg)\n",
    "        self.second_arg = nn.Parameter(multiplier * second_arg)\n",
    "        self.output = nn.Parameter(multiplier * output)\n",
    "        self.instruction = nn.Parameter(multiplier * instruction) \n",
    "        self.registers = nn.Parameter(multiplier * initial_registers)\n",
    "        IR = torch.zeros(M)\n",
    "        IR[0] = 1\n",
    "        self.IR = nn.Parameter(multiplier * IR)\n",
    "                \n",
    "        # Machine initialization\n",
    "        self.machine = Machine(M, R)\n",
    "        self.softmax = nn.Softmax(0)\n",
    "    \n",
    "    \n",
    "    def forward(self, input, forward_train):\n",
    "        if forward_train:\n",
    "            return self.forward_train(input)\n",
    "        else:\n",
    "            return self.forward_predict(self, input)\n",
    "        \n",
    "    def forward_train(self, input, output):\n",
    "        \"\"\"\n",
    "        Runs the controller on a certain input memory matrix. It returns the loss.\n",
    "        \n",
    "        :param initial_memory: The state of memory at the beginning of the program.\n",
    "        :param output_meory: The desired state of memory at the end of the program.\n",
    "        :param output_mask: The parts of the output memory that are relevant.\n",
    "        \n",
    "        :return: Returns the training loss.\n",
    "        \"\"\"\n",
    "        initial_memory = input\n",
    "        output_memory = output[0]\n",
    "        output_mask = output[1]\n",
    "        # Program's initial memory #TODO: Variable?\n",
    "        \n",
    "        self.memory = Variable(initial_memory)\n",
    "        self.output_memory = Variable(output_memory)\n",
    "        self.output_mask = Variable(output_mask)\n",
    "        self.stop_probability = Variable(torch.zeros(1))\n",
    "        \n",
    "        # Copy registers so we aren't using the values from the previous iteration. Also\n",
    "        # make both registers and IR into a probability distribution.\n",
    "        registers = nn.Softmax(1)(self.registers)\n",
    "        IR = self.softmax(self.IR)\n",
    "        \n",
    "        # loss initialization\n",
    "        self.confidence = 0\n",
    "        self.efficiency = 0\n",
    "        self.halting = 0\n",
    "        self.correctness = 0\n",
    "        \n",
    "        t = 0 \n",
    "        # Run the program, one timestep at a time, until the program terminates or whe time out\n",
    "        while t < self.t_max and float(self.stop_probability) < self.stop_threshold: \n",
    "            \n",
    "            a = self.softmax(torch.matmul(self.first_arg, IR))\n",
    "            b = self.softmax(torch.matmul(self.second_arg, IR))\n",
    "            o = self.softmax(torch.matmul(self.output, IR))\n",
    "            e = self.softmax(torch.matmul(self.instruction, IR))\n",
    "                        \n",
    "            # Update memory, registers, and IR after machine operation\n",
    "            self.old_stop_probability = self.stop_probability\n",
    "            self.memory, registers, IR, new_stop_prob = self.machine(e, a, b, o, self.memory, registers, IR)\n",
    "            \n",
    "            self.stop_probability += new_stop_prob\n",
    "            self.timestep_loss(t)\n",
    "            t += 1\n",
    "        \n",
    "        self.final_loss(t)\n",
    "        self.times.append(t)\n",
    "#         return self.memory, self.total_loss()\n",
    "        return self.total_loss()\n",
    "    \n",
    "    \n",
    "    def test_train(self, initial_memory):\n",
    "        \"\"\"\n",
    "        Runs the controller on a certain input memory matrix. It returns the loss.\n",
    "        \n",
    "        :param initial_memory: The state of memory at the beginning of the program.\n",
    "        :param output_meory: The desired state of memory at the end of the program.\n",
    "        :param output_mask: The parts of the output memory that are relevant.\n",
    "        \n",
    "        :return: Returns the training loss.\n",
    "        \"\"\"\n",
    "        # Program's initial memory #TODO: Variable?\n",
    "        \n",
    "        self.memory = Variable(initial_memory)\n",
    "        self.stop_probability = Variable(torch.zeros(1))\n",
    "        \n",
    "        # Copy registers so we aren't using the values from the previous iteration. Also\n",
    "        # make both registers and IR into a probability distribution.\n",
    "        registers = nn.Softmax(1)(self.registers)\n",
    "        IR = self.softmax(self.IR)\n",
    "        \n",
    "        # loss initialization\n",
    "        self.confidence = 0\n",
    "        self.efficiency = 0\n",
    "        self.halting = 0\n",
    "        self.correctness = 0\n",
    "        \n",
    "        t = 0 \n",
    "        # Run the program, one timestep at a time, until the program terminates or whe time out\n",
    "        while t < self.t_max and float(self.stop_probability) < self.stop_threshold: \n",
    "            \n",
    "            a = self.softmax(torch.matmul(self.first_arg, IR))\n",
    "            b = self.softmax(torch.matmul(self.second_arg, IR))\n",
    "            o = self.softmax(torch.matmul(self.output, IR))\n",
    "            e = self.softmax(torch.matmul(self.instruction, IR))\n",
    "            \n",
    "            # Update memory, registers, and IR after machine operation\n",
    "            self.old_stop_probability = self.stop_probability\n",
    "            self.memory, registers, IR, new_stop_prob = self.machine(e, a, b, o, self.memory, registers, IR)\n",
    "            self.stop_probability += new_stop_prob\n",
    "\n",
    "            t += 1\n",
    "            print(\"INTERMED\", registers)\n",
    "        \n",
    "        self.times.append(t)\n",
    "        print(\"REGS\", registers)\n",
    "        print(\"T\", t)\n",
    "        return self.memory\n",
    "        \n",
    "    \n",
    "    def forward_prediction(self, input):\n",
    "        \"\"\"\n",
    "        Runs the controller on a certain input memory matrix. It returns the output memory matrix.\n",
    "        \n",
    "        :param initial_memory: The state of memory at the beginning of the program.\n",
    "        \n",
    "        :return: Returns the output memory matrix.\n",
    "        \"\"\"\n",
    "        memory = input[0]\n",
    "        # Program's initial memory\n",
    "        self.memory = memory\n",
    "        self.stop_probability = 0\n",
    "        \n",
    "        # Copy registers so we aren't using the values from the previous iteration. Also\n",
    "        # make both registers and IR into a probability distribution.\n",
    "        registers = nn.Softmax(1)(self.registers)\n",
    "        IR = self.softmax(self.IR)\n",
    "        \n",
    "        t = 0 \n",
    "        \n",
    "        # Run the program, one timestep at a time, until the program terminates or whe time out\n",
    "        while t < self.t_max and self.stop_probability < self.stop_threshold: \n",
    "            \n",
    "            \n",
    "            \n",
    "            a = self.softmax(torch.matmul(self.first_arg, IR))\n",
    "            b = self.softmax(torch.matmul(self.second_arg, IR))\n",
    "            o = self.softmax(torch.matmul(self.output, IR))\n",
    "            e = self.softmax(torch.matmul(self.instruction, IR))\n",
    "                        \n",
    "            # Update memory, registers, and IR after machine operation\n",
    "            self.old_stop_probability = self.stop_probability\n",
    "            self.memory, registers, IR, new_stop_prob = self.machine(e, a, b, o, self.memory, registers, IR) \n",
    "            \n",
    "            self.stop_probability += new_stop_prob\n",
    "            t += 1\n",
    "        \n",
    "        return self.memory, None\n",
    "    \n",
    "    def timestep_loss(self, t):\n",
    "        # Confidence Loss \n",
    "        mem_diff = self.output_memory - self.memory\n",
    "        correctness = torch.sum(self.output_mask * mem_diff * mem_diff)\n",
    "        self.confidence += (self.stop_probability - self.old_stop_probability) * correctness\n",
    "        \n",
    "        # Efficiency Loss\n",
    "        if float(self.stop_probability) < self.stop_threshold: # don't add efficiency loss if it stops\n",
    "            self.efficiency += (1 - self.stop_probability)\n",
    "            \n",
    "        \n",
    "            \n",
    "    \n",
    "    def final_loss(self, t):\n",
    "        # Correctness loss\n",
    "        mem_diff = self.output_memory - self.memory\n",
    "        self.correctness = torch.sum(self.output_mask * mem_diff * mem_diff)\n",
    "\n",
    "        # Halting loss\n",
    "        if t == self.t_max:\n",
    "            self.halting = (1 - self.stop_probability)\n",
    "\n",
    "    def total_loss(self):\n",
    "        \"\"\" compute four diferent loss functions and return a weighted average of the four measuring correctness, \n",
    "        halting, efficiency, and confidence\"\"\"\n",
    "        print(\"confidence\", float(self.confidence * self.confidence_weight))\n",
    "        print(\"efficiency\", float(self.efficiency * self.efficiency_weight))\n",
    "        print(\"halting\", float(self.halting * self.halting_weight))\n",
    "        print(\"correctness\", float(self.correctness * self.correctness_weight))\n",
    "        \n",
    "        return  (self.correctness*self.correctness_weight) + (self.confidence_weight*self.confidence) + (self.halting_weight*self.halting) + (self.efficiency_weight*self.efficiency)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(nn.Module):\n",
    "    \"\"\"\n",
    "    Parent class for our binary operations\n",
    "    \"\"\"\n",
    "    def __init__(self, M):\n",
    "        \"\"\"\n",
    "        Initialize the memory length (needed so we can mod our answer in case it exceeds the range 0-M-1)\n",
    "        Also calculate the output matrix for the operation\n",
    "        \n",
    "        :param M: Memory length\n",
    "        \"\"\"\n",
    "        super(Operation, self).__init__()\n",
    "        self.M = M\n",
    "        \n",
    "        # Create a MxMxM matrix where the (i,j,k) cell is 1 iff operation(i,j) = k.\n",
    "        self.outputs = torch.IntTensor(M, M, M).zero_()\n",
    "        for i in range(M):\n",
    "            for j in range(M):\n",
    "                val = self.compute(i, j)\n",
    "                self.outputs[i][j][val] = 1\n",
    "                \n",
    "        self.outputs = Variable(self.outputs)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        \"\"\" \n",
    "        Perform the binary operation.  The arguments may or may not be used.\n",
    "        \n",
    "        :param x: First argument\n",
    "        :param y: Second argument\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        :return: The output matrix\n",
    "        \"\"\"\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Operation):\n",
    "\n",
    "    def __init__(self, M):\n",
    "        super(Add, self).__init__(M)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        return (x + y) % self.M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stop(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Stop, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jump(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Jump, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0 # Actual jump happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decrement(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Decrement, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x - 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Increment(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Increment, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x + 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Max, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return max(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Min(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Min, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return min(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Read, self).__init__(M)\n",
    "        # Leave output matrix blank since we're gonna do the reading elsewhere\n",
    "        self.outputs = torch.zeros(M, M, M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return 0 # Actual reading happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtract(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Subtract, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return (x - y) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Write(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Write, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return 0 # Actual write happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zero(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Zero, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine(nn.Module):\n",
    "    \"\"\"\n",
    "    The Machine executes assembly instructions passed to it by the Controller.\n",
    "    It updates the given memory, registers, and instruction pointer.\n",
    "    The Machine doesn't have any learnable parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, M, R):\n",
    "        \"\"\"\n",
    "        Initializes dimensions, operations, and counters\n",
    "        \n",
    "        :param M: Memory length.  Integer values also take on values 0-M-1.  M is also the program length.\n",
    "        :param R: Number of registers\n",
    "        \"\"\"\n",
    "        super(Machine, self).__init__()\n",
    "        \n",
    "        # Store parameters as class variables\n",
    "        self.R = R # Number of registers\n",
    "        self.M = M # Memory length (also largest number)\n",
    "        \n",
    "        # Start off with 0 probability of stopping\n",
    "        self.stop_probability = 0 \n",
    "        \n",
    "        # List of ops (must be in same order as the original ANC paper so compilation works right)\n",
    "        self.ops = [ \n",
    "            Stop(M),\n",
    "            Zero(M),\n",
    "            Increment(M),\n",
    "            Add(M),\n",
    "            Subtract(M),\n",
    "            Decrement(M),\n",
    "            Min(M),\n",
    "            Max(M),\n",
    "            Read(M),\n",
    "            Write(M),\n",
    "            Jump(M)\n",
    "        ]\n",
    "        \n",
    "        # Number of instructions\n",
    "        self.N = len(self.ops)\n",
    "        \n",
    "        # Create a 4D matrix composed of the output matrices of each of the ops\n",
    "        self.outputs = Variable(torch.zeros(self.N, self.M, self.M, self.M))\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            op = self.ops[i]\n",
    "            self.outputs[i] = op()\n",
    "                \n",
    "        # Keep track of ops which will be handled specially\n",
    "        self.jump_index = 10\n",
    "        self.stop_index = 0\n",
    "        self.write_index = 9\n",
    "        self.read_index = 8 \n",
    "        \n",
    "    def forward(self, e, a, b, o, memory, registers, IR):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run the Machine for one timestep (corresponding to the execution of one line of Assembly).\n",
    "        The first four parameter names correspond to the vector names used in the original ANC paper\n",
    "        \n",
    "        :param e: Probability distribution over the instruction being executed (M)\n",
    "        :param a: Probability distribution over the first argument register (length R)\n",
    "        :param b: Probability distribution over the second argument register (length R)\n",
    "        :param o: Probability distribution over the first argument register (length R)\n",
    "        :param memory: Memory matrix (size MxM)\n",
    "        :param registers: Register matrix (size RxM)\n",
    "        :param IR: Instruction Register (length M)\n",
    "        \n",
    "        :return: The memory, registers, and instruction register after the timestep\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate distributions over the two argument values by multiplying each \n",
    "        # register by the probability that register is being used.\n",
    "        arg1 = torch.matmul(a, registers)\n",
    "        arg2 = torch.matmul(b, registers)\n",
    "        \n",
    "        # Multiply the output matrix by the arg1, arg2, and e vectors. Also take care\n",
    "        # of doing the read.\n",
    "        \n",
    "        arg1_long = arg1.view(1, -1, 1, 1)\n",
    "        arg2_long = arg2.view(1, 1, -1, 1)\n",
    "        instr = e.view(-1, 1, 1, 1)\n",
    "        read_vec =  e[self.read_index] * torch.matmul(arg1, memory)\n",
    "        out_vec = (self.outputs * arg1_long * arg2_long * instr).sum(0).sum(0).sum(0) + read_vec      \n",
    "        out_vec = out_vec.squeeze(0)\n",
    "    \n",
    "        # Update our memory, registers, instruction register, and stopping probability\n",
    "        memory = self.writeMemory(e, memory, arg1, arg2)\n",
    "        registers = self.writeRegisters(out_vec, o, registers)\n",
    "        IR = self.updateIR(e, IR, arg1, arg2)\n",
    "        stop_prob = self.getStop(e)\n",
    "        \n",
    "        return(memory, registers, IR, stop_prob)\n",
    "             \n",
    "    def writeRegisters(self, out, o, registers):\n",
    "        \"\"\"\n",
    "        Write the result of our operation to our registers.\n",
    "        \n",
    "        :param out: Probability distribution over the output value (M)\n",
    "        :param o: Probability distribution over the output register (R)\n",
    "        :param Registers: register matrix (RxM)\n",
    "        \n",
    "        :return: The updated registers (RxM)\n",
    "        \"\"\"\n",
    "        # Multiply probability of not writing with old registers and use an outer product\n",
    "        return (1 - o).unsqueeze(1) * registers + torch.ger(o, out)\n",
    "    \n",
    "    def updateIR(self, e, IR, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the instruction register\n",
    "        \n",
    "        :param e: Distribution over the current instruction (N)\n",
    "        :param IR: Instruction register (length M)\n",
    "        :param arg1: Distribution over the first argument value (length M)\n",
    "        :param arg2: Distribution over the second argument value (length M)\n",
    "        \n",
    "        :return: The updated instruction register (BxMx1)\n",
    "        \"\"\"\n",
    "        # probability of actually jumping\n",
    "        cond = e[self.jump_index] * arg1[0]\n",
    "        \n",
    "        # Take a weighted sum of the instruction register with and without jumping\n",
    "        return torch.cat([IR[-1], IR[:-1]], 0) * (1 - cond) + arg2 * cond\n",
    "    \n",
    "    def writeMemory(self, e, mem_orig, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the memory\n",
    "        \n",
    "        :param e: Distribution over the current instruction (M)\n",
    "        :param mem_orig: Current memory matrix (MxM)\n",
    "        :param arg1: Distribution over the first argument value (M)\n",
    "        :param arg2: Distribution over the second argument value (M)\n",
    "        \n",
    "        :return: The updated memory matrix (MxM)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Probability that we're on the write instruction\n",
    "        write_probability = e[self.write_index]\n",
    "        mem_write = torch.ger(arg1, arg2) \n",
    "        mem_write = mem_write + (1 - arg1).unsqueeze(1) * mem_orig\n",
    "        \n",
    "        return mem_orig * (1 - write_probability) + write_probability * mem_write\n",
    "\n",
    "    def getStop(self, e):\n",
    "        \"\"\"\n",
    "        Obtain the probability that we will stop at this timestep based on the probability that we are running the STOP op.\n",
    "        \n",
    "        :param e: distribution over the current instruction (length M)\n",
    "        \n",
    "        :return: probability representing whether the controller should stop.\n",
    "        \"\"\"\n",
    "        return e[self.stop_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hotify(vec, number_of_classes, dimension):\n",
    "    \"\"\"\n",
    "    Turn a tensor of integers into a matrix of one-hot vectors.\n",
    "    \n",
    "    :param vec: The vector to be converted.\n",
    "    :param number_of_classes: How many possible classes the one hot vectors encode.\n",
    "    :param dimension: Which dimension stores the elements of vec.  If 0, they're stored in the rows.  If 1, the columns.\n",
    "    \n",
    "    :return A matrix of one-hot vectors, each row or column corresponding to one element of vec\n",
    "    \"\"\"\n",
    "    num_vectors = vec.size()[0]\n",
    "    binary_vec = torch.zeros(num_vectors, number_of_classes)\n",
    "    for i in range(num_vectors):\n",
    "        binary_vec[i][vec[i]] = 1\n",
    "    if dimension == 1:\n",
    "        binary_vec.t_()\n",
    "    \n",
    "    return binary_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Addition task\n",
    "# # Generate this by running the instructions here (but with the addition program file): https://github.com/aditya-khant/neural-assembly-compiler\n",
    "# # Then get rid of the .cuda in each of the tensors since we (or at least I) don't have cuda\n",
    "# init_registers = torch.IntTensor([6,2,0,1,0,0]) # Length R, should be RxM\n",
    "# first_arg = torch.IntTensor([4,3,3,3,4,2,2,5]) # Length M, should be RxM\n",
    "# second_arg = torch.IntTensor([5,5,0,5,5,1,4,5]) # Length M, should be RxM\n",
    "# target = torch.IntTensor([4,3,5,3,4,5,5,5]) # Length M, should be RxM\n",
    "# instruction = torch.IntTensor([8,8,10,5,2,10,9,0]) # Length M, should be NxM\n",
    "\n",
    "# Increment task\n",
    "init_registers = torch.IntTensor([6,0,0,0,0,0,0])\n",
    "first_arg = torch.IntTensor([5,1,1,5,5,4,6])\n",
    "second_arg = torch.IntTensor([6,0,6,3,6,2,6])\n",
    "target = torch.IntTensor([1,6,3,6,5,6,6])\n",
    "instruction = torch.IntTensor([8,10,2,9,2,10,0])\n",
    "\n",
    "# # Access task\n",
    "# init_registers = torch.IntTensor([0,0,0])\n",
    "# first_arg = torch.IntTensor([0,1,1,0,2])\n",
    "# second_arg = torch.IntTensor([2,2,2,1,2])\n",
    "# target = torch.IntTensor([1,1,1,2,2])\n",
    "# instruction = torch.IntTensor([8,2,8,9,0])\n",
    "\n",
    "\n",
    "\n",
    "# Get dimensions we'll need\n",
    "M = first_arg.size()[0]\n",
    "R = init_registers.size()[0]\n",
    "N = 11\n",
    "\n",
    "# Turn the given tensors into matrices of one-hot vectors.\n",
    "init_registers = one_hotify(init_registers, M, 0)\n",
    "first_arg = one_hotify(first_arg, R, 1)\n",
    "second_arg = one_hotify(second_arg, R, 1)\n",
    "target = one_hotify(target, R, 1)\n",
    "instruction = one_hotify(instruction, N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the addition task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            first_addend = random.randint(0, M-1)\n",
    "            second_addend = random.randint(0, M-1)\n",
    "            initial_memory = torch.zeros(M, M)\n",
    "            initial_memory[0][first_addend] = 1\n",
    "            initial_memory[1][second_addend] = 1\n",
    "            for j in range(2, M):\n",
    "                initial_memory[j][0] = 1\n",
    "\n",
    "            \n",
    "            output_memory = torch.zeros(M, M)\n",
    "            output_memory[0][(first_addend + second_addend) % M] = 1\n",
    "\n",
    "            # Output mask has ones in the row of the memory matrix where the answer will be stored.\n",
    "            output_mask = torch.zeros(M, M)\n",
    "            output_mask[0] = torch.ones(M)\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialAddTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the addition task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            first_addend = random.randint(0, M-1)\n",
    "            second_addend = random.randint(0, M-1)\n",
    "            initial_memory = torch.FloatTensor(M, M).zero_()\n",
    "            initial_memory[0][first_addend] = 1\n",
    "            initial_memory[1][second_addend] = 1\n",
    "            for j in range(2, M):\n",
    "                initial_memory[j][0] = 1\n",
    "\n",
    "            \n",
    "            output_memory = torch.FloatTensor(M, M).zero_()\n",
    "            output_memory[0][(first_addend + second_addend) % M] = 1\n",
    "\n",
    "            # Output mask has ones in the rows of the memory matrix where the answer will be stored.\n",
    "            output_mask = torch.FloatTensor(M, M).zero_()\n",
    "            output_mask[2] = torch.ones(M)\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, list_len, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the list task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param list_len: The list length\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        if list_len > M:\n",
    "            raise ValueError(\"Cannot have a list longer than M\")\n",
    "        \n",
    "        self.input_list = []\n",
    "        self.output_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "#             list_val = random.randint(1, M-1)\n",
    "            list_val = i % M\n",
    "            initial_memory = torch.zeros(M, M)\n",
    "            output_memory = torch.zeros(M, M)\n",
    "            # Output mask is length of the list itself\n",
    "            output_mask = torch.zeros(M, M)\n",
    "            \n",
    "            for i in range(list_len):\n",
    "                initial_memory[i][list_val] = 1\n",
    "                output_memory[i][(list_val + 1 ) % M] = 1\n",
    "                output_mask[i] = torch.ones(M)\n",
    "                \n",
    "            for j in range(list_len, M):\n",
    "                initial_memory[j][0] = 1\n",
    "            \n",
    "#             self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "            self.input_list.append(initial_memory)\n",
    "            print(\"IM\", initial_memory)\n",
    "            self.output_list.append((output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i], self.output_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccessTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the access task by randomly generating an array.\n",
    "        The task is to access the 3rd element of the array\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            \n",
    "            initial_memory = torch.zeros(M, M)\n",
    "            output_memory = torch.zeros(M, M)\n",
    "            \n",
    "            # Set the initial memory\n",
    "            for i in range(1,M):\n",
    "                list_val = random.randint(0, M-1)\n",
    "                initial_memory[i][list_val] = 1\n",
    "                \n",
    "                if i == 4:\n",
    "                    output_memory[0, list_val] = 1\n",
    "            \n",
    "            # Get 3rd element of array\n",
    "            initial_memory[0, 3] = 1\n",
    "            \n",
    "            # Output mask is length of the list itself\n",
    "            output_mask = torch.zeros(M, M)\n",
    "            output_mask[0] = torch.ones(M)\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialAccessTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the access task by randomly generating an array.\n",
    "        The task is to access the 3rd element of the array\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            \n",
    "            initial_memory = torch.zeros(M, M)\n",
    "            output_memory = torch.zeros(M, M)\n",
    "            \n",
    "            # Set the initial memory\n",
    "            for i in range(1,M):\n",
    "                list_val = random.randint(0, M-1)\n",
    "                initial_memory[i][list_val] = 1\n",
    "                \n",
    "#                 if i == 4:\n",
    "#                     output_memory[0, list_val] = 1\n",
    "            \n",
    "            # Get 3rd element of array\n",
    "            initial_memory[0, 3] = 1\n",
    "            output_memory[0, 4] = 1\n",
    "            \n",
    "            # Output mask is length of the list itself\n",
    "            output_mask = torch.zeros(M, M)\n",
    "            output_mask[0] = torch.ones(M)\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IM \n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 7x7]\n",
      "\n",
      "IM \n",
      "    0     1     0     0     0     0     0\n",
      "    0     1     0     0     0     0     0\n",
      "    0     1     0     0     0     0     0\n",
      "    0     1     0     0     0     0     0\n",
      "    0     1     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 7x7]\n",
      "\n",
      "IM \n",
      "    0     0     1     0     0     0     0\n",
      "    0     0     1     0     0     0     0\n",
      "    0     0     1     0     0     0     0\n",
      "    0     0     1     0     0     0     0\n",
      "    0     0     1     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 7x7]\n",
      "\n",
      "IM \n",
      "    0     0     0     1     0     0     0\n",
      "    0     0     0     1     0     0     0\n",
      "    0     0     0     1     0     0     0\n",
      "    0     0     0     1     0     0     0\n",
      "    0     0     0     1     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 7x7]\n",
      "\n",
      "IM \n",
      "    0     0     0     0     1     0     0\n",
      "    0     0     0     0     1     0     0\n",
      "    0     0     0     0     1     0     0\n",
      "    0     0     0     0     1     0     0\n",
      "    0     0     0     0     1     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 7x7]\n",
      "\n",
      "IM \n",
      "    0     0     0     0     0     1     0\n",
      "    0     0     0     0     0     1     0\n",
      "    0     0     0     0     0     1     0\n",
      "    0     0     0     0     0     1     0\n",
      "    0     0     0     0     0     1     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 7x7]\n",
      "\n",
      "IM \n",
      "    0     0     0     0     0     0     1\n",
      "    0     0     0     0     0     0     1\n",
      "    0     0     0     0     0     0     1\n",
      "    0     0     0     0     0     0     1\n",
      "    0     0     0     0     0     0     1\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 7x7]\n",
      "\n",
      "IM \n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 7x7]\n",
      "\n",
      "IM \n",
      "    0     1     0     0     0     0     0\n",
      "    0     1     0     0     0     0     0\n",
      "    0     1     0     0     0     0     0\n",
      "    0     1     0     0     0     0     0\n",
      "    0     1     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 7x7]\n",
      "\n",
      "IM \n",
      "    0     0     1     0     0     0     0\n",
      "    0     0     1     0     0     0     0\n",
      "    0     0     1     0     0     0     0\n",
      "    0     0     1     0     0     0     0\n",
      "    0     0     1     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 7x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_examples = 10 #7200\n",
    "\n",
    "# M = 8 # Don't change this (as long as we're using the add-task)\n",
    "# dataset = AddTaskDataset(M, num_examples)\n",
    "# dataset = TrivialAddTaskDataset(M, num_examples)\n",
    "\n",
    "M = 7 # Don't change this (as long as we're using the inc-task)\n",
    "dataset = IncTaskDataset(M, 5, num_examples)\n",
    "\n",
    "# M = 5\n",
    "# dataset = AccessTaskDataset(M, num_examples)\n",
    "\n",
    "# M = 5\n",
    "# dataset = TrivialAccessTaskDataset(M, num_examples)\n",
    "\n",
    "data_loader = data.DataLoader(dataset, batch_size = 1) # Don't change this batch size.  You have been warned.\n",
    "\n",
    "def anc_validation_criterion(output, label):\n",
    "    initial_memory = label[0]\n",
    "    target_memory = label[1]\n",
    "    target_mask = label[2]\n",
    "    \n",
    "    output2 = output.data * target_mask\n",
    "    target_memory = target_memory * target_mask\n",
    "    _, initial_indices = torch.max(initial_memory, 2)\n",
    "    _, target_indices = torch.max(target_memory, 2)\n",
    "    _, output_indices = torch.max(output2, 2)\n",
    "    _, unmasked_indices = torch.max(output.data, 2)\n",
    "    return 1 - torch.equal(output_indices, target_indices)\n",
    "\n",
    "plot_every = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "LR is set to 0.005\n",
      "confidence 0.0\n",
      "efficiency 0.23650184273719788\n",
      "halting 0.0\n",
      "correctness 9.950525283813477\n",
      "confidence 0.0\n",
      "efficiency 1.0624462366104126\n",
      "halting 0.0\n",
      "correctness 7.775989055633545\n",
      "confidence 0.0\n",
      "efficiency 1.0632847547531128\n",
      "halting 0.0\n",
      "correctness 7.558948516845703\n",
      "confidence 0.0\n",
      "efficiency 1.061649203300476\n",
      "halting 0.0\n",
      "correctness 7.561375617980957\n",
      "confidence 0.0\n",
      "efficiency 1.051038146018982\n",
      "halting 0.0\n",
      "correctness 7.73557186126709\n",
      "confidence 0.0\n",
      "efficiency 1.0442863702774048\n",
      "halting 0.0\n",
      "correctness 7.881731986999512\n",
      "confidence 0.0\n",
      "efficiency 1.0539664030075073\n",
      "halting 0.0\n",
      "correctness 7.1672163009643555\n",
      "confidence 0.0\n",
      "efficiency 0.23617839813232422\n",
      "halting 0.0\n",
      "correctness 9.950095176696777\n",
      "confidence 0.0\n",
      "efficiency 1.0642060041427612\n",
      "halting 0.0\n",
      "correctness 7.754860877990723\n",
      "confidence 0.0\n",
      "efficiency 1.0651696920394897\n",
      "halting 0.0\n",
      "correctness 7.535272598266602\n",
      "Epoch Number: 0, Batch Number: 10, Training Loss: 8.9810\n",
      "Time so far is 0m 0s\n",
      "\n",
      "Training complete in 0m 0s\n",
      "Best loss: 8.981031\n"
     ]
    }
   ],
   "source": [
    "# Initialize our controller\n",
    "controller = Controller(first_arg = first_arg, \n",
    "                        second_arg = second_arg, \n",
    "                        output = target, \n",
    "                        instruction = instruction, \n",
    "                        initial_registers = init_registers, \n",
    "                        stop_threshold = .9, \n",
    "                        multiplier = 5,\n",
    "                        correctness_weight = 1, \n",
    "                        halting_weight = 5, \n",
    "                        efficiency_weight = 0.1, \n",
    "                        confidence_weight = 0.5, \n",
    "                        t_max = 50) \n",
    "\n",
    "# Learning rate is a tunable hyperparameter. The paper used 1 or 0.1.\n",
    "optimizer = optim.Adam(controller.parameters(), lr = 1)\n",
    "\n",
    "best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "    controller, \n",
    "    data_loader,  \n",
    "    optimizer, \n",
    "    num_epochs = 1, \n",
    "    print_every = 10, \n",
    "    plot_every = plot_every, \n",
    "    deep_copy_desired = False, \n",
    "#     validation_criterion = anc_validation_criterion, \n",
    "#     forward_train = True, \n",
    "    batch_size = 5) # In the paper, they used batch sizes of 1 or 5\n",
    "    \n",
    "    #kangaroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot([x * plot_every for x in range(len(train_plot_losses))], train_plot_losses)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(len(controller.times)), controller.times)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot([x * plot_every for x in range(len(validation_plot_losses))], validation_plot_losses)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# cutoff = 0.7\n",
    "\n",
    "# def getBest(vec):\n",
    "#     maxVal, index = torch.max(vec, 0)\n",
    "#     if maxVal.data[0] > cutoff:\n",
    "#         return index.data[0]\n",
    "\n",
    "# def bestRegister(vec):\n",
    "#     index = getBest(vec)\n",
    "#     if index is not None:\n",
    "#         return \"R\" + str(1 + index)\n",
    "#     return \"??\"\n",
    "    \n",
    "# def bestInstruction(vec):\n",
    "#     ops = [ \n",
    "#         \"STOP\",\n",
    "#         \"ZERO\",\n",
    "#         \"INC\",\n",
    "#         \"ADD\",\n",
    "#         \"SUB\",\n",
    "#         \"DEC\",\n",
    "#         \"MIN\",\n",
    "#         \"MAX\",\n",
    "#         \"READ\",\n",
    "#         \"WRITE\",\n",
    "#         \"JEZ\"\n",
    "#     ]\n",
    "#     index = getBest(vec)\n",
    "#     if index is not None:\n",
    "#         return ops[index]\n",
    "#     return \"??\"\n",
    "    \n",
    "# # registers = controller.registers\n",
    "\n",
    "# # # Add task\n",
    "# # orig_register = [6,2,0,1,0,0]\n",
    "# # orig_output = [4,3,5,3,4,5,5,5]\n",
    "# # orig_instruction = [8,8,10,5,2,10,9,0]\n",
    "# # orig_first = [4,3,3,3,4,2,2,5]\n",
    "# # orig_second = [5,5,0,5,5,1,4,5]\n",
    "# # orig_ir = [1,0,0,0,0,0,0,0]\n",
    "\n",
    "# # # INC Task\n",
    "# # orig_register = [6,0,0,0,0,0,0]\n",
    "# # orig_first = [5,1,1,5,5,4,6]\n",
    "# # orig_second = [6,0,6,3,6,2,6]\n",
    "# # orig_output = [1,6,3,6,5,6,6]\n",
    "# # orig_instruction = [8,10,2,9,2,10,0]\n",
    "\n",
    "# # Access Task\n",
    "# orig_register = [0,0,0]\n",
    "# orig_first = [0,1,1,0,2]\n",
    "# orig_second = [2,2,2,1,2]\n",
    "# orig_output = [1,1,1,2,2]\n",
    "# orig_instruction = [8,2,8,9,0]\n",
    "# orig_ir = [1,0,0,0,0]\n",
    "\n",
    "\n",
    "# R, M = controller.registers.size()\n",
    "    \n",
    "# def printProgram():   \n",
    "    \n",
    "#     print(\"IR = \" + str(getBest(controller.IR)))\n",
    "    \n",
    "#     # Print registers\n",
    "#     for i in range(R):\n",
    "#         print(\"R\" + str(i + 1) + \" = \" + str(getBest(controller.registers[i,:])))\n",
    "\n",
    "#     print()\n",
    "\n",
    "#     # Print the actual program\n",
    "#     for i in range (M):\n",
    "#         print(bestRegister(controller.output[:, i]) + \" = \" + \n",
    "#               bestInstruction(controller.instruction[:, i]) + \"(\" +\n",
    "#               bestRegister(controller.first_arg[:, i]) + \", \" +\n",
    "#               bestRegister(controller.second_arg[:, i]) + \")\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# def compareOutput():\n",
    "#     # compare our output to theirs\n",
    "#     # we get one point for every matching number\n",
    "#     match_count = 0\n",
    "#     softmax = nn.Softmax(0)\n",
    "#     for i in range(R):\n",
    "#         if getBest(nn.Softmax(1)(controller.registers)[i,:]) == orig_register[i]:\n",
    "#             match_count += 1\n",
    "#     for i in range (M):\n",
    "#         if getBest(softmax(controller.output)[:, i]) == orig_output[i]:\n",
    "#             match_count += 1\n",
    "#         if getBest(softmax(controller.instruction)[:, i]) == orig_instruction[i]:\n",
    "#             match_count += 1\n",
    "#         if getBest(softmax(controller.first_arg)[:, i]) == orig_first[i]:\n",
    "#             match_count += 1\n",
    "#         if getBest(softmax(controller.second_arg)[:, i]) == orig_second[i]:\n",
    "#             match_count += 1\n",
    "#     if getBest(softmax(controller.IR)) == orig_ir:\n",
    "#         match_count += 1\n",
    "\n",
    "#     percent_orig = match_count / (len(orig_register) + len(orig_output) + \n",
    "#                                            len(orig_instruction) + len(orig_first) + len(orig_second) + 1)\n",
    "#     return percent_orig\n",
    "#     print(\"PERCENT MATCH\", percent_orig)\n",
    "    \n",
    "# printProgram()\n",
    "# compareOutput()\n",
    "\n",
    "# # Original Add Program   \n",
    "# # R1 = 6\n",
    "# # R2 = 2\n",
    "# # R3 = 0\n",
    "# # R4 = 1\n",
    "# # R5 = 0\n",
    "# # R6 = 0\n",
    "\n",
    "\n",
    "# # R5 = READ(R5, R6)\n",
    "# # R4 = READ(R4, R6)\n",
    "# # R6 = JEZ(R4, R1)\n",
    "# # R4 = DEC(R4, R6)\n",
    "# # R5 = INC(R5, R6)\n",
    "# # R6 = JEZ(R3, R2)\n",
    "# # R6 = WRITE(R3, R5)\n",
    "# # R6 = STOP(R6, R6)\n",
    "\n",
    "# #koala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a bunch of times\n",
    "num_trials = 20\n",
    "\n",
    "num_original_convergences = 0\n",
    "num_0_losses = 0\n",
    "num_better_convergences = 0\n",
    "otherPrograms = []\n",
    "for i in range(num_trials):\n",
    "    print(\"Trial \", i)\n",
    "    best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "        controller, \n",
    "        data_loader,  \n",
    "        optimizer, \n",
    "        num_epochs = 15, \n",
    "        print_every = 50, \n",
    "        plot_every = plot_every, \n",
    "        deep_copy_desired = False, \n",
    "        validation_criterion = anc_validation_criterion, \n",
    "        forward_train = True, \n",
    "        batch_size = 5) # In the paper, they used batch sizes of 1 or 5\n",
    "    percent_orig = compareOutput()\n",
    "    if percent_orig > .99:\n",
    "        num_original_convergences += 1\n",
    "    end_losses = validation_plot_losses[-2:]\n",
    "    if sum(end_losses) < .01:\n",
    "        num_0_losses += 1\n",
    "    if percent_orig < .99 and sum(end_losses) < .01:\n",
    "        num_better_convergences += 1\n",
    "        otherPrograms.append((controller.output, controller.instruction, controller.first_arg, controller.second_arg, controller.registers))\n",
    "print(\"LOSS CONVERGENCES\", num_0_losses * 1.0 / num_trials)\n",
    "print(\"ORIG CONVERGENCES\", num_original_convergences * 1.0 / num_trials)\n",
    "print(\"BETTER CONVERGENCES\", num_better_convergences * 1.0 / num_trials)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(1)\n",
    "print(softmax(controller.instruction))\n",
    "print(controller.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printProgram()\n",
    "softmax = nn.Softmax(0)\n",
    "print(softmax(controller.output))\n",
    "print(softmax(controller.first_arg))\n",
    "print(softmax(controller.second_arg))\n",
    "print(softmax(controller.instruction))\n",
    "print(nn.Softmax(1)(controller.registers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IGNORE THIS... WORK IN PROGRESS\n",
    "# self.ops = [ \n",
    "#     0 Stop(M),\n",
    "#     1 Zero(M),\n",
    "#     2 Increment(M),\n",
    "#     3 Add(M),\n",
    "#     4 Subtract(M),\n",
    "#     5 Decrement(M),\n",
    "#     6 Min(M),\n",
    "#     7 Max(M),\n",
    "#     8 Read(M),\n",
    "#     9 Write(M),\n",
    "#     10 Jump(M)\n",
    "# ]\n",
    "N = 11\n",
    "\n",
    "\n",
    "# # Stop Test\n",
    "# M = 4\n",
    "# R = 3\n",
    "# init_registers = torch.IntTensor([0,0,0])\n",
    "# first_arg = torch.IntTensor([0,0,0,0])\n",
    "# second_arg = torch.IntTensor([0,0,0,0])\n",
    "# target = torch.IntTensor([0,0,0,0])\n",
    "# instruction =  torch.IntTensor([3, 0, 0, 0]) # OK\n",
    "\n",
    "\n",
    "# init_registers = one_hotify(init_registers, M, 0)\n",
    "# first_arg = one_hotify(first_arg, R, 1)\n",
    "# second_arg = one_hotify(second_arg, R, 1)\n",
    "# target = one_hotify(target, R, 1)\n",
    "# instruction = one_hotify(instruction, N, 1)\n",
    "# instruction[0,1] = 0.5\n",
    "# instruction[0,2] = 0.5\n",
    "# instruction[2,1] = 0.5\n",
    "# instruction[2,2] = 0.5\n",
    "\n",
    "# memory = torch.IntTensor([0,0,0,0])\n",
    "# memory = one_hotify(memory, M, 0)\n",
    "\n",
    "# # What we expect: stops after 3 iterations; reg should have  [0.5, 0.25, 0.25]\n",
    "\n",
    "\n",
    "# # Write test\n",
    "\n",
    "# M = 2\n",
    "# R = 3\n",
    "# init_registers = torch.IntTensor([1,1,0])\n",
    "# first_arg = torch.IntTensor([1,0])\n",
    "# second_arg = torch.IntTensor([0,0])\n",
    "# target = torch.IntTensor([0,0])\n",
    "# instruction =  torch.IntTensor([0,0]) #\n",
    "\n",
    "# init_registers = one_hotify(init_registers, M, 0)\n",
    "# first_arg = one_hotify(first_arg, R, 1)\n",
    "# second_arg = one_hotify(second_arg, R, 1)\n",
    "# target = one_hotify(target, R, 1)\n",
    "# instruction = one_hotify(instruction, N, 1)\n",
    "# instruction[0,0] = 0.5\n",
    "# instruction[9,0] = 0.5\n",
    "\n",
    "# memory = torch.IntTensor([0,0])\n",
    "# memory = one_hotify(memory, M, 0)\n",
    "\n",
    "# # What we expect: stops after 2 iterations; index 1 of memory should have value (0:.5; 1:.5)\n",
    "\n",
    "\n",
    "# # Read test\n",
    "\n",
    "# M = 2\n",
    "# R = 3\n",
    "# init_registers = torch.IntTensor([0,0,0])\n",
    "# first_arg = torch.IntTensor([0,0,0,0,0,0])\n",
    "# second_arg = torch.IntTensor([0,0,0,0,0,0])\n",
    "# target = torch.IntTensor([0,0,0,0,0,0])\n",
    "# instruction =  torch.IntTensor([0,10,0,0,2,0])\n",
    "\n",
    "# init_registers = one_hotify(init_registers, M, 0)\n",
    "# first_arg = one_hotify(first_arg, R, 1)\n",
    "# second_arg = one_hotify(second_arg, R, 1)\n",
    "# target = one_hotify(target, R, 1)\n",
    "# instruction = one_hotify(instruction, N, 1)\n",
    "# instruction[0,0] = 0.5\n",
    "# instruction[9,0] = 0.5\n",
    "\n",
    "# memory = torch.IntTensor([0,0])\n",
    "# memory = one_hotify(memory, M, 0)\n",
    "\n",
    "# # What we expect: stops after 2 iterations; index 1 of memory should have value (0:.5; 1:.5)\n",
    "\n",
    "\n",
    "# Normal ops test\n",
    "\n",
    "M = 5\n",
    "R = 3\n",
    "init_registers = torch.IntTensor([0,0,0])\n",
    "first_arg = torch.IntTensor([0,0,0,0,0])\n",
    "second_arg = torch.IntTensor([1,1,1,1,1])\n",
    "target = torch.IntTensor([0,0,0,0,0])\n",
    "instruction =  torch.IntTensor([1,3,4,7,0])\n",
    "\n",
    "init_registers = one_hotify(init_registers, M, 0)\n",
    "first_arg = one_hotify(first_arg, R, 1)\n",
    "second_arg = one_hotify(second_arg, R, 1)\n",
    "target = one_hotify(target, R, 1)\n",
    "instruction = one_hotify(instruction, N, 1)\n",
    "# zero, inc\n",
    "instruction[1,0] = 0.5\n",
    "instruction[2,0] = 0.5\n",
    "\n",
    "# add, dec\n",
    "instruction[3,1] = 0.5\n",
    "instruction[5,1] = 0.5\n",
    "\n",
    "# sub, min\n",
    "instruction[4,2] = 0.5\n",
    "instruction[6,2] = 0.5\n",
    "\n",
    "# max, write\n",
    "instruction[7,3] = 0.5\n",
    "instruction[9,3] = 0.5\n",
    "\n",
    "\n",
    "memory = torch.IntTensor([0,1,2,3,4])\n",
    "memory = one_hotify(memory, M, 0)\n",
    "\n",
    "# What we expect: stops after 2 iterations; index 1 of memory should have value (0:.5; 1:.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize our controller\n",
    "controller = Controller(first_arg = first_arg, \n",
    "                        second_arg = second_arg, \n",
    "                        output = target, \n",
    "                        instruction = instruction, \n",
    "                        initial_registers = init_registers, \n",
    "                        stop_threshold = .9, \n",
    "                        multiplier = 50,\n",
    "                        correctness_weight = 10, \n",
    "                        halting_weight = 0, \n",
    "                        efficiency_weight = 1, \n",
    "                        confidence_weight = 0, \n",
    "                        t_max = 50) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N=11\n",
    "\n",
    "\n",
    "# # AddTest\n",
    "# M = 3\n",
    "# R = 4\n",
    "\n",
    "# init_registers = torch.IntTensor([0,1,0,0])\n",
    "# first_arg = torch.IntTensor([2,0,3])\n",
    "# second_arg = torch.IntTensor([1,2,3])\n",
    "# target = torch.IntTensor([2,3,3])\n",
    "# instruction = torch.IntTensor([3,9,0])\n",
    "\n",
    "# # # DecTest\n",
    "# M = 3\n",
    "# R = 3\n",
    "# init_registers = torch.IntTensor([1,0,0])\n",
    "# first_arg = torch.IntTensor([0,1,2])\n",
    "# second_arg = torch.IntTensor([2,0,2])\n",
    "# target = torch.IntTensor([0,2,2])\n",
    "# instruction = torch.IntTensor([5,9,0])\n",
    "\n",
    "# # IncTest\n",
    "# M = 3\n",
    "# R = 3\n",
    "# init_registers = torch.IntTensor([0,0,0])\n",
    "# first_arg = torch.IntTensor([1,0,2])\n",
    "# second_arg = torch.IntTensor([2,1,2])\n",
    "# target = torch.IntTensor([1,2,2])\n",
    "# instruction = torch.IntTensor([2,9,0])\n",
    "\n",
    "# JezTest\n",
    "M = 3\n",
    "R = 3\n",
    "init_registers = torch.IntTensor([2,0,1,0])\n",
    "first_arg = torch.IntTensor([1,1,2,3])\n",
    "second_arg = torch.IntTensor([3,0,1,3])\n",
    "target = torch.IntTensor([1,3,3,3])\n",
    "instruction = torch.IntTensor([1,10,9,0])\n",
    "\n",
    "# # MaxTest\n",
    "# M = 3\n",
    "# R = 4\n",
    "# init_registers = torch.IntTensor([2,1,0,0])\n",
    "# first_arg = torch.IntTensor([1,2,3])\n",
    "# second_arg = torch.IntTensor([0,1,3])\n",
    "# target = torch.IntTensor([1,3,3])\n",
    "# instruction = torch.IntTensor([7,9,0])\n",
    "\n",
    "# # MinTest\n",
    "# M = 3\n",
    "# R = 4\n",
    "# init_registers = torch.IntTensor([1,2,0,0])\n",
    "# first_arg = torch.IntTensor([1,2,3])\n",
    "# second_arg = torch.IntTensor([0,1,3])\n",
    "# target = torch.IntTensor([1,3,3])\n",
    "# instruction = torch.IntTensor([6,9,0])\n",
    "\n",
    "# ReadTest\n",
    "# M=3\n",
    "# R=3\n",
    "# init_registers = torch.IntTensor([0,0,0])\n",
    "# first_arg = torch.IntTensor([0,1,2])\n",
    "# second_arg = torch.IntTensor([2,0,2])\n",
    "# target = torch.IntTensor([0,2,2])\n",
    "# instruction = torch.IntTensor([8,9,0])\n",
    "\n",
    "# # SubTest\n",
    "# M = 3\n",
    "# R = 4\n",
    "# init_registers = torch.IntTensor([1,2,0,0])\n",
    "# first_arg = torch.IntTensor([1,2,3])\n",
    "# second_arg = torch.IntTensor([0,1,3])\n",
    "# target = torch.IntTensor([1,3,3])\n",
    "# instruction = torch.IntTensor([4,9,0])\n",
    "\n",
    "# # WriteTest\n",
    "# M = 2\n",
    "# R = 3\n",
    "# init_registers = torch.IntTensor([1,0,0])\n",
    "# first_arg = torch.IntTensor([1,2])\n",
    "# second_arg = torch.IntTensor([0,2])\n",
    "# target = torch.IntTensor([2,2])\n",
    "# instruction = torch.IntTensor([9,0])\n",
    "\n",
    "# ZeroTest\n",
    "M = 3\n",
    "R = 3\n",
    "init_registers = torch.IntTensor([0,1,0])\n",
    "first_arg = torch.IntTensor([1,0,2])\n",
    "second_arg = torch.IntTensor([2,1,2])\n",
    "target = torch.IntTensor([1,2,2])\n",
    "instruction = torch.IntTensor([1,9,0])\n",
    "\n",
    "init_registers = one_hotify(init_registers, M, 0)\n",
    "first_arg = one_hotify(first_arg, R, 1)\n",
    "second_arg = one_hotify(second_arg, R, 1)\n",
    "target = one_hotify(target, R, 1)\n",
    "instruction = one_hotify(instruction, N, 1)\n",
    "initial_memory = torch.zeros(M,M)\n",
    "initial_memory[:, 2] = 1\n",
    "# initial_memory[0,0] = 0\n",
    "# initial_memory[0,2] = 1\n",
    "                                    \n",
    "                                    \n",
    "controller = Controller(first_arg = first_arg, \n",
    "                        second_arg = second_arg, \n",
    "                        output = target, \n",
    "                        instruction = instruction, \n",
    "                        initial_registers = init_registers, \n",
    "                        stop_threshold = .9, \n",
    "                        multiplier = 50,\n",
    "                        correctness_weight = 10, \n",
    "                        halting_weight = 0, \n",
    "                        efficiency_weight = 1, \n",
    "                        confidence_weight = 0, \n",
    "                        t_max = 5) \n",
    "print(controller.forward_train(initial_memory, initial_memory, initial_memory))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
