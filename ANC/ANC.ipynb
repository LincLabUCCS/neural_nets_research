{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets_library import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkProb(vec, dim, name):\n",
    "    # Get rid of batch dim\n",
    "    vec = torch.squeeze(vec, 0)\n",
    "    sums = torch.sum(vec, dim)\n",
    "    size = sums.size()\n",
    "    prob = abs(torch.sum(sums - 1).data[0])\n",
    "    if not prob < .001:\n",
    "        print(\"BAD PROB\", prob, name, vec)\n",
    "    return prob < .001\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    Contains the two learnable parts of the model in four independent, fully connected layers.\n",
    "    First the initial values for the registers and instruction registers and second the \n",
    "    parameters that computes the required distributions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 first_arg = None, \n",
    "                 second_arg = None, \n",
    "                 output = None, \n",
    "                 instruction = None, \n",
    "                 initial_registers = None, \n",
    "                 stop_threshold = 1, \n",
    "                 multiplier = 5,\n",
    "                 correctness_weight = .2, \n",
    "                 halting_weight = .2, \n",
    "                 confidence_weight = .2, \n",
    "                 efficiency_weight = .4,\n",
    "                 t_max = 75):\n",
    "        \"\"\"\n",
    "        Initialize a bunch of constants and pass in matrices defining a program.\n",
    "        \n",
    "        :param first_arg: Matrix with the 1st register argument for each timestep stored in the columns (RxM)\n",
    "        :param second_arg: Matrix with the 2nd register argument for each timestep stored in the columns (RxM)\n",
    "        :param output: Matrix with the output register for each timestep stored in the columns (RxM)\n",
    "        :param instruction: Matrix with the instruction for each timestep stored in the columns (NxM)\n",
    "        :param initial_registers: Matrix where each row is a distribution over the value in one register (RxM)\n",
    "        :param stop_threshold: The stop probability threshold at which the controller should stop running\n",
    "        :param multiplier: The factor our one-hot vectors are be multiplied by before they're softmaxed to add blur\n",
    "        :param correctness_weight: Weight given to the correctness component of the loss function\n",
    "        :param halting_weight: Weight given to the halting component of the loss function\n",
    "        :param confidence_weight: Weight given to the confidence component of the loss function\n",
    "        :param efficiency_weight: Weight given to the efficiency component of the loss function\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Controller, self).__init__()\n",
    "        \n",
    "        # Initialize dimension constants\n",
    "        R, M = initial_registers.size()\n",
    "        self.M = M\n",
    "        self.R = R\n",
    "        \n",
    "        # Initialize loss function weights\n",
    "        # In the ANC paper, these scalars are called, alpha, beta, gamma, and delta\n",
    "        self.correctness_weight = correctness_weight\n",
    "        self.halting_weight = halting_weight\n",
    "        self.confidence_weight = confidence_weight\n",
    "        self.efficiency_weight = efficiency_weight\n",
    "        \n",
    "        # And yet more initialized constants... yeah, there are a bunch, I know.\n",
    "        self.t_max = t_max\n",
    "        self.stop_threshold = stop_threshold\n",
    "        self.multiplier = multiplier\n",
    "\n",
    "        # Initialize parameters.  These are the things that are going to be optimized. \n",
    "        self.first_arg = nn.Parameter(multiplier * first_arg)\n",
    "        self.second_arg = nn.Parameter(multiplier * second_arg)\n",
    "        self.output = nn.Parameter(multiplier * output)\n",
    "        self.instruction = nn.Parameter(multiplier * instruction) \n",
    "        self.registers = nn.Parameter(multiplier * initial_registers)\n",
    "        IR = torch.zeros(M)\n",
    "        IR[0] = 1\n",
    "        self.IR = nn.Parameter(multiplier * IR)\n",
    "                \n",
    "        # Machine initialization\n",
    "        self.machine = Machine(M, R)\n",
    "        self.softmax = nn.Softmax(0)\n",
    "    \n",
    "    \n",
    "    def forward(self, input, forward_train):\n",
    "        if forward_train:\n",
    "            return self.forward_train(input[0], input[1], input[2])\n",
    "        else:\n",
    "            return self.forward_predict(self, input[0])\n",
    "        \n",
    "    def forward_train(self, initial_memory, output_memory, output_mask):\n",
    "        \"\"\"\n",
    "        Runs the controller on a certain input memory matrix. It returns the loss.\n",
    "        \n",
    "        :param initial_memory: The state of memory at the beginning of the program.\n",
    "        :param output_meory: The desired state of memory at the end of the program.\n",
    "        :param output_mask: The parts of the output memory that are relevant.\n",
    "        \n",
    "        :return: Returns the training loss.\n",
    "        \"\"\"\n",
    "        # Program's initial memory #TODO: Variable?\n",
    "        \n",
    "        self.memory = Variable(initial_memory)\n",
    "        self.output_memory = Variable(output_memory)\n",
    "        self.output_mask = Variable(output_mask)\n",
    "        self.stop_probability = Variable(torch.zeros(1))\n",
    "        \n",
    "        \n",
    "        print(\"MEM\", self.memory)\n",
    "        print(\"OUT\", self.output_memory)\n",
    "        print(\"OUT MASK\", self.output_mask)\n",
    "        print(\"STOP\", self.stop_probability)\n",
    "        \n",
    "        # Copy registers so we aren't using the values from the previous iteration. Also\n",
    "        # make both registers and IR into a probability distribution.\n",
    "        registers = nn.Softmax(1)(self.registers)\n",
    "        IR = self.softmax(self.IR)\n",
    "        \n",
    "        # loss initialization\n",
    "        self.confidence = 0\n",
    "        self.efficiency = 0\n",
    "        self.halting = 0\n",
    "        self.correctness = 0\n",
    "        \n",
    "        t = 0 \n",
    "        print(\"FIRST\", t, self.t_max)\n",
    "        print(torch.max(self.stop_probability))\n",
    "        print(self.stop_threshold)\n",
    "            \n",
    "        #TODO: AAAAA\n",
    "        # Run the program, one timestep at a time, until the program terminates or whe time out\n",
    "        while t < self.t_max and self.stop_probability.data[0] > self.stop_threshold: \n",
    "            a = self.softmax(torch.matmul(self.first_arg, IR))\n",
    "            b = self.softmax(torch.matmul(self.second_arg, IR))\n",
    "            o = self.softmax(torch.matmul(self.output, IR))\n",
    "            e = self.softmax(torch.matmul(self.instruction, IR))\n",
    "                        \n",
    "            # Update memory, registers, and IR after machine operation\n",
    "            self.old_stop_probability = self.stop_probability\n",
    "            self.memory, registers, IR, new_stop_prob = self.machine(e, a, b, o, self.memory, registers, IR) \n",
    "            \n",
    "            self.stop_probability += new_stop_prob\n",
    "            self.timestep_loss(t)\n",
    "            t += 1\n",
    "        \n",
    "        self.final_loss(t)\n",
    "        print(\"TOTAL LOSSS TYPE, \", self.total_loss())\n",
    "        return self.memory, self.total_loss()\n",
    "        \n",
    "    \n",
    "    def forward_prediction(self, memory):\n",
    "        \"\"\"\n",
    "        Runs the controller on a certain input memory matrix. It returns the output memory matrix.\n",
    "        \n",
    "        :param initial_memory: The state of memory at the beginning of the program.\n",
    "        \n",
    "        :return: Returns the output memory matrix.\n",
    "        \"\"\"\n",
    "        # Program's initial memory\n",
    "        self.memory = initial_memory\n",
    "        self.stop_probability = 0\n",
    "        \n",
    "        # Copy registers so we aren't using the values from the previous iteration. Also\n",
    "        # make both registers and IR into a probability distribution.\n",
    "        registers = nn.Softmax(1)(self.registers)\n",
    "        IR = self.softmax(self.IR)\n",
    "        \n",
    "        t = 0 \n",
    "        # Run the program, one timestep at a time, until the program terminates or whe time out\n",
    "        while t < self.t_max and self.stop_probability < self.stop_threshold: \n",
    "            a = self.softmax(torch.matmul(self.first_arg, IR))\n",
    "            b = self.softmax(torch.matmul(self.second_arg, IR))\n",
    "            o = self.softmax(torch.matmul(self.output, IR))\n",
    "            e = self.softmax(torch.matmul(self.instruction, IR))\n",
    "                        \n",
    "            # Update memory, registers, and IR after machine operation\n",
    "            self.old_stop_probability = self.stop_probability\n",
    "            self.memory, registers, IR, new_stop_prob = self.machine(e, a, b, o, self.memory, registers, IR) \n",
    "            \n",
    "            self.stop_probability += new_stop_prob\n",
    "            t += 1\n",
    "        \n",
    "        return self.memory, None\n",
    "    \n",
    "    def timestep_loss(self, t):\n",
    "        # Confidence Loss \n",
    "        neg_mem = -1 * self.memory\n",
    "        mem_diff = self.output_memory - self.memory\n",
    "        correctness = torch.sum(self.output_mask * mem_diff * mem_diff)\n",
    "        self.confidence += (self.stop_probability - self.old_stop_probability) * correctness\n",
    "        \n",
    "        # Efficiency Loss\n",
    "        if self.stop_probability < self.stop_threshold: # don't add efficiency loss if it stops\n",
    "            self.efficiency += (1 - self.stop_probability)\n",
    "            \n",
    "        \n",
    "            \n",
    "    \n",
    "    def final_loss(self, t):\n",
    "        # Correctness loss\n",
    "        mem_diff = self.output_memory - self.memory #TODO: This Variable-izing here seems really sketch\n",
    "        print(\"MEM DIFF\", type(mem_diff))\n",
    "        self.correctness = torch.sum(self.output_mask * mem_diff * mem_diff)\n",
    "\n",
    "        # Halting loss\n",
    "        if t == self.t_max:\n",
    "            self.halting = (1 - self.stop_probability)\n",
    "            \n",
    "        print(\"T FINAL\", t)\n",
    "#         print(\"FINAL MEM\", t, self.memory)\n",
    "            \n",
    "\n",
    "    def total_loss(self):\n",
    "        \"\"\" compute four diferent loss functions and return a weighted average of the four measuring correctness, \n",
    "        halting, efficiency, and confidence\"\"\"\n",
    "        return  (self.correctness*self.correctness_weight) + (self.confidence_weight*self.confidence) + (self.halting_weight*self.halting) + (self.efficiency_weight*self.efficiency)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(nn.Module):\n",
    "    \"\"\"\n",
    "    Parent class for our binary operations\n",
    "    \"\"\"\n",
    "    def __init__(self, M):\n",
    "        \"\"\"\n",
    "        Initialize the memory length (needed so we can mod our answer in case it exceeds the range 0-M-1)\n",
    "        Also calculate the output matrix for the operation\n",
    "        \n",
    "        :param M: Memory length\n",
    "        \"\"\"\n",
    "        super(Operation, self).__init__()\n",
    "        self.M = M\n",
    "        \n",
    "        # Create a MxMxM matrix where the (i,j,k) cell is 1 iff operation(i,j) = k.\n",
    "        self.outputs = torch.IntTensor(M, M, M).zero_()\n",
    "        for i in range(M):\n",
    "            for j in range(M):\n",
    "                val = self.compute(i, j)\n",
    "                self.outputs[val][i][j] = 1\n",
    "                \n",
    "        self.outputs = Variable(self.outputs)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        \"\"\" \n",
    "        Perform the binary operation.  The arguments may or may not be used.\n",
    "        \n",
    "        :param x: First argument\n",
    "        :param y: Second argument\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        :return: The output matrix\n",
    "        \"\"\"\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Operation):\n",
    "\n",
    "    def __init__(self, M):\n",
    "        super(Add, self).__init__(M)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        return (x + y) % self.M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stop(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Stop, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jump(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Jump, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0 # Actual jump happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decrement(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Decrement, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x - 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Increment(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Increment, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x + 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Max, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return max(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Min(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Min, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return min(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Read, self).__init__(M)\n",
    "        # Leave output matrix blank since we're gonna do the reading elsewhere\n",
    "        self.outputs = torch.zeros(M, M, M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return 0 # Actual reading happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtract(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Subtract, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return (x - y) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Write(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Write, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return 0 # Actual write happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zero(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Zero, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine(nn.Module):\n",
    "    \"\"\"\n",
    "    The Machine executes assembly instructions passed to it by the Controller.\n",
    "    It updates the given memory, registers, and instruction pointer.\n",
    "    The Machine doesn't have any learnable parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, M, R):\n",
    "        \"\"\"\n",
    "        Initializes dimensions, operations, and counters\n",
    "        \n",
    "        :param M: Memory length.  Integer values also take on values 0-M-1.  M is also the program length.\n",
    "        :param R: Number of registers\n",
    "        \"\"\"\n",
    "        super(Machine, self).__init__()\n",
    "        \n",
    "        # Store parameters as class variables\n",
    "        self.R = R # Number of registers\n",
    "        self.M = M # Memory length (also largest number)\n",
    "        \n",
    "        # Start off with 0 probability of stopping\n",
    "        self.stop_probability = 0 \n",
    "        \n",
    "        # List of ops (must be in same order as the original ANC paper so compilation works right)\n",
    "        self.ops = [ \n",
    "            Stop(M),\n",
    "            Zero(M),\n",
    "            Increment(M),\n",
    "            Add(M),\n",
    "            Subtract(M),\n",
    "            Decrement(M),\n",
    "            Min(M),\n",
    "            Max(M),\n",
    "            Read(M),\n",
    "            Write(M),\n",
    "            Jump(M)\n",
    "        ]\n",
    "        \n",
    "        # Number of instructions\n",
    "        self.N = len(self.ops)\n",
    "        \n",
    "        # Create a 4D matrix composed of the output matrices of each of the ops\n",
    "        self.outputs = Variable(torch.zeros(self.N, self.M, self.M, self.M))\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            op = self.ops[i]\n",
    "            self.outputs[i] = op()\n",
    "                \n",
    "        # Keep track of ops which will be handled specially\n",
    "        self.jump_index = 10\n",
    "        self.stop_index = 0\n",
    "        self.write_index = 9\n",
    "        self.read_index = 8 \n",
    "        \n",
    "    def forward(self, e, a, b, o, memory, registers, IR):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run the Machine for one timestep (corresponding to the execution of one line of Assembly).\n",
    "        The first four parameter names correspond to the vector names used in the original ANC paper\n",
    "        \n",
    "        :param e: Probability distribution over the instruction being executed (M)\n",
    "        :param a: Probability distribution over the first argument register (length R)\n",
    "        :param b: Probability distribution over the second argument register (length R)\n",
    "        :param o: Probability distribution over the first argument register (length R)\n",
    "        :param memory: Memory matrix (size MxM)\n",
    "        :param registers: Register matrix (size RxM)\n",
    "        :param IR: Instruction Register (length M)\n",
    "        \n",
    "        :return: The memory, registers, and instruction register after the timestep\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate distributions over the two argument values by multiplying each \n",
    "        # register by the probability that register is being used.\n",
    "        arg1 = torch.matmul(a, registers)\n",
    "        arg2 = torch.matmul(b, registers)\n",
    "        \n",
    "        # Multiply the output matrix by the arg1, arg2, and e vectors. Also take care\n",
    "        # of doing the read.\n",
    "        \n",
    "        arg1_long = arg1.view(1, -1, 1, 1)\n",
    "        arg2_long = arg2.view(1, 1, -1, 1)\n",
    "        instr = e.view(-1, 1, 1, 1)\n",
    "        read_vec =  e[self.read_index] * torch.matmul(arg1, memory)\n",
    "        out_vec = (self.outputs * arg1_long * arg2_long * instr).sum(0).sum(0).sum(0) + read_vec      \n",
    "        out_vec = out_vec.squeeze(0)\n",
    "    \n",
    "        # Update our memory, registers, instruction register, and stopping probability\n",
    "        memory = self.writeMemory(e, memory, arg1, arg2)\n",
    "        registers = self.writeRegisters(out_vec, o, registers)\n",
    "        IR = self.updateIR(e, IR, arg1, arg2)\n",
    "        stop_prob = self.getStop(e)\n",
    "        \n",
    "        return(memory, registers, IR, stop_prob)\n",
    "             \n",
    "    def writeRegisters(self, out, o, registers):\n",
    "        \"\"\"\n",
    "        Write the result of our operation to our registers.\n",
    "        \n",
    "        :param out: Probability distribution over the output value (M)\n",
    "        :param o: Probability distribution over the output register (R)\n",
    "        :param Registers: register matrix (RxM)\n",
    "        \n",
    "        :return: The updated registers (RxM)\n",
    "        \"\"\"\n",
    "        # Multiply probability of not writing with old registers and use an outer product\n",
    "        # to find the probability of the new register values.\n",
    "        return (1 - o).unsqueeze(1) * registers + torch.ger(out, o)\n",
    "    \n",
    "    def updateIR(self, e, IR, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the instruction register\n",
    "        \n",
    "        :param e: Distribution over the current instruction (N)\n",
    "        :param IR: Instruction register (length M)\n",
    "        :param arg1: Distribution over the first argument value (length M)\n",
    "        :param arg2: Distribution over the second argument value (length M)\n",
    "        \n",
    "        :return: The updated instruction register (BxMx1)\n",
    "        \"\"\"\n",
    "        # probability of actually jumping\n",
    "        cond = e[self.jump_index] * arg1[0]\n",
    "        \n",
    "        # Take a weighted sum of the instruction register with and without jumping\n",
    "        return torch.cat([IR[-1], IR[:-1]], 0) * (1 - cond) + arg2 * cond\n",
    "    \n",
    "    def writeMemory(self, e, mem_orig, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the memory\n",
    "        \n",
    "        :param e: Distribution over the current instruction (M)\n",
    "        :param mem_orig: Current memory matrix (MxM)\n",
    "        :param arg1: Distribution over the first argument value (M)\n",
    "        :param arg2: Distribution over the second argument value (M)\n",
    "        \n",
    "        :return: The updated memory matrix (MxM)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Probability that we're on the write instruction\n",
    "        write_probability = e[self.write_index]\n",
    "        mem_write = torch.ger(arg1, arg2) \n",
    "        mem_write = mem_write + (1 - arg1).unsqueeze(1) * mem_orig\n",
    "        \n",
    "        return mem_orig * (1 - write_probability) + write_probability * mem_write\n",
    "\n",
    "    def getStop(self, e):\n",
    "        \"\"\"\n",
    "        Obtain the probability that we will stop at this timestep based on the probability that we are running the STOP op.\n",
    "        \n",
    "        :param e: distribution over the current instruction (length M)\n",
    "        \n",
    "        :return: probability representing whether the controller should stop.\n",
    "        \"\"\"\n",
    "        return e[self.stop_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hotify(vec, number_of_classes, dimension):\n",
    "    \"\"\"\n",
    "    Turn a tensor of integers into a matrix of one-hot vectors.\n",
    "    \n",
    "    :param vec: The vector to be converted.\n",
    "    :param number_of_classes: How many possible classes the one hot vectors encode.\n",
    "    :param dimension: Which dimension stores the elements of vec.  If 0, they're stored in the rows.  If 1, the columns.\n",
    "    \n",
    "    :return A matrix of one-hot vectors, each row or column corresponding to one element of vec\n",
    "    \"\"\"\n",
    "    num_vectors = vec.size()[0]\n",
    "    binary_vec = torch.zeros(num_vectors, number_of_classes)\n",
    "    for i in range(num_vectors):\n",
    "        binary_vec[i][vec[i]] = 1\n",
    "    if dimension == 1:\n",
    "        binary_vec.t_()\n",
    "    \n",
    "    return binary_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Addition task\n",
    "# # Generate this by running the instructions here (but with the addition program file): https://github.com/aditya-khant/neural-assembly-compiler\n",
    "# # Then get rid of the .cuda in each of the tensors since we (or at least I) don't have cuda\n",
    "# init_registers = torch.IntTensor([6,2,0,1,0,0]) # Length R, should be RxM\n",
    "# first_arg = torch.IntTensor([4,3,3,3,4,2,2,5]) # Length M, should be RxM\n",
    "# second_arg = torch.IntTensor([5,5,0,5,5,1,4,5]) # Length M, should be RxM\n",
    "# target = torch.IntTensor([4,3,5,3,4,5,5,5]) # Length M, should be RxM\n",
    "# instruction = torch.IntTensor([8,8,10,5,2,10,9,0]) # Length M, should be NxM\n",
    "\n",
    "# Increment task\n",
    "init_registers = torch.IntTensor([6,0,0,0,0,0,0])\n",
    "first_arg = torch.IntTensor([5,1,1,5,5,4,6])\n",
    "second_arg = torch.IntTensor([6,0,6,3,6,2,6])\n",
    "target = torch.IntTensor([1,6,3,6,5,6,6])\n",
    "instruction = torch.IntTensor([8,10,2,9,2,10,0])\n",
    "\n",
    "\n",
    "\n",
    "# Get dimensions we'll need\n",
    "M = first_arg.size()[0]\n",
    "R = init_registers.size()[0]\n",
    "N = 11\n",
    "\n",
    "# Turn the given tensors into matrices of one-hot vectors.\n",
    "init_registers = one_hotify(init_registers, M, 0)\n",
    "first_arg = one_hotify(first_arg, R, 1)\n",
    "second_arg = one_hotify(second_arg, R, 1)\n",
    "target = one_hotify(target, R, 1)\n",
    "instruction = one_hotify(instruction, N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the addition task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            first_addend = random.randint(0, M-1)\n",
    "            second_addend = random.randint(0, M-1)\n",
    "            initial_memory = torch.zeros(M, M)\n",
    "            initial_memory[0][first_addend] = 1\n",
    "            initial_memory[1][second_addend] = 1\n",
    "            for j in range(2, M):\n",
    "                initial_memory[j][0] = 1\n",
    "\n",
    "            \n",
    "            output_memory = torch.zeros(M, M)\n",
    "            output_memory[0][(first_addend + second_addend) % M] = 1\n",
    "\n",
    "            # Output mask has ones in the row of the memory matrix where the answer will be stored.\n",
    "            output_mask = torch.zeros(M, M)\n",
    "            output_mask[0] = torch.ones(M)\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, list_len, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the list task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param list_len: The list length\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        if list_len > M:\n",
    "            raise ValueError(\"Cannot have a list longer than M\")\n",
    "        \n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            list_val = random.randint(1, M-1)\n",
    "            initial_memory = torch.zeros(M, M)\n",
    "            output_memory = torch.zeros(M, M)\n",
    "            # Output mask is length of the list itself\n",
    "            output_mask = torch.zeros(M, M)\n",
    "            \n",
    "            for i in range(list_len):\n",
    "                initial_memory[i][list_val] = 1\n",
    "                output_memory[i][(list_val + 1 ) % M] = 1\n",
    "                output_mask[i] = torch.ones(M)\n",
    "                \n",
    "            for j in range(list_len, M):\n",
    "                initial_memory[j][0] = 1\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_examples = 500 #7200\n",
    "\n",
    "# M = 8 # Don't change this (as long as we're using the add-task)\n",
    "# dataset = AddTaskDataset(M, num_examples)\n",
    "\n",
    "M = 7 # Don't change this (as long as we're using the add-task)\n",
    "dataset = IncTaskDataset(M, M - 2, num_examples)\n",
    "\n",
    "data_loader = data.DataLoader(dataset, batch_size = 1) # Don't change this batch size.  You have been warned.\n",
    "\n",
    "def anc_validation_criterion(output, label):\n",
    "    target_memory = label[1]\n",
    "    target_mask = label[2]\n",
    "    \n",
    "    output = output.data * target_mask\n",
    "    target_memory = target_memory * target_mask\n",
    "    _, target_indices = torch.max(target_memory, 2)\n",
    "    _, output_indices = torch.max(output, 2)\n",
    "    \n",
    "    return 1 - torch.equal(output_indices, target_indices)\n",
    "\n",
    "plot_every = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize our controller\n",
    "controller = Controller(first_arg = first_arg, \n",
    "                        second_arg = second_arg, \n",
    "                        output = target, \n",
    "                        instruction = instruction, \n",
    "                        initial_registers = init_registers, \n",
    "                        stop_threshold = .9, \n",
    "                        multiplier = 5,\n",
    "                        correctness_weight = 1, \n",
    "                        halting_weight = 0, \n",
    "                        confidence_weight = 5, \n",
    "                        efficiency_weight = 0.1, \n",
    "                        t_max = 50) \n",
    "\n",
    "# Learning rate is a tunable hyperparameter. The paper used 1 or 0.1.\n",
    "optimizer = optim.Adam(controller.parameters(), lr = 0.1)\n",
    "\n",
    "best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "    controller, \n",
    "    data_loader,  \n",
    "    optimizer, \n",
    "    num_epochs = 1, \n",
    "    print_every = 10, \n",
    "    plot_every = plot_every, \n",
    "    deep_copy_desired = False, \n",
    "    validation_criterion = anc_validation_criterion, \n",
    "    forward_train = True, \n",
    "    batch_size = 5) # In the paper, they used batch sizes of 1 or 5\n",
    "    \n",
    "#     #kangaroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x * plot_every for x in range(len(train_plot_losses))], train_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x * plot_every for x in range(len(validation_plot_losses))], validation_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cutoff = 0.7\n",
    "\n",
    "def getBest(vec):\n",
    "    maxVal, index = torch.max(vec, 0)\n",
    "    if maxVal.data[0] > cutoff:\n",
    "        return index.data[0]\n",
    "\n",
    "def bestRegister(vec):\n",
    "    index = getBest(vec)\n",
    "    if index is not None:\n",
    "        return \"R\" + str(1 + index)\n",
    "    return \"??\"\n",
    "    \n",
    "def bestInstruction(vec):\n",
    "    ops = [ \n",
    "        \"STOP\",\n",
    "        \"ZERO\",\n",
    "        \"INC\",\n",
    "        \"ADD\",\n",
    "        \"SUB\",\n",
    "        \"DEC\",\n",
    "        \"MIN\",\n",
    "        \"MAX\",\n",
    "        \"READ\",\n",
    "        \"WRITE\",\n",
    "        \"JEZ\"\n",
    "    ]\n",
    "    index = getBest(vec)\n",
    "    if index is not None:\n",
    "        return ops[index]\n",
    "    return \"??\"\n",
    "    \n",
    "# registers = controller.registers\n",
    "\n",
    "# # Add task\n",
    "# orig_register = [6,2,0,1,0,0]\n",
    "# orig_output = [4,3,5,3,4,5,5,5]\n",
    "# orig_instruction = [8,8,10,5,2,10,9,0]\n",
    "# orig_first = [4,3,3,3,4,2,2,5]\n",
    "# orig_second = [5,5,0,5,5,1,4,5]\n",
    "# print(controller.output)\n",
    "# print(controller.first_arg)\n",
    "# output = controller.output\n",
    "# first = controller.first_arg\n",
    "# second = controller.second_arg\n",
    "\n",
    "\n",
    "orig_register = [6,0,0,0,0,0,0]\n",
    "orig_first = [5,1,1,5,5,4,6]\n",
    "orig_second = [6,0,6,3,6,2,6]\n",
    "orig_output = [1,6,3,6,5,6,6]\n",
    "orig_instruction = [8,10,2,9,2,10,0]\n",
    "\n",
    "\n",
    "\n",
    "instruction = controller.instruction\n",
    "_, R, M = controller.registers.size()\n",
    "    \n",
    "def printProgram():   \n",
    "    # Print registers\n",
    "    for i in range(R):\n",
    "        print(\"R\" + str(i + 1) + \" = \" + str(getBest(controller.registers[0, i,:])))\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Print the actual program\n",
    "    for i in range (M):\n",
    "        print(bestRegister(controller.output[0, :, i]) + \" = \" + \n",
    "              bestInstruction(controller.instruction[0, :, i]) + \"(\" +\n",
    "              bestRegister(controller.first_arg[0, :, i]) + \", \" +\n",
    "              bestRegister(controller.second_arg[0, :, i]) + \")\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def compareOutput():\n",
    "    # compare our output to theirs\n",
    "    # we get one point for every matching number\n",
    "    match_count = 0\n",
    "    softmax = nn.Softmax(1)\n",
    "    print(softmax(controller.output))\n",
    "    for i in range(R):\n",
    "        if getBest(nn.Softmax(2)(controller.registers)[0, i,:]) == orig_register[i]:\n",
    "            match_count += 1\n",
    "    for i in range (M):\n",
    "        if getBest(softmax(controller.output)[0, :, i]) == orig_output[i]:\n",
    "            match_count += 1\n",
    "        if getBest(softmax(controller.instruction)[0, :, i]) == orig_instruction[i]:\n",
    "            match_count += 1\n",
    "        if getBest(softmax(controller.first_arg)[0, :, i]) == orig_first[i]:\n",
    "            match_count += 1\n",
    "        if getBest(softmax(controller.second_arg)[0, :, i]) == orig_second[i]:\n",
    "            match_count += 1\n",
    "\n",
    "    percent_orig = match_count / (len(orig_register) + len(orig_output) + \n",
    "                                           len(orig_instruction) + len(orig_first) + len(orig_second))\n",
    "    return percent_orig\n",
    "    print(\"PERCENT MATCH\", percent_orig)\n",
    "    \n",
    "printProgram()\n",
    "compareOutput()\n",
    "\n",
    "# Original Add Program   \n",
    "# R1 = 6\n",
    "# R2 = 2\n",
    "# R3 = 0\n",
    "# R4 = 1\n",
    "# R5 = 0\n",
    "# R6 = 0\n",
    "\n",
    "\n",
    "# R5 = READ(R5, R6)\n",
    "# R4 = READ(R4, R6)\n",
    "# R6 = JEZ(R4, R1)\n",
    "# R4 = DEC(R4, R6)\n",
    "# R5 = INC(R5, R6)\n",
    "# R6 = JEZ(R3, R2)\n",
    "# R6 = WRITE(R3, R5)\n",
    "# R6 = STOP(R6, R6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a bunch of times\n",
    "num_trials = 2\n",
    "\n",
    "num_original_convergences = 0\n",
    "num_0_losses = 0\n",
    "num_better_convergences = 0\n",
    "otherPrograms = []\n",
    "for i in range(num_trials):\n",
    "    print(\"Trial \", i)\n",
    "    best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "        controller, \n",
    "        data_loader,  \n",
    "        optimizer, \n",
    "        num_epochs = 1, \n",
    "        print_every = 10000, \n",
    "        plot_every = plot_every, \n",
    "        deep_copy_desired = False, \n",
    "        validation_criterion = anc_validation_criterion, \n",
    "        forward_train = True, \n",
    "        batch_size = 1) # In the paper, they used batch sizes of 1 or 5\n",
    "    percent_orig = compareOutput()\n",
    "    if percent_orig > .99:\n",
    "        num_original_convergences += 1\n",
    "    end_losses = validation_plot_losses[-2:]\n",
    "    if sum(end_losses) < .01:\n",
    "        num_0_losses += 1\n",
    "    if percent_orig < .99 and sum(end_losses) < .01:\n",
    "        num_better_convergences += 1\n",
    "        otherPrograms.append((controller.output, controller.instruction, controller.first_arg, controller.second_arg, controller.registers))\n",
    "print(\"LOSS CONVERGENCES\", num_0_losses * 1.0 / num_trials)\n",
    "print(\"ORIG CONVERGENCES\", num_original_convergences * 1.0 / num_trials)\n",
    "print(\"BETTER CONVERGENCES\", num_better_convergences * 1.0 / num_trials)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(1)\n",
    "print(softmax(controller.instruction))\n",
    "print(controller.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printProgram()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
