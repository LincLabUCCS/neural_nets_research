{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets_library import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkProb(vec, dim, name):\n",
    "    # Get rid of batch dim\n",
    "    vec = torch.squeeze(vec, 0)\n",
    "    sums = torch.sum(vec, dim)\n",
    "    size = sums.size()\n",
    "    prob = abs(torch.sum(sums - 1).data[0])\n",
    "    if not prob < .001:\n",
    "        print(\"BAD PROB\", prob, name, vec)\n",
    "    return prob < .001\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    Contains the two learnable parts of the model in four independent, fully connected layers.\n",
    "    First the initial values for the registers and instruction registers and second the \n",
    "    parameters that computes the required distributions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 first_arg = None, \n",
    "                 second_arg = None, \n",
    "                 output = None, \n",
    "                 instruction = None, \n",
    "                 initial_registers = None, \n",
    "                 stop_threshold = .99, \n",
    "                 multiplier = 10,\n",
    "                 correctness_weight = .2, \n",
    "                 halting_weight = .2, \n",
    "                 confidence_weight = .2, \n",
    "                 efficiency_weight = .4,\n",
    "                 t_max = 100):\n",
    "        #TODO: Read over ANC paper, check if there are more reasonable default initial values.\n",
    "        \"\"\"\n",
    "        Initialize a bunch of constants and pass in matrices defining a program.\n",
    "        \n",
    "        :param first_arg: Matrix with the 1st register argument for each timestep stored in the columns (1xRxM)\n",
    "        :param second_arg: Matrix with the 2nd register argument for each timestep stored in the columns (1xRxM)\n",
    "        :param output: Matrix with the output register for each timestep stored in the columns (1xRxM)\n",
    "        :param instruction: Matrix with the instruction for each timestep stored in the columns (1xNxM)\n",
    "        :param initial_registers: Matrix where each row is a distribution over the value in one register (1xRxM)\n",
    "        :param stop_threshold: The stop probability threshold at which the controller should stop running\n",
    "        :param multiplier: The factor our one-hot vectors are be multiplied by before they're softmaxed to add blur\n",
    "        :param correctness_weight: Weight given to the correctness component of the loss function\n",
    "        :param halting_weight: Weight given to the halting component of the loss function\n",
    "        :param confidence_weight: Weight given to the confidence component of the loss function\n",
    "        :param efficiency_weight: Weight given to the efficiency component of the loss function\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Controller, self).__init__()\n",
    "        \n",
    "        # Initialize dimension constants\n",
    "        B, R, M = initial_registers.size()\n",
    "        self.M = M\n",
    "        self.R = R\n",
    "        \n",
    "        # Initialize loss function weights\n",
    "        # In the ANC paper, these scalars are called, alpha, beta, gamma, and delta\n",
    "        self.correctness_weight = correctness_weight\n",
    "        self.halting_weight = halting_weight\n",
    "        self.confidence_weight = confidence_weight\n",
    "        self.efficiency_weight = efficiency_weight\n",
    "        \n",
    "        \n",
    "        # And yet more initialized constants... yeah, there are a bunch, I know.\n",
    "        self.t_max = t_max\n",
    "        self.stop_threshold = stop_threshold\n",
    "        \n",
    "\n",
    "        # Initialize parameters.  These are the things that are going to be optimized. \n",
    "        self.first_arg = nn.Parameter(multiplier * first_arg)\n",
    "        self.second_arg = nn.Parameter(multiplier * second_arg)\n",
    "        self.output = nn.Parameter(multiplier * output)\n",
    "        self.instruction = nn.Parameter(multiplier * instruction) \n",
    "#         initial_registers = self.blur(initial_registers, multiplier, 2)\n",
    "        self.registers = nn.Parameter(multiplier * initial_registers)\n",
    "        IR_initial = torch.DoubleTensor(1, M, 1).zero_()\n",
    "        IR_initial[0,0,0] = 1\n",
    "        self.IR = nn.Parameter(multiplier * IR_initial)\n",
    "        \n",
    "        \n",
    "        # Machine initialization\n",
    "        self.machine = Machine(B, M, R)\n",
    "        \n",
    "        \n",
    "        self.iter = 0\n",
    "    \n",
    "    def blur(self, matrix, scale_factor, dimension):\n",
    "        \"\"\"\n",
    "        Takes a matrix, each row (or column) of which is a one-hot vector.\n",
    "        Multiply each 1 by a constant and then softmax it, which \n",
    "        effectively \"blurs\" the matrix a little bit.\n",
    "        \n",
    "        :param matrix: Matrix to blur\n",
    "        :param scale_factor: Constant to multiply the matrix by before it's softmaxed\n",
    "        :param dimension: Dimension to softmax over\n",
    "        \n",
    "        :return: Blurred matrix\n",
    "        \"\"\"\n",
    "        matrix = scale_factor * matrix\n",
    "        softmax = nn.Softmax(dimension)\n",
    "        return softmax(Variable(matrix))    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def forward(self, input, train):\n",
    "        \"\"\"\n",
    "        Runs the controller on a certain input memory matrix.\n",
    "        It either returns the loss or the output memory.\n",
    "        \n",
    "        :param input: A three-tuple of three MxM matrices: (memory matrix, output_memory, output_mask)\n",
    "        \n",
    "        :return: If train is true, return the loss. Otherwise, return the output matrix\n",
    "        \"\"\"\n",
    "        # Program's initial memory\n",
    "        self.memory = input[0]\n",
    "        # Desired output memory\n",
    "        self.output_memory = Variable(input[1])\n",
    "        # Mask with 1's in the rows of the output memory matrix which actually contain the answer.\n",
    "        self.output_mask = Variable(input[2])\n",
    "    \n",
    "        \n",
    "        # Initialize instruction regiser (1xMx1)\n",
    "#         self.register_buffer('IR', torch.DoubleTensor(1, M, 1).zero_())\n",
    "#         self.IR[0, 0, 0] = 1\n",
    "        \n",
    "        IR = nn.Softmax(1)(self.IR)\n",
    "#         print(\"IR\", IR)\n",
    "        self.memory = Variable(self.memory)\n",
    "        \n",
    "#         print(\"INITIAL MEMORY\", self.memory)\n",
    "        \n",
    "        efficiency_loss = 0\n",
    "        confidence_loss = 0\n",
    "        self.stop_probability = 0\n",
    "        \n",
    "        # Copy registers so we aren't using the values from the previous iteration.\n",
    "        registers = nn.Softmax(2)(self.registers)\n",
    "        \n",
    "        # loss initialization\n",
    "        self.confidence = 0\n",
    "        self.efficiency = 0\n",
    "        self.halting = 0\n",
    "        self.correctness = 0\n",
    "        \n",
    "        t = 0 \n",
    "        \n",
    "#         print(\"ITERATION \", self.iter)\n",
    "        self.iter += 1\n",
    "    \n",
    "#         print(\"INITIAL REGISTERS\", registers)\n",
    "            \n",
    "        # Run the program, one timestep at a time, until the program terminates or whe time out\n",
    "        while t < self.t_max and self.stop_probability < self.stop_threshold: \n",
    "            \n",
    "#             print(\"REGISTERS\", registers)\n",
    "#             print(\"MEMORY\", self.memory)\n",
    "            \n",
    "            softmax = nn.Softmax(1)\n",
    "            \n",
    "            a = softmax(torch.bmm(self.first_arg, IR))\n",
    "            b = softmax(torch.bmm(self.second_arg, IR))\n",
    "            o = softmax(torch.bmm(self.output, IR))\n",
    "            e = softmax(torch.bmm(self.instruction, IR))\n",
    "            \n",
    "            anyBad = False\n",
    "            anyBad = anyBad or not checkProb(a, 0, \"A\")\n",
    "            anyBad = anyBad or not checkProb(b, 0, \"B\")\n",
    "            anyBad = anyBad or not checkProb(o, 0, \"O\")\n",
    "            anyBad = anyBad or not checkProb(e, 0, \"E\")\n",
    "            anyBad = anyBad or not checkProb(self.memory, 1, \"Memory\")\n",
    "            anyBad = anyBad or not checkProb(IR, 0, \"IR\")\n",
    "            anyBad = anyBad or not checkProb(registers, 1, \"Registers\")\n",
    "            if anyBad:\n",
    "                print(\"PROBLEM ON ITERATION \" + str(t))\n",
    "            \n",
    "            old_mem = self.memory\n",
    "            \n",
    "            # Update memory, registers, and IR after machine operation\n",
    "            self.old_stop_probability = self.stop_probability\n",
    "            self.memory, registers, IR, new_stop_prob = self.machine(e, a, b, o, self.memory, registers, IR) \n",
    "            \n",
    "            self.stop_probability += new_stop_prob[0]\n",
    "            print(self.stop_probability)\n",
    "            \n",
    "            # If we're training, calculate loss\n",
    "            if train:\n",
    "                self.timestep_loss(t)\n",
    "            \n",
    "            t += 1\n",
    "            \n",
    "#         print(\"END REGISTERS \", registers)\n",
    "        \n",
    "        # If we're training, return loss.  Otherwise return memory.\n",
    "        if train:\n",
    "            self.final_loss(t)\n",
    "            total_loss  = self.total_loss()\n",
    "            return (self.memory, total_loss)\n",
    "        else:\n",
    "            return (self.memory, None)\n",
    "        \n",
    "        \n",
    "    def timestep_loss(self, t):\n",
    "        \"\"\"\n",
    "        @ Rakia @ Aditya feel free to use this function definition or not.  My main thought was that this would \n",
    "        compute the types of loss which get updated every timestep.\n",
    "        \"\"\"\n",
    "        # Confidence Loss \n",
    "        mem_diff = self.output_memory - self.memory\n",
    "        correctness = torch.sum((self.output_mask) * mem_diff * mem_diff)\n",
    "        self.confidence += (self.stop_probability - self.old_stop_probability) * correctness\n",
    "        \n",
    "        # Efficiency Loss\n",
    "        if t < self.t_max and self.stop_probability < self.stop_threshold: # don't add efficiency loss on the last timestep\n",
    "            self.efficiency += (1 - self.stop_probability)\n",
    "            \n",
    "        \n",
    "            \n",
    "    \n",
    "    def final_loss(self, t):\n",
    "        \"\"\"\n",
    "        @ Rakia @ Aditya feel free to use this function definition or not.  My main thought was that this would \n",
    "        compute the types of loss which get updated every timestep.\n",
    "        \"\"\"\n",
    "        # Correctness loss\n",
    "        mem_diff = self.output_memory - self.memory\n",
    "        self.correctness = torch.sum((self.output_mask) * mem_diff * mem_diff)\n",
    "\n",
    "        # Halting loss\n",
    "        if t == self.t_max:\n",
    "            self.halting = (1 - self.stop_probability)\n",
    "            \n",
    "        print(\"T FINAL\", t)\n",
    "#         print(\"FINAL MEM\", t, self.memory)\n",
    "            \n",
    "\n",
    "    def total_loss(self):\n",
    "        \"\"\" compute four diferent loss functions and return a weighted average of the four measuring correctness, \n",
    "        halting, efficiency, and confidence\"\"\"\n",
    "        return  (self.correctness*self.correctness_weight) + (self.confidence_weight*self.confidence) + (self.halting_weight*self.halting) + (self.efficiency_weight*self.efficiency) \n",
    "        \n",
    "      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(nn.Module):\n",
    "    \"\"\"\n",
    "    Parent class for our binary operations\n",
    "    \"\"\"\n",
    "    def __init__(self, M):\n",
    "        \"\"\"\n",
    "        Initialize the memory length (needed so we can mod our answer in case it exceeds the range 0-M-1)\n",
    "        Also calculate the output matrix for the operation\n",
    "        \n",
    "        :param M: Memory length\n",
    "        \"\"\"\n",
    "        super(Operation, self).__init__()\n",
    "        self.M = M\n",
    "        \n",
    "        # Create a MxMxM matrix where the (i,j,k) cell is 1 iff operation(i,j) = k.\n",
    "        self.outputs = torch.IntTensor(M, M, M).zero_()\n",
    "        for i in range(M):\n",
    "            for j in range(M):\n",
    "                val = self.compute(i, j)\n",
    "                self.outputs[val][i][j] = 1\n",
    "                \n",
    "        self.outputs = Variable(self.outputs)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        \"\"\" \n",
    "        Perform the binary operation.  The arguments may or may not be used.\n",
    "        \n",
    "        :param x: First argument\n",
    "        :param y: Second argument\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        :return: The output matrix\n",
    "        \"\"\"\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = Variable(torch.FloatTensor([[1,2,],[3,4]]))\n",
    "# print(test)\n",
    "# print(nn.Softmax(0)(test))\n",
    "# print(nn.Softmax(1)(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Operation):\n",
    "\n",
    "    def __init__(self, M):\n",
    "        super(Add, self).__init__(M)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        return (x + y) % self.M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stop(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Stop, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jump(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Jump, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0 # Actual jump happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decrement(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Decrement, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x - 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Increment(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Increment, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x + 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Max, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return max(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Min(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Min, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return min(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Read, self).__init__(M)\n",
    "        # Leave output matrix blank since we're gonna do the reading elsewhere\n",
    "        self.outputs = torch.DoubleTensor(M, M, M).zero_()\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return 0 # Actual reading happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtract(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Subtract, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return (x - y) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Write(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Write, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return 0 # Actual write happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zero(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Zero, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine(nn.Module):\n",
    "    \"\"\"\n",
    "    The Machine executes assembly instructions passed to it by the Controller.\n",
    "    It updates the given memory, registers, and instruction pointer.\n",
    "    The Machine doesn't have any learnable parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, B, M, R):\n",
    "        \"\"\"\n",
    "        Initializes dimensions, operations, and counters\n",
    "        \n",
    "        :param B: Batch size (meant to be 1)\n",
    "        :param M: Memory length.  Integer values also take on values 0-M-1.  M is also the program length.\n",
    "        :param R: Number of registers\n",
    "        \"\"\"\n",
    "        super(Machine, self).__init__()\n",
    "        \n",
    "        # Store parameters as class variables\n",
    "        self.R = R # Number of registers\n",
    "        self.M = M # Memory length (also largest number)\n",
    "        self.B = B # Batch size\n",
    "        \n",
    "        # Start off with 0 probability of stopping\n",
    "        self.stop_probability = 0 \n",
    "        \n",
    "        # List of ops (must be in same order as the original ANC paper so compilation works right)\n",
    "        self.ops = [ \n",
    "            Stop(M),\n",
    "            Zero(M),\n",
    "            Increment(M),\n",
    "            Add(M),\n",
    "            Subtract(M),\n",
    "            Decrement(M),\n",
    "            Min(M),\n",
    "            Max(M),\n",
    "            Read(M),\n",
    "            Write(M),\n",
    "            Jump(M)\n",
    "        ]\n",
    "        \n",
    "        # Number of instructions\n",
    "        self.N = len(self.ops)\n",
    "        \n",
    "        # Create a 4D matrix composed of the output matrices of each of the ops\n",
    "        self.outputs = Variable(torch.DoubleTensor(self.N, M, M, M)).zero_()\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            op = self.ops[i]\n",
    "            self.outputs[i] = op()\n",
    "            \n",
    "        # Add an extra batch dimension\n",
    "        self.outputs = torch.unsqueeze(self.outputs, 0)\n",
    "        self.outputs = self.outputs.expand(B, -1, -1, -1, -1)\n",
    "        \n",
    "        # Keep track of ops which will be handled specially\n",
    "        self.jump_index = 10\n",
    "        self.stop_index = 0\n",
    "        self.write_index = 9\n",
    "        self.read_index = 8\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, e, a, b, o, memory, registers, IR):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run the Machine for one timestep (corresponding to the execution of one line of Assembly).\n",
    "        The first four parameter names correspond to the vector names used in the original ANC paper\n",
    "        \n",
    "        :param e: Probability distribution over the instruction being executed (BxMx1)\n",
    "        :param a: Probability distribution over the first argument register (length BxRx1)\n",
    "        :param b: Probability distribution over the second argument register (length BxRx1)\n",
    "        :param o: Probability distribution over the first argument register (length BxRx1)\n",
    "        :param memory: Memory matrix (size BxMxM)\n",
    "        :param registers: Register matrix (size BxRxM)\n",
    "        :param IR: Instruction Register (length BxM)\n",
    "        \n",
    "        :return: The memory, registers, and instruction register after the timestep\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dimensions B x 1 x R -> B x 1 x R\n",
    "        a = torch.transpose(a, 1, 2)\n",
    "        b = torch.transpose(b, 1, 2)\n",
    "        \n",
    "        # Calculate distributions over the two argument values by multiplying each \n",
    "        # register by the probability that register is being used.\n",
    "        arg1 = torch.bmm(a, registers)\n",
    "        arg2 = torch.bmm(b, registers)\n",
    "        \n",
    "#         print(\"A\", a)\n",
    "#         print(\"REG\", registers)\n",
    "        \n",
    "        # Multiply the output matrix by the arg1 and arg2 vectors to take into account\n",
    "        # Before we do this, we're going to have to do a bunch of dimension squishing.\n",
    "        \n",
    "        # arg1_long dimensions: B x 1 x M --> B x 1 x 1 x 1 x M\n",
    "        arg1_long = torch.unsqueeze(arg1, 1)\n",
    "        arg1_long = torch.unsqueeze(arg1_long, 1)\n",
    "        \n",
    "        outputs_x_arg1 = torch.matmul(arg1_long, self.outputs)\n",
    "        \n",
    "        # outputs_x_arg1 dimensions: B x N x M x 1 x M -> B x N x M x M\n",
    "        outputs_x_arg1 = torch.squeeze(outputs_x_arg1, 3)\n",
    "        \n",
    "        # arg2_long dimensions: B x 1 x M --> B x 1 x M x 1\n",
    "        arg2_long = torch.unsqueeze(arg2, 3)\n",
    "        \n",
    "        outputs_x_args = torch.matmul(outputs_x_arg1, arg2_long)\n",
    "        \n",
    "        # outputs_x_args dimensions: B x N x M x 1 -> B x N x M\n",
    "        outputs_x_args = torch.squeeze(outputs_x_args, 3)\n",
    "        \n",
    "        # e dimensions B x N x 1 -> B x 1 x N\n",
    "        e = torch.transpose(e, 1, 2)\n",
    "        \n",
    "        # read_vec dimensions B x 1 -> B x 1 x 1\n",
    "        read_vec =  e[:, :, self.read_index]\n",
    "        read_vec = read_vec.unsqueeze(1)\n",
    "        \n",
    "        # Length Bx1xM vector over the output of the operation\n",
    "        out_vec = torch.matmul(e, outputs_x_args)\n",
    "        \n",
    "        # Deal with memory reads separately\n",
    "        out_vec = out_vec + read_vec * torch.matmul(arg1, memory)        \n",
    "        \n",
    "        # Update our memory, registers, instruction register, and stopping probability\n",
    "        mem_old = memory\n",
    "        memory = self.writeMemory(e, o, memory, arg1, arg2)\n",
    "        registers = self.writeRegisters(out_vec, o, registers)\n",
    "        IR = self.updateIR(e, IR, arg1, arg2)\n",
    "        stop_prob = self.getStop(e)\n",
    "        \n",
    "        return(memory, registers, IR, stop_prob)\n",
    "        \n",
    "        \n",
    "    def writeRegisters(self, out, o, registers):\n",
    "        \"\"\"\n",
    "        Write the result of our operation to our registers.\n",
    "        \n",
    "        :param out: Probability distribution over the output value (Bx1xM)\n",
    "        :param o: Probability distribution over the output register (BxRx1)\n",
    "        :param Registers: register matrix (BxRxM)\n",
    "        \n",
    "        :return: The updated registers (BxRxM)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Multiply probability of writing to each output register by the distribution over the value we're writing there.\n",
    "        new_register_vals = torch.matmul(o, out)\n",
    "        \n",
    "        # Multiply each original register cell by the probabilty of not writing to that register\n",
    "        old_register_vals = (1-o).expand(self.B, self.R, self.M) * registers\n",
    "        \n",
    "        # Take a weighted sum over the old and new register values\n",
    "        registers =  new_register_vals + old_register_vals\n",
    "     #         print(\"SIZE\", o.size())\n",
    "#         print(\"NEW\", new_register_vals)\n",
    "#         print(\"OLD\", old_register_vals)\n",
    "        \n",
    "        return registers\n",
    "    \n",
    "    def updateIR(self, e, IR, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the instruction register\n",
    "        \n",
    "        :param e: Distribution over the current instruction (BxNx1)\n",
    "        :param IR: Instruction register (length BxMx1)\n",
    "        :param arg1: Distribution over the first argument value (length BxMx1)\n",
    "        :param arg2: Distribution over the second argument value (length BxMx1)\n",
    "        \n",
    "        :return: The updated instruction register (BxMx1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dimensions B x 1 x M -> B x M x 1\n",
    "        arg2 = arg2.transpose(1, 2)\n",
    "        \n",
    "        # Probability that we're on the jump instruction\n",
    "        jump_probability = e[:, :, self.jump_index]\n",
    "        \n",
    "        # Probability that the first argument is 0\n",
    "        is_zero = arg1[:, :, 0]\n",
    "        \n",
    "        # Slicing lost a dimension.  Let's add it back\n",
    "        jump_probability = torch.unsqueeze(jump_probability, 1)\n",
    "        is_zero = torch.unsqueeze(is_zero, 1)\n",
    "        \n",
    "        # If we're not jumping, just shift IR by one slot\n",
    "        wraparound = IR[:, -1]\n",
    "        normal_instructions = IR[:, :-1]\n",
    "        \n",
    "        # For whatever reason, when you chop off one row/column, that dimension disappears.  Add it back.\n",
    "        wraparound = wraparound.unsqueeze(1)\n",
    "        IR_no_jump = torch.cat([wraparound, normal_instructions], 1)\n",
    "        \n",
    "        # If we are on a jump instruction, check whether the argument's 0.\n",
    "        # If it is, jump to the location specified by arg2.  Otherwise, increment like normal.\n",
    "        IR_jump = arg2 * is_zero + (1 - is_zero) * IR_no_jump\n",
    "        \n",
    "        # Take a weighted sum of the instruction register with and without jumping\n",
    "        IR = IR_no_jump * (1 - jump_probability) + IR_jump * jump_probability\n",
    "        \n",
    "        return IR\n",
    "    \n",
    "    def writeMemory(self, e, o, mem_orig, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the memory\n",
    "        \n",
    "        :param e: Distribution over the current instruction (B x1xM)\n",
    "        :param mem_orig: Current memory matrix (BxMxM)\n",
    "        :param arg1: Distribution over the first argument value (Bx1xM)\n",
    "        :param arg2: Distribution over the second argument value (Bx1xM)\n",
    "        \n",
    "        :return: The updated memory matrix (BxMxM)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Probability that we're on the write instruction\n",
    "        write_probability = e[:,:, self.write_index]\n",
    "        \n",
    "        # write_probability dimensions: Bx1 -> B x 1 x 1\n",
    "        write_probability = torch.unsqueeze(write_probability, 1)\n",
    "        \n",
    "        # arg1 dimensions: B x 1 x M -> B x M x 1\n",
    "        arg1 = torch.transpose(arg1, 1, 2)\n",
    "        \n",
    "        # If we are on a write instruction, write the value arg2 in register arg1. Otherwise, leave memory as is.\n",
    "        mem_changed = torch.bmm(arg1, arg2)\n",
    "#         print(\"ARG1\", arg1)\n",
    "#         print(\"ARG2\", arg2)\n",
    "#         print(\"MEM CHANGED\", mem_changed)\n",
    "        mem_unchanged = mem_orig * (1-arg1).expand(-1, -1, self.M)\n",
    "        mem_write = mem_changed + mem_unchanged\n",
    "        \n",
    "        \n",
    "        # Take a weighted sum over the new memory and old memory\n",
    "        memory = mem_orig * (1 - write_probability) + mem_write * write_probability\n",
    "        return memory\n",
    "        \n",
    "    def getStop(self, e):\n",
    "        \"\"\"\n",
    "        Obtain the probability that we will stop at this timestep based on the probability that we are running the STOP op.\n",
    "        \n",
    "        :param e: distribution over the current instruction (length Bx1xM)\n",
    "        \n",
    "        :return: probability representing whether the controller should stop.\n",
    "        \"\"\"\n",
    "        return e[:, :, self.stop_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hotify(vec, length, dimension):\n",
    "    \"\"\"\n",
    "    Turn a tensor of integers into a matrix of one-hot vectors.\n",
    "    \n",
    "    :param vec: The vector to be converted.\n",
    "    :param length: One dimension of the matrix (the other is the length of vec)\n",
    "    :param dimension: Which dimension stores the elements of vec.  If 0, they're stored in the rows.  If 1, the columns.\n",
    "    \n",
    "    :return A matrix of one-hot vectors, each row or column corresponding to one element of vec\n",
    "    \"\"\"\n",
    "    x = vec.size()[0]\n",
    "    if dimension == 0:\n",
    "        binary_vec = torch.DoubleTensor(x, length).zero_()\n",
    "        for i in range(x):\n",
    "            binary_vec[i][vec[i]] = 1\n",
    "        return binary_vec\n",
    "    elif dimension == 1:\n",
    "        binary_vec = torch.DoubleTensor(length, x).zero_()\n",
    "        for i in range(x):\n",
    "            binary_vec[vec[i]][i] = 1\n",
    "        return binary_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Addition task\n",
    "# # Generate this by running the instructions here (but with the addition program file): https://github.com/aditya-khant/neural-assembly-compiler\n",
    "# # Then get rid of the .cuda in each of the tensors since we (or at least I) don't have cuda\n",
    "# init_registers = torch.IntTensor([6,2,0,1,0,0]) # Length R, should be RxM\n",
    "# first_arg = torch.IntTensor([4,3,3,3,4,2,2,5]) # Length M, should be RxM\n",
    "# second_arg = torch.IntTensor([5,5,0,5,5,1,4,5]) # Length M, should be RxM\n",
    "# target = torch.IntTensor([4,3,5,3,4,5,5,5]) # Length M, should be RxM\n",
    "# instruction = torch.IntTensor([8,8,10,5,2,10,9,0]) # Length M, should be NxM\n",
    "\n",
    "# Increment task\n",
    "init_registers = torch.IntTensor([6,0,0,0,0,0,0])\n",
    "first_arg = torch.IntTensor([5,1,1,5,5,4,6])\n",
    "second_arg = torch.IntTensor([6,0,6,3,6,2,6])\n",
    "target = torch.IntTensor([1,6,3,6,5,6,6])\n",
    "instruction = torch.IntTensor([8,10,2,9,2,10,0])\n",
    "\n",
    "\n",
    "\n",
    "# Get dimensions we'll need\n",
    "M = first_arg.size()[0]\n",
    "R = init_registers.size()[0]\n",
    "N = 11\n",
    "\n",
    "# Turn the given tensors into matrices of one-hot vectors.\n",
    "init_registers = one_hotify(init_registers, M, 0)\n",
    "first_arg = one_hotify(first_arg, R, 1)\n",
    "second_arg = one_hotify(second_arg, R, 1)\n",
    "target = one_hotify(target, R, 1)\n",
    "instruction = one_hotify(instruction, N, 1)\n",
    "\n",
    "# Add a fake first batch\n",
    "init_registers = init_registers.unsqueeze(0)\n",
    "first_arg = first_arg.unsqueeze(0)\n",
    "second_arg = second_arg.unsqueeze(0)\n",
    "target = target.unsqueeze(0)\n",
    "instruction = instruction.unsqueeze(0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the addition task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            first_addend = random.randint(0, M-1)\n",
    "            second_addend = random.randint(0, M-1)\n",
    "            initial_memory = torch.DoubleTensor(M, M).zero_()\n",
    "            initial_memory[0][first_addend] = 1\n",
    "            initial_memory[1][second_addend] = 1\n",
    "            for j in range(2, M):\n",
    "                initial_memory[j][0] = 1\n",
    "\n",
    "            \n",
    "            output_memory = torch.DoubleTensor(M, M).zero_()\n",
    "            output_memory[0][(first_addend + second_addend) % M] = 1\n",
    "\n",
    "            # Output mask has ones in the rows of the memory matrix where the answer will be stored.\n",
    "            output_mask = torch.DoubleTensor(M, M).zero_()\n",
    "            output_mask[0] = torch.ones(M)\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialAddTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the addition task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            first_addend = random.randint(0, M-1)\n",
    "            second_addend = random.randint(0, M-1)\n",
    "            initial_memory = torch.DoubleTensor(M, M).zero_()\n",
    "            initial_memory[0][first_addend] = 1\n",
    "            initial_memory[1][second_addend] = 1\n",
    "            for j in range(2, M):\n",
    "                initial_memory[j][0] = 1\n",
    "\n",
    "            \n",
    "            output_memory = torch.DoubleTensor(M, M).zero_()\n",
    "            output_memory[0][(first_addend + second_addend) % M] = 1\n",
    "\n",
    "            # Output mask has ones in the rows of the memory matrix where the answer will be stored.\n",
    "            output_mask = torch.DoubleTensor(M, M).zero_()\n",
    "            output_mask[2] = torch.ones(M)\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, list_len, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the list task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param list_len: The list length\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        if list_len > M:\n",
    "            raise ValueError(\"Cannot have a list longer than M\")\n",
    "        \n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            list_val = random.randint(1, M-1)\n",
    "            initial_memory = torch.DoubleTensor(M, M).zero_()\n",
    "            output_memory = torch.DoubleTensor(M, M).zero_()\n",
    "            # Output mask is length of the list itself\n",
    "            output_mask = torch.DoubleTensor(M, M).zero_()\n",
    "            for i in range(list_len):\n",
    "                initial_memory[i][list_val] = 1\n",
    "                output_memory[i][(list_val + 1 ) % M] = 1\n",
    "                output_mask[i] = torch.ones(M)\n",
    "            for j in range(list_len, M):\n",
    "                initial_memory[j][0] = 1\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_examples = 500 #7200\n",
    "\n",
    "# M = 8 # Don't change this (as long as we're using the add-task)\n",
    "# dataset = AddTaskDataset(M, num_examples)\n",
    "\n",
    "# M = 8 # Don't change this (as long as we're using the add-task)\n",
    "# dataset = TrivialAddTaskDataset(M, num_examples)\n",
    "\n",
    "M = 7 # Don't change this (as long as we're using the add-task)\n",
    "dataset = IncTaskDataset(M, M - 2, num_examples)\n",
    "\n",
    "data_loader = data.DataLoader(dataset, batch_size = 1) # Don't change this batch size.  You have been warned.\n",
    "\n",
    "def anc_validation_criterion(output, label):\n",
    "    target_memory = label[1]\n",
    "    target_mask = label[2]\n",
    "    \n",
    "    output = output.data * target_mask\n",
    "    target_memory = target_memory * target_mask\n",
    "    _, target_indices = torch.max(target_memory, 2)\n",
    "    _, output_indices = torch.max(output, 2)\n",
    "    \n",
    "    if not torch.equal(output_indices, target_indices):\n",
    "        print(\"TARGET:\", target_indices)\n",
    "        print(\"OUTPUT:\", output_indices)\n",
    "    \n",
    "    return 1 - torch.equal(output_indices, target_indices)\n",
    "\n",
    "# Initialize our controller\n",
    "controller = Controller(first_arg = first_arg, \n",
    "                        second_arg = second_arg, \n",
    "                        output = target, \n",
    "                        instruction = instruction, \n",
    "                        initial_registers = init_registers, \n",
    "                        stop_threshold = .9, \n",
    "                        multiplier = 5,\n",
    "                        correctness_weight = 1, \n",
    "                        halting_weight = 0, \n",
    "                        confidence_weight = 5, \n",
    "                        efficiency_weight = 0.1, \n",
    "                        t_max = 50) \n",
    "\n",
    "# Learning rate is a tunable hyperparameter\n",
    "# The paper didn't mention which one they used\n",
    "optimizer = optim.Adam(controller.parameters(), lr = 0.1)\n",
    "\n",
    "plot_every = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "    controller, \n",
    "    data_loader,  \n",
    "    optimizer, \n",
    "    num_epochs = 1, \n",
    "    print_every = 10, \n",
    "    plot_every = plot_every, \n",
    "    deep_copy_desired = False, \n",
    "    validation_criterion = anc_validation_criterion, \n",
    "    forward_train = True, \n",
    "    batch_size = 5) # In the paper, they used batch sizes of 1 or 5\n",
    "    \n",
    "#     #kangaroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x * plot_every for x in range(len(train_plot_losses))], train_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x * plot_every for x in range(len(validation_plot_losses))], validation_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(controller.first_arg)\n",
    "# print(controller.second_arg)\n",
    "# print(controller.output)\n",
    "# print(controller.instruction)\n",
    "# print(controller.registers)\n",
    "\n",
    "cutoff = 0.7\n",
    "\n",
    "def getBest(vec):\n",
    "    maxVal, index = torch.max(vec, 0)\n",
    "    if maxVal.data[0] > cutoff:\n",
    "        return index.data[0]\n",
    "\n",
    "def bestRegister(vec):\n",
    "    index = getBest(vec)\n",
    "    if index is not None:\n",
    "        return \"R\" + str(1 + index)\n",
    "    return \"??\"\n",
    "    \n",
    "def bestInstruction(vec):\n",
    "    ops = [ \n",
    "        \"STOP\",\n",
    "        \"ZERO\",\n",
    "        \"INC\",\n",
    "        \"ADD\",\n",
    "        \"SUB\",\n",
    "        \"DEC\",\n",
    "        \"MIN\",\n",
    "        \"MAX\",\n",
    "        \"READ\",\n",
    "        \"WRITE\",\n",
    "        \"JEZ\"\n",
    "    ]\n",
    "    index = getBest(vec)\n",
    "    if index is not None:\n",
    "        return ops[index]\n",
    "    return \"??\"\n",
    "    \n",
    "# registers = controller.registers\n",
    "\n",
    "# # Add task\n",
    "# orig_register = [6,2,0,1,0,0]\n",
    "# orig_output = [4,3,5,3,4,5,5,5]\n",
    "# orig_instruction = [8,8,10,5,2,10,9,0]\n",
    "# orig_first = [4,3,3,3,4,2,2,5]\n",
    "# orig_second = [5,5,0,5,5,1,4,5]\n",
    "# print(controller.output)\n",
    "# print(controller.first_arg)\n",
    "# output = controller.output\n",
    "# first = controller.first_arg\n",
    "# second = controller.second_arg\n",
    "\n",
    "\n",
    "orig_register = [6,0,0,0,0,0,0]\n",
    "orig_first = [5,1,1,5,5,4,6]\n",
    "orig_second = [6,0,6,3,6,2,6]\n",
    "orig_output = [1,6,3,6,5,6,6]\n",
    "orig_instruction = [8,10,2,9,2,10,0]\n",
    "\n",
    "\n",
    "\n",
    "instruction = controller.instruction\n",
    "_, R, M = controller.registers.size()\n",
    "    \n",
    "def printProgram():   \n",
    "    # Print registers\n",
    "    for i in range(R):\n",
    "        print(\"R\" + str(i + 1) + \" = \" + str(getBest(controller.registers[0, i,:])))\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Print the actual program\n",
    "    for i in range (M):\n",
    "        print(bestRegister(controller.output[0, :, i]) + \" = \" + \n",
    "              bestInstruction(controller.instruction[0, :, i]) + \"(\" +\n",
    "              bestRegister(controller.first_arg[0, :, i]) + \", \" +\n",
    "              bestRegister(controller.second_arg[0, :, i]) + \")\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def compareOutput():\n",
    "    # compare our output to theirs\n",
    "    # we get one point for every matching number\n",
    "    match_count = 0\n",
    "    softmax = nn.Softmax(1)\n",
    "    print(softmax(controller.output))\n",
    "    for i in range(R):\n",
    "        if getBest(nn.Softmax(2)(controller.registers)[0, i,:]) == orig_register[i]:\n",
    "            match_count += 1\n",
    "    for i in range (M):\n",
    "        if getBest(softmax(controller.output)[0, :, i]) == orig_output[i]:\n",
    "            match_count += 1\n",
    "        if getBest(softmax(controller.instruction)[0, :, i]) == orig_instruction[i]:\n",
    "            match_count += 1\n",
    "        if getBest(softmax(controller.first_arg)[0, :, i]) == orig_first[i]:\n",
    "            match_count += 1\n",
    "        if getBest(softmax(controller.second_arg)[0, :, i]) == orig_second[i]:\n",
    "            match_count += 1\n",
    "\n",
    "    percent_orig = match_count / (len(orig_register) + len(orig_output) + \n",
    "                                           len(orig_instruction) + len(orig_first) + len(orig_second))\n",
    "    return percent_orig\n",
    "    print(\"PERCENT MATCH\", percent_orig)\n",
    "    \n",
    "printProgram()\n",
    "compareOutput()\n",
    "\n",
    "# Original Add Program   \n",
    "# R1 = 6\n",
    "# R2 = 2\n",
    "# R3 = 0\n",
    "# R4 = 1\n",
    "# R5 = 0\n",
    "# R6 = 0\n",
    "\n",
    "\n",
    "# R5 = READ(R5, R6)\n",
    "# R4 = READ(R4, R6)\n",
    "# R6 = JEZ(R4, R1)\n",
    "# R4 = DEC(R4, R6)\n",
    "# R5 = INC(R5, R6)\n",
    "# R6 = JEZ(R3, R2)\n",
    "# R6 = WRITE(R3, R5)\n",
    "# R6 = STOP(R6, R6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a bunch of times\n",
    "num_trials = 2\n",
    "\n",
    "num_original_convergences = 0\n",
    "num_0_losses = 0\n",
    "num_better_convergences = 0\n",
    "otherPrograms = []\n",
    "for i in range(num_trials):\n",
    "    print(\"Trial \", i)\n",
    "    best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "        controller, \n",
    "        data_loader,  \n",
    "        optimizer, \n",
    "        num_epochs = 1, \n",
    "        print_every = 10000, \n",
    "        plot_every = plot_every, \n",
    "        deep_copy_desired = False, \n",
    "        validation_criterion = anc_validation_criterion, \n",
    "        forward_train = True, \n",
    "        batch_size = 1) # In the paper, they used batch sizes of 1 or 5\n",
    "    percent_orig = compareOutput()\n",
    "    if percent_orig > .99:\n",
    "        num_original_convergences += 1\n",
    "    end_losses = validation_plot_losses[-2:]\n",
    "    if sum(end_losses) < .01:\n",
    "        num_0_losses += 1\n",
    "    if percent_orig < .99 and sum(end_losses) < .01:\n",
    "        num_better_convergences += 1\n",
    "        otherPrograms.append((controller.output, controller.instruction, controller.first_arg, controller.second_arg, controller.registers))\n",
    "print(\"LOSS CONVERGENCES\", num_0_losses * 1.0 / num_trials)\n",
    "print(\"ORIG CONVERGENCES\", num_original_convergences * 1.0 / num_trials)\n",
    "print(\"BETTER CONVERGENCES\", num_better_convergences * 1.0 / num_trials)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(1)\n",
    "print(softmax(controller.instruction))\n",
    "print(controller.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printProgram()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
