{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rakiasegev/Documents\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neural_nets_library import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    Contains the two learnable parts of the model in four independent, fully connected layers.\n",
    "    First the initial values for the registers and instruction registers and second the \n",
    "    parameters that computes the required distributions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 first_arg = None, \n",
    "                 second_arg = None, \n",
    "                 output = None, \n",
    "                 instruction = None, \n",
    "                 initial_registers = None, \n",
    "                 stop_threshold = .99, \n",
    "                 multiplier = 8.4,\n",
    "                 correctness_weight = .25, \n",
    "                 halting_weight = .25, \n",
    "                 confidence_weight = .25, \n",
    "                 efficiency_weight = .25,\n",
    "                 t_max = 100):\n",
    "        #TODO: Read over ANC paper, check if there are more reasonable default initial values.\n",
    "        \"\"\"\n",
    "        Initialize a bunch of constants and pass in matrices defining a program.\n",
    "        \n",
    "        :param first_arg: Matrix with the 1st register argument for each timestep stored in the columns (1xRxM)\n",
    "        :param second_arg: Matrix with the 2nd register argument for each timestep stored in the columns (1xRxM)\n",
    "        :param output: Matrix with the output register for each timestep stored in the columns (1xRxM)\n",
    "        :param instruction: Matrix with the instruction for each timestep stored in the columns (1xNxM)\n",
    "        :param initial_registers: Matrix where each row is a distribution over the value in one register (1xRxM)\n",
    "        :param stop_threshold: The stop probability threshold at which the controller should stop running\n",
    "        :param multiplier: The factor our one-hot vectors are be multiplied by before they're softmaxed to add blur\n",
    "        :param correctness_weight: Weight given to the correctness component of the loss function\n",
    "        :param halting_weight: Weight given to the halting component of the loss function\n",
    "        :param confidence_weight: Weight given to the confidence component of the loss function\n",
    "        :param efficiency_weight: Weight given to the efficiency component of the loss function\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Controller, self).__init__()\n",
    "        \n",
    "        # Initialize dimension constants\n",
    "        B, R, M = initial_registers.size()\n",
    "        self.M = M\n",
    "        self.R = R\n",
    "        \n",
    "        # Initialize loss function weights\n",
    "        # In the ANC paper, these scalars are called, alpha, beta, gamma, and delta\n",
    "        self.correctness_weight = correctness_weight\n",
    "        self.halting_weight = halting_weight\n",
    "        self.confidence_weight = confidence_weight\n",
    "        self.efficiency_weight = efficiency_weight\n",
    "        \n",
    "        \n",
    "        # And yet more initialized constants... yeah, there are a bunch, I know.\n",
    "        self.t_max = t_max\n",
    "        self.stop_threshold = stop_threshold\n",
    "        \n",
    "#         # Blur matrices - i.e. give each different operation/argument/register value some nonzero probability\n",
    "#         if blur is not None:\n",
    "#             first_arg = self.blur(first_arg, blur, 1)\n",
    "#             second_arg = self.blur(second_arg, blur, 1)\n",
    "#             output = self.blur(output, blur, 1)\n",
    "#             instruction = self.blur(instruction, blur, 1)\n",
    "#             initial_registers = self.blur(initial_registers, blur, 2)\n",
    "            \n",
    "#             # Initialize parameters.  These are the things that are going to be optimized. \n",
    "#             self.first_arg = nn.Parameter(multiplier * first_arg.data)\n",
    "#             self.second_arg = nn.Parameter(multiplier * second_arg.data)\n",
    "#             self.output = nn.Parameter(output.data)\n",
    "#             self.instruction = nn.Parameter(multiplier * instruction.data) \n",
    "#             self.registers = nn.Parameter(multiplier * initial_registers.data)\n",
    "#         else:\n",
    "            # Initialize parameters.  These are the things that are going to be optimized. \n",
    "        self.first_arg = nn.Parameter(multiplier * first_arg)\n",
    "        self.second_arg = nn.Parameter(multiplier * second_arg)\n",
    "        self.output = nn.Parameter(multiplier * output)\n",
    "        self.instruction = nn.Parameter(multiplier * instruction) \n",
    "#         self.registers = nn.Parameter(initial_registers)\n",
    "        initial_registers = self.blur(initial_registers, multiplier, 2)\n",
    "        self.registers = nn.Parameter(initial_registers.data)\n",
    "\n",
    "        \n",
    "        # Machine initialization\n",
    "        self.machine = Machine(B, M, R)\n",
    "    \n",
    "    def blur(self, matrix, scale_factor, dimension):\n",
    "        \"\"\"\n",
    "        Takes a matrix, each row (or column) of which is a one-hot vector.\n",
    "        Multiply each 1 by a constant and then softmax it, which \n",
    "        effectively \"blurs\" the matrix a little bit.\n",
    "        \n",
    "        :param matrix: Matrix to blur\n",
    "        :param scale_factor: Constant to multiply the matrix by before it's softmaxed\n",
    "        :param dimension: Dimension to softmax over\n",
    "        \n",
    "        :return: Blurred matrix\n",
    "        \"\"\"\n",
    "        matrix = scale_factor * matrix\n",
    "        print(\"DIM\", dimension)\n",
    "        softmax = nn.Softmax(dimension)\n",
    "        return softmax(Variable(matrix))    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def forward(self, input, train):\n",
    "        \"\"\"\n",
    "        Runs the controller on a certain input memory matrix.\n",
    "        It either returns the loss or the output memory.\n",
    "        \n",
    "        :param input: A three-tuple of three MxM matrices: (memory matrix, output_memory, output_mask)\n",
    "        \n",
    "        :return: If train is true, return the loss. Otherwise, return the output matrix\n",
    "        \"\"\"\n",
    "        # Program's initial memory\n",
    "        self.memory = input[0]\n",
    "        # Desired output memory\n",
    "        self.output_memory = Variable(input[1])\n",
    "        # Mask with 1's in the rows of the output memory matrix which actually contain the answer.\n",
    "        self.output_mask = Variable(input[2])\n",
    "    \n",
    "        \n",
    "        # Initialize instruction regiser (1xMx1)\n",
    "        self.register_buffer('IR', torch.DoubleTensor(1, M, 1).zero_())\n",
    "        self.IR[0, 0, 0] = 1\n",
    "        \n",
    "#         # Blur memory and instruction register\n",
    "#         if self.blur_factor is not None:\n",
    "#             self.memory = self.blur(self.memory, self.blur_factor, 2) \n",
    "#             IR = self.blur(self.IR, self.blur_factor, 1)\n",
    "#         else:\n",
    "        IR = Variable(self.IR)\n",
    "        self.memory = Variable(self.memory)\n",
    "        \n",
    "        efficiency_loss = 0\n",
    "        confidence_loss = 0\n",
    "        self.stop_probability = 0\n",
    "        \n",
    "        # Copy registers so we aren't using the values from the previous iteration.\n",
    "        registers = self.registers\n",
    "        \n",
    "        # loss initialization\n",
    "        self.confidence = 0\n",
    "        self.efficiency = 0\n",
    "        self.halting = 0\n",
    "        self.correctness = 0\n",
    "        \n",
    "        t = 0 \n",
    "        # Run the program, one timestep at a time, until the program terminates or whe time out\n",
    "        while t < self.t_max and self.stop_probability < self.stop_threshold: \n",
    "            \n",
    "            softmax = nn.Softmax(1)\n",
    "            \n",
    "            a = softmax(torch.bmm(self.first_arg, IR))\n",
    "            b = softmax(torch.bmm(self.second_arg, IR))\n",
    "            o = softmax(torch.bmm(self.output, IR))\n",
    "            e = softmax(torch.bmm(self.instruction, IR))\n",
    "            \n",
    "            # print(\"STUFFF!!!\")\n",
    "           #  print(a)\n",
    "           #  print(b)\n",
    "            # print(o)\n",
    "            # print(e)\n",
    "            #print(self.registers)\n",
    "           # print(\"DONE\")\n",
    "            \n",
    "            old_mem = self.memory\n",
    "            \n",
    "            # Update memory, registers, and IR after machine operation\n",
    "            self.old_stop_probability = self.stop_probability\n",
    "            self.memory, registers, IR, new_stop_prob = self.machine(e, a, b, o, self.memory, registers, IR) \n",
    "            \n",
    "#             print(\"NEW STOP PROB\", new_stop_prob)\n",
    "            \n",
    "            self.stop_probability += new_stop_prob[0]\n",
    "            \n",
    "            # If we're training, calculate loss\n",
    "            if train:\n",
    "                self.timestep_loss(t)\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        # If we're training, return loss.  Otherwise return memory.\n",
    "        if train:\n",
    "            self.final_loss(t)\n",
    "            total_loss  = self.total_loss()\n",
    "            return (self.memory, total_loss)\n",
    "        else:\n",
    "            return (self.memory, None)\n",
    "        \n",
    "        \n",
    "    def timestep_loss(self, t):\n",
    "        \"\"\"\n",
    "        @ Rakia @ Aditya feel free to use this function definition or not.  My main thought was that this would \n",
    "        compute the types of loss which get updated every timestep.\n",
    "        \"\"\"\n",
    "        # Confidence Loss \n",
    "        mem_diff = self.output_memory - self.memory\n",
    "        correctness = torch.sum((self.output_mask) * mem_diff * mem_diff)\n",
    "        self.confidence += (self.stop_probability - self.old_stop_probability) * correctness\n",
    "        \n",
    "        # Efficiency Loss\n",
    "        if t < self.t_max and self.stop_probability < self.stop_threshold: # don't add efficiency loss on the last timestep\n",
    "            self.efficiency += (1 - self.stop_probability)\n",
    "            \n",
    "    \n",
    "    def final_loss(self, t):\n",
    "        \"\"\"\n",
    "        @ Rakia @ Aditya feel free to use this function definition or not.  My main thought was that this would \n",
    "        compute the types of loss which get updated every timestep.\n",
    "        \"\"\"\n",
    "        # Correctness loss\n",
    "        mem_diff = self.output_memory - self.memory\n",
    "        self.correctness = torch.sum((self.output_mask) * mem_diff * mem_diff)\n",
    "\n",
    "        # Halting loss\n",
    "        if t == self.t_max:\n",
    "            self.halting = (1 - self.stop_probability)\n",
    "            \n",
    "#         print(\"TTT\", t)\n",
    "   \n",
    "\n",
    "    def total_loss(self):\n",
    "        \"\"\" compute four diferent loss functions and return a weighted average of the four measuring correctness, \n",
    "        halting, efficiency, and confidence\"\"\"\n",
    "        return  (self.correctness*self.correctness_weight) + (self.confidence_weight*self.confidence) + (self.halting_weight*self.halting) + (self.efficiency_weight*self.efficiency) \n",
    "        \n",
    "      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Operation(nn.Module):\n",
    "    \"\"\"\n",
    "    Parent class for our binary operations\n",
    "    \"\"\"\n",
    "    def __init__(self, M):\n",
    "        \"\"\"\n",
    "        Initialize the memory length (needed so we can mod our answer in case it exceeds the range 0-M-1)\n",
    "        Also calculate the output matrix for the operation\n",
    "        \n",
    "        :param M: Memory length\n",
    "        \"\"\"\n",
    "        super(Operation, self).__init__()\n",
    "        self.M = M\n",
    "        \n",
    "        # Create a MxMxM matrix where the (i,j,k) cell is 1 iff operation(i,j) = k.\n",
    "        self.outputs = torch.IntTensor(M, M, M).zero_()\n",
    "        for i in range(M):\n",
    "            for j in range(M):\n",
    "                val = self.compute(i, j)\n",
    "                self.outputs[val][i][j] = 1\n",
    "                \n",
    "        self.outputs = Variable(self.outputs)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        \"\"\" \n",
    "        Perform the binary operation.  The arguments may or may not be used.\n",
    "        \n",
    "        :param x: First argument\n",
    "        :param y: Second argument\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        :return: The output matrix\n",
    "        \"\"\"\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Add(Operation):\n",
    "\n",
    "    def __init__(self, M):\n",
    "        super(Add, self).__init__(M)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        return (x + y) % self.M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stop(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Stop, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Jump(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Jump, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0 # Actual jump happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decrement(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Decrement, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x - 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Increment(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Increment, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x + 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Max(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Max, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return max(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Min(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Min, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return min(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Read(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Read, self).__init__(M)\n",
    "        # Leave output matrix blank since we're gonna do the reading elsewhere\n",
    "        self.outputs = torch.DoubleTensor(M, M, M).zero_()\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return 0 # Actual reading happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Subtract(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Subtract, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return (x - y) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Write(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Write, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return 0 # Actual write happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Zero(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Zero, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Machine(nn.Module):\n",
    "    \"\"\"\n",
    "    The Machine executes assembly instructions passed to it by the Controller.\n",
    "    It updates the given memory, registers, and instruction pointer.\n",
    "    The Machine doesn't have any learnable parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, B, M, R):\n",
    "        \"\"\"\n",
    "        Initializes dimensions, operations, and counters\n",
    "        \n",
    "        :param B: Batch size (meant to be 1)\n",
    "        :param M: Memory length.  Integer values also take on values 0-M-1.  M is also the program length.\n",
    "        :param R: Number of registers\n",
    "        \"\"\"\n",
    "        super(Machine, self).__init__()\n",
    "        \n",
    "        # Store parameters as class variables\n",
    "        self.R = R # Number of registers\n",
    "        self.M = M # Memory length (also largest number)\n",
    "        self.B = B # Batch size\n",
    "        \n",
    "        # Start off with 0 probability of stopping\n",
    "        self.stop_probability = 0 \n",
    "        \n",
    "        # List of ops (must be in same order as the original ANC paper so compilation works right)\n",
    "        self.ops = [ \n",
    "            Stop(M),\n",
    "            Zero(M),\n",
    "            Increment(M),\n",
    "            Add(M),\n",
    "            Subtract(M),\n",
    "            Decrement(M),\n",
    "            Min(M),\n",
    "            Max(M),\n",
    "            Read(M),\n",
    "            Write(M),\n",
    "            Jump(M)\n",
    "        ]\n",
    "        \n",
    "        # Number of instructions\n",
    "        self.N = len(self.ops)\n",
    "        \n",
    "        # Create a 4D matrix composed of the output matrices of each of the ops\n",
    "        self.outputs = Variable(torch.DoubleTensor(self.N, M, M, M)).zero_()\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            op = self.ops[i]\n",
    "            self.outputs[i] = op()\n",
    "            \n",
    "        # Add an extra batch dimension\n",
    "        self.outputs = torch.unsqueeze(self.outputs, 0)\n",
    "        self.outputs = self.outputs.expand(B, -1, -1, -1, -1)\n",
    "        \n",
    "        # Keep track of ops which will be handled specially\n",
    "        self.jump_index = 10\n",
    "        self.stop_index = 0\n",
    "        self.write_index = 9\n",
    "        self.read_index = 8\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, e, a, b, o, memory, registers, IR):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run the Machine for one timestep (corresponding to the execution of one line of Assembly).\n",
    "        The first four parameter names correspond to the vector names used in the original ANC paper\n",
    "        \n",
    "        :param e: Probability distribution over the instruction being executed (BxMx1)\n",
    "        :param a: Probability distribution over the first argument register (length BxRx1)\n",
    "        :param b: Probability distribution over the second argument register (length BxRx1)\n",
    "        :param o: Probability distribution over the first argument register (length BxRx1)\n",
    "        :param memory: Memory matrix (size BxMxM)\n",
    "        :param registers: Register matrix (size BxRxM)\n",
    "        :param IR: Instruction Register (length BxM)\n",
    "        \n",
    "        :return: The memory, registers, and instruction register after the timestep\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dimensions B x 1 x R -> B x 1 x R\n",
    "        a = torch.transpose(a, 1, 2)\n",
    "        b = torch.transpose(b, 1, 2)\n",
    "        \n",
    "        # Calculate distributions over the two argument values by multiplying each \n",
    "        # register by the probability that register is being used.\n",
    "        arg1 = torch.bmm(a, registers)\n",
    "        arg2 = torch.bmm(b, registers)\n",
    "        \n",
    "        # Multiply the output matrix by the arg1 and arg2 vectors to take into account\n",
    "        # Before we do this, we're going to have to do a bunch of dimension squishing.\n",
    "        \n",
    "        # arg1_long dimensions: B x 1 x M --> B x 1 x 1 x 1 x M\n",
    "        arg1_long = torch.unsqueeze(arg1, 1)\n",
    "        arg1_long = torch.unsqueeze(arg1_long, 1)\n",
    "        \n",
    "        outputs_x_arg1 = torch.matmul(arg1_long, self.outputs)\n",
    "        \n",
    "        # outputs_x_arg1 dimensions: B x N x M x 1 x M -> B x N x M x M\n",
    "        outputs_x_arg1 = torch.squeeze(outputs_x_arg1, 3)\n",
    "        \n",
    "        # arg2_long dimensions: B x 1 x M --> B x 1 x M x 1\n",
    "        arg2_long = torch.unsqueeze(arg2, 3)\n",
    "        \n",
    "        outputs_x_args = torch.matmul(outputs_x_arg1, arg2_long)\n",
    "        \n",
    "        # outputs_x_args dimensions: B x N x M x 1 -> B x N x M\n",
    "        outputs_x_args = torch.squeeze(outputs_x_args, 3)\n",
    "        \n",
    "        # e dimensions B x N x 1 -> B x 1 x N\n",
    "        e = torch.transpose(e, 1, 2)\n",
    "        \n",
    "        # read_vec dimensions B x 1 -> B x 1 x 1\n",
    "        read_vec =  e[:, :, self.read_index]\n",
    "        read_vec = read_vec.unsqueeze(1)\n",
    "        \n",
    "        # Length Bx1xM vector over the output of the operation\n",
    "        out_vec = torch.matmul(e, outputs_x_args)\n",
    "        \n",
    "        # Deal with memory reads separately\n",
    "        out_vec = out_vec + read_vec * torch.matmul(arg1, memory)        \n",
    "        \n",
    "        # Update our memory, registers, instruction register, and stopping probability\n",
    "        mem_old = memory\n",
    "        memory = self.writeMemory(e, o, memory, arg1, arg2)\n",
    "        registers = self.writeRegisters(out_vec, o, registers)\n",
    "        IR = self.updateIR(e, IR, arg1, arg2)\n",
    "        stop_prob = self.getStop(e)\n",
    "        \n",
    "        return(memory, registers, IR, stop_prob)\n",
    "        \n",
    "        \n",
    "    def writeRegisters(self, out, o, registers):\n",
    "        \"\"\"\n",
    "        Write the result of our operation to our registers.\n",
    "        \n",
    "        :param out: Probability distribution over the output value (Bx1xM)\n",
    "        :param o: Probability distribution over the output register (BxRx1)\n",
    "        :param Registers: register matrix (BxRxM)\n",
    "        \n",
    "        :return: The updated registers (BxRxM)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Multiply probability of writing to each output register by the distribution over the value we're writing there.\n",
    "        new_register_vals = torch.matmul(o, out)\n",
    "        \n",
    "        # Multiply each original register cell by the probabilty of not writing to that register\n",
    "        old_register_vals = (1-o).expand(self.B, self.R, self.M) * registers\n",
    "        \n",
    "        # Take a weighted sum over the old and new register values\n",
    "        registers =  new_register_vals + old_register_vals\n",
    "        \n",
    "        return registers\n",
    "    \n",
    "    def updateIR(self, e, IR, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the instruction register\n",
    "        \n",
    "        :param e: Distribution over the current instruction (BxNx1)\n",
    "        :param IR: Instruction register (length BxMx1)\n",
    "        :param arg1: Distribution over the first argument value (length BxMx1)\n",
    "        :param arg2: Distribution over the second argument value (length BxMx1)\n",
    "        \n",
    "        :return: The updated instruction register (BxMx1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dimensions B x 1 x M -> B x M x 1\n",
    "        arg2 = arg2.transpose(1, 2)\n",
    "        \n",
    "        # Probability that we're on the jump instruction\n",
    "        jump_probability = e[:, :, self.jump_index]\n",
    "        \n",
    "        # Probability that the first argument is 0\n",
    "        is_zero = arg1[:, :, 0]\n",
    "        \n",
    "        # Slicing lost a dimension.  Let's add it back\n",
    "        jump_probability = torch.unsqueeze(jump_probability, 1)\n",
    "        is_zero = torch.unsqueeze(is_zero, 1)\n",
    "        \n",
    "        # If we're not jumping, just shift IR by one slot\n",
    "        wraparound = IR[:, -1]\n",
    "        normal_instructions = IR[:, :-1]\n",
    "        \n",
    "        # For whatever reason, when you chop off one row/column, that dimension disappears.  Add it back.\n",
    "        wraparound = wraparound.unsqueeze(1)\n",
    "        IR_no_jump = torch.cat([wraparound, normal_instructions], 1)\n",
    "        \n",
    "        # If we are on a jump instruction, check whether the argument's 0.\n",
    "        # If it is, jump to the location specified by arg2.  Otherwise, increment like normal.\n",
    "        IR_jump = arg2 * is_zero + (1 - is_zero) * IR_no_jump\n",
    "        \n",
    "        # Take a weighted sum of the instruction register with and without jumping\n",
    "        IR = IR_no_jump * (1 - jump_probability) + IR_jump * jump_probability\n",
    "        \n",
    "        return IR\n",
    "    \n",
    "    def writeMemory(self, e, o, mem_orig, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the memory\n",
    "        \n",
    "        :param e: Distribution over the current instruction (B x1xM)\n",
    "        :param mem_orig: Current memory matrix (BxMxM)\n",
    "        :param arg1: Distribution over the first argument value (Bx1xM)\n",
    "        :param arg2: Distribution over the second argument value (Bx1xM)\n",
    "        \n",
    "        :return: The updated memory matrix (BxMxM)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Probability that we're on the write instruction\n",
    "        write_probability = e[:,:, self.write_index]\n",
    "        \n",
    "        # write_probability dimensions: Bx1 -> B x 1 x 1\n",
    "        write_probability = torch.unsqueeze(write_probability, 1)\n",
    "        \n",
    "        # arg1 dimensions: B x 1 x M -> B x M x 1\n",
    "        arg1 = torch.transpose(arg1, 1, 2)\n",
    "        \n",
    "        # If we are on a write instruction, write the value arg2 in register arg1. Otherwise, leave memory as is.\n",
    "        mem_changed = torch.bmm(arg1, arg2)\n",
    "        mem_unchanged = mem_orig * (1-arg1).expand(-1, -1, self.M)\n",
    "        mem_write = mem_changed + mem_unchanged\n",
    "        \n",
    "        \n",
    "        # Take a weighted sum over the new memory and old memory\n",
    "        memory = mem_orig * (1 - write_probability) + mem_write * write_probability\n",
    "        return memory\n",
    "        \n",
    "    def getStop(self, e):\n",
    "        \"\"\"\n",
    "        Obtain the probability that we will stop at this timestep based on the probability that we are running the STOP op.\n",
    "        \n",
    "        :param e: distribution over the current instruction (length Bx1xM)\n",
    "        \n",
    "        :return: probability representing whether the controller should stop.\n",
    "        \"\"\"\n",
    "        return e[:, :, self.stop_index].data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hotify(vec, length, dimension):\n",
    "    \"\"\"\n",
    "    Turn a tensor of integers into a matrix of one-hot vectors.\n",
    "    \n",
    "    :param vec: The vector to be converted.\n",
    "    :param length: One dimension of the matrix (the other is the length of vec)\n",
    "    :param dimension: Which dimension stores the elements of vec.  If 0, they're stored in the rows.  If 1, the columns.\n",
    "    \n",
    "    :return A matrix of one-hot vectors, each row or column corresponding to one element of vec\n",
    "    \"\"\"\n",
    "    x = vec.size()[0]\n",
    "    if dimension == 0:\n",
    "        binary_vec = torch.DoubleTensor(x, length).zero_()\n",
    "        for i in range(x):\n",
    "            binary_vec[i][vec[i]] = 1\n",
    "        return binary_vec\n",
    "    elif dimension == 1:\n",
    "        binary_vec = torch.DoubleTensor(length, x).zero_()\n",
    "        for i in range(x):\n",
    "            binary_vec[vec[i]][i] = 1\n",
    "        return binary_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Addition task\n",
    "# # Generate this by running the instructions here (but with the addition program file): https://github.com/aditya-khant/neural-assembly-compiler\n",
    "# # Then get rid of the .cuda in each of the tensors since we (or at least I) don't have cuda\n",
    "# init_registers = torch.IntTensor([6,2,0,1,0,0]) # Length R, should be RxM\n",
    "# first_arg = torch.IntTensor([4,3,3,3,4,2,2,5]) # Length M, should be RxM\n",
    "# second_arg = torch.IntTensor([5,5,0,5,5,1,4,5]) # Length M, should be RxM\n",
    "# target = torch.IntTensor([4,3,5,3,4,5,5,5]) # Length M, should be RxM\n",
    "# instruction = torch.IntTensor([8,8,10,5,2,10,9,0]) # Length M, should be NxM\n",
    "\n",
    "# Increment task\n",
    "init_registers = torch.IntTensor([6,0,0,0,0,0,0])\n",
    "first_arg = torch.IntTensor([5,1,1,5,5,4,6])\n",
    "second_arg = torch.IntTensor([6,0,6,3,6,2,6])\n",
    "target = torch.IntTensor([1,6,3,6,5,6,6])\n",
    "instruction = torch.IntTensor([8,10,2,9,2,10,0])\n",
    "\n",
    "\n",
    "\n",
    "# Get dimensions we'll need\n",
    "M = first_arg.size()[0]\n",
    "R = init_registers.size()[0]\n",
    "N = 11\n",
    "\n",
    "# Turn the given tensors into matrices of one-hot vectors.\n",
    "init_registers = one_hotify(init_registers, M, 0)\n",
    "first_arg = one_hotify(first_arg, R, 1)\n",
    "second_arg = one_hotify(second_arg, R, 1)\n",
    "target = one_hotify(target, R, 1)\n",
    "instruction = one_hotify(instruction, N, 1)\n",
    "\n",
    "# Add a fake first batch\n",
    "init_registers = init_registers.unsqueeze(0)\n",
    "first_arg = first_arg.unsqueeze(0)\n",
    "second_arg = second_arg.unsqueeze(0)\n",
    "target = target.unsqueeze(0)\n",
    "instruction = instruction.unsqueeze(0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the addition task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            first_addend = random.randint(0, M-1)\n",
    "            second_addend = random.randint(0, M-1)\n",
    "            initial_memory = torch.DoubleTensor(M, M).zero_()\n",
    "            initial_memory[0][first_addend] = 1\n",
    "            initial_memory[1][second_addend] = 1\n",
    "            for j in range(2, M):\n",
    "                initial_memory[j][0] = 1\n",
    "\n",
    "            \n",
    "            output_memory = torch.DoubleTensor(M, M).zero_()\n",
    "            output_memory[0][(first_addend + second_addend) % M] = 1\n",
    "\n",
    "            # Output mask has ones in the rows of the memory matrix where the answer will be stored.\n",
    "            output_mask = torch.DoubleTensor(M, M).zero_()\n",
    "            output_mask[0] = torch.ones(M)\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IncTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, list_len, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the list task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param list_len: The list length\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        if list_len > M:\n",
    "            raise ValueError(\"Cannot have a list longer than M\")\n",
    "        \n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            list_val = random.randint(0, M-1)\n",
    "            initial_memory = torch.DoubleTensor(M, M).zero_()\n",
    "            output_memory = torch.DoubleTensor(M, M).zero_()\n",
    "            # Output mask is length of the list itself\n",
    "            output_mask = torch.DoubleTensor(M, M).zero_()\n",
    "            for i in range(list_len):\n",
    "                initial_memory[i][list_val] = 1\n",
    "                output_memory[i][(list_val + 1 ) % M] = 1\n",
    "                output_mask[i] = torch.ones(M)\n",
    "            for j in range(list_len, M):\n",
    "                initial_memory[j][0] = 1\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIM 2\n",
      "Epoch 0/0\n",
      "----------\n",
      "LR is set to 0.001\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "value cannot be converted to type double without overflow: inf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-f07de6f4751f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mvalidation_criterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manc_validation_criterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mforward_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     batch_size = 5) # In the paper, they used batch sizes of 1 or 5\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m#kangaroo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rakiasegev/Documents/GitHub/neural_nets_research/neural_nets_library/training.py\u001b[0m in \u001b[0;36mtrain_model_anc\u001b[0;34m(model, dset_loader, optimizer, lr_scheduler, num_epochs, print_every, plot_every, deep_copy_desired, validation_criterion, forward_train, batch_size)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalidation_criterion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-3388b4478772>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, train)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# If we're training, calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestep_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-3388b4478772>\u001b[0m in \u001b[0;36mtimestep_loss\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# Confidence Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mmem_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_memory\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mcorrectness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmem_diff\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmem_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfidence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_probability\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mold_stop_probability\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrectness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: value cannot be converted to type double without overflow: inf"
     ]
    }
   ],
   "source": [
    "num_examples = 100\n",
    "\n",
    "#M = 8 # Don't change this (as long as we're using the add-task)\n",
    "#dataset = AddTaskDataset(M, num_examples)\n",
    "\n",
    "M = 7 # Don't change this (as long as we're using the add-task)\n",
    "dataset = IncTaskDataset(M, M - 2, num_examples)\n",
    "\n",
    "data_loader = data.DataLoader(dataset, batch_size = 1) # Don't change this batch size.  You have been warned.\n",
    "\n",
    "def anc_validation_criterion(output, label):\n",
    "    target_memory = label[1]\n",
    "    target_mask = label[2]\n",
    "    \n",
    "    output = output.data * target_mask\n",
    "    target_memory = target_memory * target_mask\n",
    "    _, target_indices = torch.max(target_memory, 2)\n",
    "    _, output_indices = torch.max(output, 2)\n",
    "    \n",
    "    return 1 - torch.equal(output_indices, target_indices)\n",
    "\n",
    "# Initialize our controller\n",
    "controller = Controller(first_arg = first_arg, \n",
    "                        second_arg = second_arg, \n",
    "                        output = target, \n",
    "                        instruction = instruction, \n",
    "                        initial_registers = init_registers, \n",
    "                        stop_threshold = .9, \n",
    "                        multiplier = 8.4,\n",
    "                        correctness_weight = .75, \n",
    "                        halting_weight = .1, \n",
    "                        confidence_weight = .1, \n",
    "                        efficiency_weight = .05, \n",
    "                        t_max = 1000) \n",
    "\n",
    "# Learning rate is a tunable hyperparameter\n",
    "# The paper didn't mention which one they used\n",
    "optimizer = optim.Adam(controller.parameters(), lr = 0.00001)\n",
    "\n",
    "plot_every = 10\n",
    "\n",
    "best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "    controller, \n",
    "    data_loader,  \n",
    "    optimizer, \n",
    "    num_epochs = 1, \n",
    "    print_every = 10, \n",
    "    plot_every = plot_every, \n",
    "    deep_copy_desired = False, \n",
    "    validation_criterion = anc_validation_criterion, \n",
    "    forward_train = True, \n",
    "    batch_size = 5) # In the paper, they used batch sizes of 1 or 5\n",
    "    \n",
    "    #kangaroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDX+Dzd7Jv6ajfm4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10776b710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x * plot_every for x in range(len(train_plot_losses))], train_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt0W+Wd6P3vz/IttmU7vkRO7Di2E8sQLglgkkApEyCk\n0E6HTt/pFM50OqWdQ+mBlgJn3tOZ86511rvOeteatc5wa2FK6Qxt53SmtJ2BDp1yCkm4I4UkQCAJ\n4IsUJ7FzsS3f75b0vH9IDiY4sZJI2ntLv89aWYm095Z+3tn6+dHvefbziDEGpZRS2SPH6gCUUkql\nlyZ+pZTKMpr4lVIqy2jiV0qpLKOJXymlsowmfqWUyjKa+JVSKsto4ldKqSyjiV8ppbJMrtUBLKSq\nqso0NDRYHYZSSjnGW2+91W+MqU5kX1sm/oaGBvbs2WN1GEop5RgicijRfbXUo5RSWUYTv1JKZRlN\n/EoplWU08SulVJbRxK+UUllm0cQvIk+KSK+I7D/NdhGR74tIp4i8JyKXz9t2k4i0xbd9L5mBK6WU\nOjeJtPh/Ctx0hu03A83xP3cAPwQQERfwWHz7WuA2EVl7PsEqpZQ6f4smfmPMq8DAGXa5BfgnE7MT\nKBeR5cAGoNMYEzTGzABPxfdVSimmwxH++c1DTIcjVoeSdZJR468Fjsx73B1/7nTPL0hE7hCRPSKy\np6+vLwlhKaXs7KdvdPHfn9nPSx/2Wh1K1rFN564x5gljTKsxprW6OqG7jpVSDjU2HebxVwIAtB0f\nszia7JOMKRt6gJXzHtfFn8s7zfNKqSz3k9cPMjgxS0lBLu0nRq0OJ+skI/E/C9wtIk8BG4FhY8wx\nEekDmkWkkVjCvxX4T0l4P6WUgw1PzPLEa0G2XOhBBNo08afdoolfRH4BbAaqRKQb+B/EWvMYYx4H\nngM+C3QCE8Dt8W1hEbkbeB5wAU8aYw6k4GdQSjnIP7weZHQqzH03enlu3zFe/LCX6XCEglyX1aFl\njUUTvzHmtkW2G+Cu02x7jtgvBqWUYmB8hidfP8jnLlnO2hWldPaNEYkaDvaPc0FNqdXhZQ3bdO4q\npTLfj14JMDEb4btbmgFo8bgBaDuu5Z500sSvlEqL3tEpfubv4gvra2mOJ/zGqmJyc0Q7eNNME79S\nKi1++HKA2YjhnhuaTz6Xn5tDY1WxDulMM038SqmUOzY8yT/vPMyfXF5HQ1Xxx7Z5a9za4k8zTfxK\nqZR79MVODIZv37DmE9taPG6ODE4wMRO2ILLspIlfKZVSRwYm+OXuI3z5ypXULS36xHavx40x0Nmr\n5Z500cSvlEqp7+/oICdHuPu65gW3t9ToyJ5008SvlEqZYN8YT7/Tw1c2rqKmrHDBfeoriijIzdE6\nfxpp4ldKpcwjOzrId+Xwrc2rT7uPK0dYs6yEthNa6kkXTfxKqZRoPzHKs+8e5S+ubqDaXXDGfVs8\nbjq0xZ82mviVUinx8PZ2ivNz+ea1TYvu661xc2x4iuHJ2TREpjTxK6WS7sDRYZ7bd5yvX9PI0uL8\nRfefm7pBW/3poYlfKZV0D21rp7Qwl29c05jQ/s2eEkCnaE4XTfxKqaR65/Ag2z/o5Y5rmyhbkpfQ\nMbXlSyjOd9GuQzrTQhO/UiqpHtzWTkVxPl/7VGKtfQARiU/doCN70kETv1IqaXYdHOC1jn7u/IMm\nSgrOboG/Fo/O2ZMumviVUklhjOGBF9qodhfw55sazvp4r8dNaHyG/rHp5AenPkYTv1IqKXyBEG8e\nHOCuzatZkn/2yyh64yN7tM6fepr4lVLnzRjD373QxvKyQm7dUH9Or+Gt0ZE96aKJXyl13l5u6+Od\nw0N8+/pmCvPObdH06pIClhblaQdvGmjiV0qdF2MMD2xrY2XFEr7UWnfOryMieLWDNy008Sulzsvz\nB06wv2eE71zfTJ7r/FJKS42b9uOjGGOSFJ1aiCZ+pdQ5i0YND21rp6mqmD++rPa8X6/Z42Z0Osyx\n4akkRKdORxO/Uuqc/W7fMdpOjHLPlmZyz7O1Dx/N2aMdvKmV0P+UiNwkIm0i0iki31tg+1IReUZE\n3hORXSJy8bxtXSKyT0T2isieZAavlLJOOBLloe3ttHjcfP7SFUl5TW98zh6drC21Fk38IuICHgNu\nBtYCt4nI2lN2+xtgrzHmUuCrwCOnbL/OGLPeGNOahJiVUjbw73uPEuwb594bm8nJkaS8ZnlRPp7S\nAtqO68ieVEqkxb8B6DTGBI0xM8BTwC2n7LMWeBHAGPMh0CAinqRGqpSyjdlIlEd2dHDRilI+c1FN\nUl9bR/akXiKJvxY4Mu9xd/y5+d4FvgggIhuAVcDcuC4DbBeRt0TkjtO9iYjcISJ7RGRPX19fovEr\npSzwr291c3hggvu3ehFJTmt/jtfjpqN3lGhUR/akSrI6d/8WKBeRvcC3gXeASHzbNcaY9cRKRXeJ\nyLULvYAx5gljTKsxprW6ujpJYSmlkm06HOEHOzpYv7Kc61qWJf31WzxupmajHBmcSPprq5hEEn8P\nsHLe47r4cycZY0aMMbfHE/xXgWogGN/WE/+7F3iGWOlIKeVQT+06wtHhqZS09iG2DCNAm87ZkzKJ\nJP7dQLOINIpIPnAr8Oz8HUSkPL4N4C+BV40xIyJSLCLu+D7FwFZgf/LCV0ql0+RMhEdf6mRDYwXX\nrKlKyXs0L4uN7NE6f+osOmG2MSYsIncDzwMu4EljzAERuTO+/XHgQuBnImKAA8A34od7gGfirYJc\n4F+MMb9P/o+hlEqHn+88RN/oNI/edllKWvsAxQW51C1dQpvO2ZMyCa2UYIx5DnjulOcen/dvP+Bd\n4LggsO48Y1RK2cD4dJgfvhLg081VbGyqTOl7tXjcOj1zCumdu0qphPzU18XA+Az33fiJNl7SeWvc\nBPvHmI1EU/5e2UgTv1JqUSNTszzxapDrL1jGZfVLU/5+LR43sxFDV/94yt8rG2niV0ot6h9fO8jw\n5GxaWvvw0WpcOmdPamjiV0qd0eD4DE++fpCbLqrh4tqytLxnU3UxOaLLMKaKJn6l1Bk98VqQsZkw\n96aptQ9QmOeioapYW/wpoolfKXVa/WPT/PSNLj5/6Qpa4jdWpUuLx02HDulMCU38SqnT+uHLAabD\nEe7Z0pz29/Z63HSFxpmajSy+szormviVUgs6MTLFz3ce4ouX17G6uiTt799S4yZqoLNXW/3Jpolf\nKbWgx17qJBI13HND+lv78NGiLDp1Q/Jp4ldKfUL34AS/2HWYL7WuZGVFkSUxrKosJt+Vox28KaCJ\nXyn1CY++2IkgfPv6NZbFkOfKoam6WDt4U0ATv1LqY7r6x/n1W938p431rChfYmksLTVunZ45BTTx\nK6U+5vs7OshzCf9l82qrQ8HrcdMzNMno1KzVoWQUTfxKqZM6e0f5zd4evnpVA8tKC60O5+TUDR06\nsiepNPErpU56aHsHhXkuvnltk9WhALGbuECnbkg2TfxKKQA+ODbC7947xtc/1UhlSYHV4QBQt3QJ\nS/JctGsHb1Jp4ldKAfDQtnbchbn850/bo7UPkJMjeD0lOpY/yTTxK6V4r3uIF94/wV9e00RZUZ7V\n4XyM1+PWsfxJpolfKcWD29opL8rj69c0WB3KJ3g9bvpGpxkYn7E6lIyhiV+pLPfWoQFebuvjm9eu\nxl1or9Y+xJZhBJ26IZk08SuV5R54oZ2qknz+4upVVoeyoLmRPR2a+JNGE79SWcwX6McXCPGtzWso\nys+1OpwFeUoLKC3M1Tp/EmniVypLGWN48IV2PKUF/NnGeqvDOS0RoaXGTftxHdKZLAklfhG5SUTa\nRKRTRL63wPalIvKMiLwnIrtE5OJEj1VKWePVjn72HBrk7uubKcxzWR3OGTXHR/YYY6wOJSMsmvhF\nxAU8BtwMrAVuE5G1p+z2N8BeY8ylwFeBR87iWKVUmhljeOCFNmrLl/Dl1pVWh7OoFo+b4clZeken\nrQ4lIyTS4t8AdBpjgsaYGeAp4JZT9lkLvAhgjPkQaBART4LHKqXSbPsHvbzXPcw9NzSTn2v/iu/c\nnD06sic5EunNqQWOzHvcDWw8ZZ93gS8Cr4nIBmAVUJfgsSoF+semeXh7O9OzUatDYVVlEXddtwYR\nsToUBUSjhge3tdNQWcQXL6+1OpyEzK3G1XZ8lE83V1scTWo88043B3pG+OvPXogrJ7WflWR14/8t\n8IiI7AX2Ae8AZ7VCsojcAdwBUF9v344mp/jNOz38fOdhlpcVYmW6nYkY+semuaSunD/wZuYH1mn+\nz/7jfHBshIe+vI5cl/1b+wCVJQVUlRRkdIv/N+8cpWdokv/nD1NfDU8k8fcA84uAdfHnTjLGjAC3\nA0isWXcQCAJLFjt23ms8ATwB0Nraqj0458kfCNFYVcxL/3WzpXHMhKNc93cv8+ALbVzbXKWtfotF\nooaHtrezZlkJf7TOGa39OV5PCW0ZOlnbbCTK7q4B/uSKurS8XyK/7ncDzSLSKCL5wK3As/N3EJHy\n+DaAvwRejf8yWPRYlXzhSJRdBwe4anWl1aGQn5vDPTc08273MDs+6LU6nKz323eP0tk7xr1bvCkv\nJySb1+Om48Qo0WjmtQvf6x5iYibCVU3p+cwumviNMWHgbuB54APgV8aYAyJyp4jcGd/tQmC/iLQR\nG8Fzz5mOTf6Poebbf3SE0elw2i6ixXzx8loaKot4YFt7Rn5onSIcifLw9nYuqHFz88U1Vodz1lpq\n3EzMROgZmrQ6lKTzdYYA2JSmz2xCNX5jzHPAc6c89/i8f/sBb6LHqtTyBfqB9F1Ei8l15XDPlmbu\n/eW7/P7AcT57yXKrQ8pKT7/dQ1dogh9/tZUch7X24eMje1ZWFFkcTXL5gyHWLi9laXH+4jsngTN6\ndtRZ8QdCtHjcVLvtsZgGwB+tq2XNshIe3NZORFv9aTcTjvLIjg7W1ZWx5cJlVodzTprnRvZkWAfv\n1GyEPYcG01qa1cSfYWbCsU4iO9T353PlCPdu8dLZO8Zv3z1qdThZ55d7jtAzNMl9W1sc28FeWpjH\nirLCjFuG8e3Dg8yEo1ytiV+dq71Hhpiajdou8QPcfHENFy4v5eHt7YQj1t9fkC2mZiM8+mIHrauW\ncm1zldXhnBdvjTvjRvbsDIRw5QgbGivS9p6a+DOMPxBCBDY12i/x5+QI993opSs0wdNvLziqV6XA\nP795mBMj09y31evY1v6cFo+bQN9YRjUcfIEQF9eWpXUtBE38GcYX6OeiFaW2Wz5vzpYLl7GuroxH\ndnQwE86cD69dTcyE+eHLnVy9upKrVzu7tQ+xDt6ZcJRDAxNWh5IU49Nh9h4ZSmuZBzTxZ5Sp2Qjv\nHB6y9QdcRLhvaws9Q5P8cs+RxQ9Q5+VnvkP0j81w/9YFB905zsmRPRlS59/dNUA4ajTxq3P31qFB\nZiL2rO/Pd21zFa2rlvLYi51MzZ7VzB7qLIxOzfKjVwNsbqnmilXpqx+n0pplJYhkzsgefzBEnkto\nTfP/jyb+DOIL9JObI1zZYO8PuYhw/9YWjo9M8S9vHrY6nIz1kze6GJqY5b4bM6O1D7Ak38WqiqKM\nmbPHHwhx2cqlLMlP73oImvgziC8Q4tK6MkoK7LmE3nxXra7k6tWV/P3LnUzMhK0OJ+MMT8zy49eC\n3LjWw6V15VaHk1Rej5v2DBjZMzw5y/6eYUu+oWvizxBj02He6x62dX3/VPdv9dI/NsM/+Q9ZHUrG\n+fFrQUanwhnV2p/TUuPmYP8402Fnlwl3HRwgakh7fR808WeM3QcHiFjQSXQ+rlhVweaWah5/JcDo\n1KzV4WSM0Ng0T75xkM9dupwLl5daHU7SNXvcRKKGYN+41aGcF1+gn4LcHNbXp/8bmSb+DOEL9JPv\nyuHyVUutDuWs3Hejl6GJWX7yRpfVoWSMH70aZGo2wr1bmq0OJSVaMmQ1Ln8gxJUNFRTkpn+9Y038\nGcIXCHH5qnLbL5p9qkvrytm61sOPXwsyPKGt/vPVOzLFP/m7+ML6WtYsc1sdTko0VhWTmyO0OXhI\nZ2hsmg+Pj1o2Ak8TfwYYmpjh/WMjjqrvz3fvjV5Gp8L8+LWg1aE43t+/HGA2YvjODZnZ2ofYGg9N\n1cWO7uDdGRwA0MSvzt3O4ADGWHcRna8Ll5fyuUuX85M3DhIam7Y6HMc6OjTJv7x5mC9dUUdDVbHV\n4aRUbGSPc1v8vkA/JQW5XFpbZsn7a+LPAP5AP0vyXKxz8LC9e7c0Mzkb4Uevaqv/XP3gxU4Mhruv\nX2N1KCnn9bg5PDDh2KHA/kCIDY0Vlq15rIk/A/iDIa5srCA/17n/nWuWufnC+lr+yd9F7+iU1eE4\nzuHQBL/ec4TbNtRTtzSzFilZyNzUDR0OLPccH54i2D9u6Qp5zs0UCoC+0WnaT4w5ahjn6XznhmZm\nI4a/fylgdSiO8/0XO3DlCHddl/mtfYiN5QdnTt3gD8ZWyLOyNKuJ3+H8wdhanXZZX/d8NFQV86Ur\n6viXNw9zNAPXVU2VQN8YT7/dzVc2rcJTWmh1OGlRX1FEQW4OHQ5M/L7OEGVL8lhr4T0Wmvgdzh8I\n4S7M5aIVmXGjzt3Xr8FgePSlTqtDcYxHtndQkOviW5tXWx1K2rhyhGZPiSMXZfEHQ1zVVGnpusea\n+B3OH+hnY2OlZZ1EyVa3tIjbNtTzq91HOJIhc66nUtvxUX773lG+9qkGqkrss8ZyOniXuR03PfOR\ngQm6ByctH4GXGdkiS/UMTdIVmrD8Ikq2u65bgytHeGRHh9Wh2N5D29opyc/lm9c2WR1K2nlr3Bwf\nmXLUjX++QKy+b3WfnCZ+B/MHYvV9qy+iZPOUFvKVTat4+u1ugn3O+yqfLvt7hvn9geN8/ZpGyovy\nrQ4n7U5O3dDrnFa/PxCiqqSANctKLI1DE7+D+QMhKorzT34AMsm3Nq+mINfFw9u11X86D25rp2xJ\nHt/4dKPVoVjCW+OsOXuMMfgCIa5aXWn52scJJX4RuUlE2kSkU0S+t8D2MhH5rYi8KyIHROT2edu6\nRGSfiOwVkT3JDD6bGWPwB/rZ1FRhaSdRqlSVFPC1TzXw2/eOOnpOllR5+/AgL37Yyx3XNlGaxkW6\n7WRFWSElBbmOqfMH+sbpHZ22xTf0RRO/iLiAx4CbgbXAbSKy9pTd7gLeN8asAzYDD4jI/O+e1xlj\n1htjWpMTtjoUmuDo8BRXOXR+nkR889omSvJzeXh7u9Wh2M5D29qpLM7na1c3WB2KZUTmRvY4I/H7\nbVLfh8Ra/BuATmNM0BgzAzwF3HLKPgZwS+z7SwkwADjzXmqHmBu/b4eLKFXKi/L5+jWN/J/9x9nf\nM2x1OLbxZjDEax39fGvzaoodsNpaKrV43LQdH8UYY3Uoi/IHQ6woK6S+wvo7qxNJ/LXAkXmPu+PP\nzfcocCFwFNgH3GOMica3GWC7iLwlInecZ7wqzhcI4SktoCnDJ+P6xqcbKVuSx0PbtNUPsRLfAy+0\ns8xdwFc2rbI6HMt5PW4GJ2bpH5uxOpQzikYN/kCIq1ZXWV7fh+R17n4G2AusANYDj4rI3B1F1xhj\n1hMrFd0lItcu9AIicoeI7BGRPX19fUkKKzPF6vuxm0DscBGlUmlhHndc28SOD3t5+/Cg1eFY7vXO\nfnZ1DXDXdWsct/ZCKsxN3WD3O3g/PD7K4MSsbb6hJ5L4e4CV8x7XxZ+b73bgaRPTCRwELgAwxvTE\n/+4FniFWOvoEY8wTxphWY0xrdXX12f0UWaazd4z+sWnHzr9/tr52dQOVxflZ3+qfa+2vKCvk1g0r\nFz8gC8xN1mb3Ov/JqVUclPh3A80i0hjvsL0VePaUfQ4DNwCIiAdoAYIiUiwi7vjzxcBWYH+ygs9W\nvoC9LqJUKy7I5VubV/NaRz9vxj9A2eiltl72Hhni2zc0W7Jcnx1VleSztCjP9kM6/YF+GiqLWFG+\nxOpQgAQSvzEmDNwNPA98APzKGHNARO4UkTvju/1P4GoR2QfsAP6bMaYf8ACvi8i7wC7gd8aY36fi\nB8kmvkA/dUuXsNIGnUTp8pVNq1jmLuCBbe2O6MhLtrnWfn1FEX9yRZ3V4diGiOCNd/DaVTgS5c3g\ngK1G4CU0JMAY8xzw3CnPPT7v30eJteZPPS4IrDvPGNU80ahhZ3CAz1zksTqUtCrMc3HXdWv4H88e\n4I3OENc02+dDlA7PHzjOgaMjPPCldeRlyLxMydJS4+aZt3swxtiyz+vA0RFGp8O2qe+D3rnrOO8f\nG2F4cjZr6vvz3bphJSvKCvm7F9qyqtUfiRoe3NZOU3UxX7js1AF1yutxMzod5tiwPRfwmSvNbrLR\n1Oma+B3Gn2X1/fkKcl18+4Zm9h4Z4qW2XqvDSZv/eO8o7SfGuHeLF1cG3qV9vuzewesL9OP1lFDt\nts/sqZr4HcYX6KepujhrFtw41Z9cUUd9RREPvJAdtf5wJMoj2zu4oMbN5y5ZbnU4tuT1xCY8s+PU\nDTPhKHu6Bm33DV0Tv4PMRqLsOjhgq1phuuW5crjnhmYOHB3h+QPHrQ4n5Z55p4dg/zj33ujNyDmZ\nkqG8KB9PaYEtW/zvdg8xORuxVZkHNPE7yr6eYcZnIrZrPaTbFy6rpam6mIe2dRCNZm6rfzYS5fsv\ndnBJbRlb12ZXZ/7Z8nrcthzS6esMIQKbmiqsDuVjNPE7iN+GnURWcOUI927x0nZilP/Yd8zqcFLm\n13u6OTIwyX03em05WsVOWjxuOnvHiNisIeAL9HPRilLbrZegid9B/IEQF9S4qSi210Vkhc9dspwL\natw8vK2dcCS6+AEOMzUb4QcvdnB5fTmbW/RO9sV4PW6mZqO2Wq5zajbCO4eHuMqGDTVN/A4xHY6w\nu2sg68s8c3JyhO9u8RLsH+c3e49aHU7SPbXrMMeGp7h/a4u29hMwtyiLner8bx0aZCYSteVnVhO/\nQ7xzeIjpcDQrh3Gezmcu8nBxbSmP7GhnNoNa/ZMzER59KcCmpoqs7sg/G83L7Deyxxfox5UjXNlo\nr/o+aOJ3DH8gRI7ABhteRFYREe6/sYUjA5P8ek+31eEkzf/e2UX/2LS29s9CcUEuKyuW2KrF7w+E\nWFdXRokN10zQxO8Q/kCIS2rLKFuSncvsnc7mlmoury/nBy92MDUbsTqc8zY2HebxV4J8urmKKxv0\nl/zZaPG46TgxZnUYQOz/8d3uYdt+Q9fE7wCTMxHeOTLIJpteRFYSEe7f2sKx4Sme2nXY6nDO20/f\nOMjA+Az3b22xOhTHafa4CfSNMRO2vuy3++AAkaixZX0fNPE7wp5DA8xG7HsRWe3q1ZVsbKzgsZcD\nTM44t9U/PDnLE68G2XLhMtavLLc6HMdp8bgJRw1doXGrQ8EfDJHvyuGKVUutDmVBmvgdwBcIkZsj\nXNlgz4vIanOt/r7Raf73zi6rwzln//hakJGpMPfe6LU6FEc6OWePDTp4fYF+Lqsvt+0qaZr4HcAX\nCHFZfTlF+fbrJLKLDY0VfLq5isdfCTI2HbY6nLM2OD7Dk2908dlLarhoRZnV4ThSU3Uxrhyx/A7e\noYkZDhwdsfU3dE38NjcyNcu+bnveBGI3929tYWB8hp/5uqwO5az96NUg4zNhvrtFW/vnqjDPRUNl\nkeWJf2dwAGPg6jX2/cxq4re53QcHiBpstXqPXa1fWc6WC5fxo1cCDE/OWh1OwvpGp/mZr4tb1q04\nWa5Q5yY2Z4+1I3t2BkMsyXOxrs6+/TSa+G3OFwhRkJvDZfX2vYjs5N4bvYxMhfnH1w9aHUrCfvhy\ngJlIlHu0tX/evB43XaFxS4f2+gL9tDYsJT/XvunVvpEpIJb4r1i11LadRHZz0YoyPntJDU++fpDB\n8Rmrw1nUseFJfv7mIf6vy2tprCq2OhzHa6lxYwx09lrT6u8bnab9xJit6/ugid/WBsdn+ODYiN62\nf5a+u8XL+EyYH70atDqURT32UifGGL59fbPVoWSEuVKZVXX+nUFnrJCnid/GPrqI7N16sBuvx80t\n61bwM18XfaPTVodzWkcGJvjl7iP8aetKVlYUWR1ORmioLCLflWPZ1A2+QAh3QS4Xryi15P0TpYnf\nxnyBEEX5Li6t0+F9Z+ueLV5mIlF++HLA6lBO6wcvdiAi3H39GqtDyRi5rhyaqostm6zNH+hnY1MF\nuS57p1Z7R5fl/MEQGxoryLP5RWRHjVXFfPGyWn7+5iGOD09ZHc4ndPWP829v9/BnG+tZXrbE6nAy\nSkuNNSN7jg5N0hWacMQ3dM0oNtU7MkVn75jW98/Dd25oxhjDYy91Wh3KJzyyo4N8Vw7f2rza6lAy\njtfjpmdoktGp9A7pnVshzwn33CSU+EXkJhFpE5FOEfneAtvLROS3IvKuiBwQkdsTPVYtzB+v79t9\ndICdrawo4k9bV/LU7sN0D9pnZaaOE6P8Zm8PX716FcvchVaHk3Fa4h28HWke2eMLhFhalMcFNfa/\nF2PRxC8iLuAx4GZgLXCbiKw9Zbe7gPeNMeuAzcADIpKf4LFqAf5AiNLCXC5cbu9OIru7+/o1iAg/\n2GGfVv/D2zsoynPxzWu1tZ8KLfHEm846vzEGf6Cfq1ZXkpNj/zUUEmnxbwA6jTFBY8wM8BRwyyn7\nGMAtsVUjSoABIJzgsWoBvkCITU2VuBxwEdnZ8rIl/NnGev717W66+q2ftfHA0WF+t+8Y37imUddO\nTpHa8iUsyXOldWTP4YEJjg5POaLMA5DIrF+1wJF5j7uBjafs8yjwLHAUcANfNsZERSSRY9Upugcn\nODwwwdc/1WB1KBnhW5tX84tdh7n9p7upW2ptR+qh0ASlhbl849NNlsaRyXJyBK+nJK1j+X0BZw29\nTtZ0j58B9gLXA6uBbSLy2tm8gIjcAdwBUF9fn6SwnMnvsIvI7pa5C/nvn1vL0293Wz5zZ2VJPt+5\noVlXUksxr8fNS219aXs/XyDEMncBq6udcfd1Iom/B1g573Fd/Ln5bgf+1hhjgE4ROQhckOCxABhj\nngCeAGhtbTUJRZ+h/IEQlcX5eD0lVoeSMf580yr+fNMqq8NQadJS4+bXb3UzMD6T8pJarL4f4lNr\nKh2zRnIPV4ieAAAOxUlEQVQiNf7dQLOINIpIPnArsbLOfIeBGwBExAO0AMEEj1XzGGPwBUJctdo5\nF5FSdpPOqRs6e8foH5t21NDrRRO/MSYM3A08D3wA/MoYc0BE7hSRO+O7/U/gahHZB+wA/psxpv90\nx6biB8kUB/vHOT4yZfu5PpSys3Qm/rn6vpOGXidU4zfGPAc8d8pzj8/791Fga6LHqtPT8ftKnT9P\naQGlhblpWYbRHwhRW77EUfMt6Z27NuMLhFheVkhDpXMuIqXsRkTiUzekNvFHowZ/MOSoMg9o4rcV\nYww7tb6vVFLMrcYVG3OSGu8fG2F4ctbWyywuRBO/jbSfGCM0PuOYm0CUsrOWGjfDk7P0pnBq7o/m\n53FWaVYTv434Av2A/RdxUMoJmpfFOnhTWef3B0M0VRVTU+asOZc08duILxBiVWURdUu1vq/U+Zq7\nDyZVdf7ZSJQ3gyFHNtQ08dtEJGpiF5GWeZRKisqSAqpKClLW4t/XM8z4TMSRI/A08dvE+0dHGJkK\nO7L1oJRdtdSU0J6i6Znn6vubmipS8vqppInfJrS+r1TyNS9z03FilGg0+SN7/IEQF9S4qSwpSPpr\np5omfpvwB0OsWVaiC3MolUQtNW4mZiL0DE0m9XWnwxF2dw04tqGmid8GZiNRdh0ccNxNIErZ3dzU\nDcmu8+89PMR0OOrYPjlN/DbwXvcQEzMRTfxKJdncyJ5kL8riC4TIEdioiV+dK19nCBHY2OjMi0gp\nu3IX5lFbvoSOJCd+fyDExbVljl1XQRO/DfiDIS6sKWWpLsWnVNI1e0poO5G8kT2TMxHeOTLo2Po+\naOK33NRshD2HBrXMo1SKtHjcBHrHCEeiSXm9PYcGmI0Yx9b3QRO/5d4+PMhMOOq4SZ6Ucgqvx81M\nJEpXaCIpr+cLhMjNEa5scN74/Tma+C22MxDC5fCLSCk7a6lJ7qIsvkCI9SvLKS5I1pLl6aeJ32K+\nQIhLastwFzqzk0gpu1uzrASR5CT+kalZ9nUPObq+D5r4LTU+HWbvkSGt7yuVQoV5LlZVFCUl8e8+\nOEDUOP8Oe038FtpzaJBw1Dj+IlLK7rwed1Ju4vIFQuTn5nB5/dIkRGUdTfwW8gX6yXMJrau0vq9U\nKrXUuOkKTTA1Gzmv1/EHQlxRv5TCPFeSIrOGJn4L+QMhLqtfypJ8Z19EStmd1+MmEjUE+8bP+TUG\nx2d4/9hIRpRmNfFbZHhylv09wxlxESlld3Mjezp6z73cszMYm4Y5E4Zea+K3yK65TiIH3wSilFM0\nVBaTmyPnVef3BUIU5bu4tK48iZFZQxO/RXyBfgrzclhf7/yLSCm7y8/Noam6+LxG9viDIa5sqCDP\n5fy0mdBPICI3iUibiHSKyPcW2P5XIrI3/me/iEREpCK+rUtE9sW37Un2D+BU/kDsIirI1fq+Uung\n9bjPeZbO3pEpOnvHMqY0u2jiFxEX8BhwM7AWuE1E1s7fxxjzv4wx640x64G/Bl4xxgzM2+W6+PbW\nJMbuWKGxaT48PsomLfMolTYtHjdHBiYZnw6f9bH+ufq+A9fXXUgiLf4NQKcxJmiMmQGeAm45w/63\nAb9IRnCZamcw9jsxU1oPSjmBN97B23kOa/D6AyFKC3NZu6I02WFZIpHEXwscmfe4O/7cJ4hIEXAT\n8G/znjbAdhF5S0TuONdAM4kv0E9JQS6X1JZZHYpSWePkalznUO7xBUJsbKrElSPJDssSye6l+Dzw\nxillnmviJaCbgbtE5NqFDhSRO0Rkj4js6evrS3JY9uIPhtjQWEFuBnQSKeUU9RVFFOTm0H6WI3u6\nByc4PDCRUd/QE8k8PcDKeY/r4s8t5FZOKfMYY3rif/cCzxArHX2CMeYJY0yrMaa1uro6gbCc6fjw\nFMG+8Yy6iJRyAleOxBdlObvE7w/E6vuZNLVKIol/N9AsIo0ikk8suT976k4iUgb8AfDv854rFhH3\n3L+BrcD+ZATuVP5gP5BZF5FSTuH1uM96SKc/EKKyOB/vMneKokq/RRO/MSYM3A08D3wA/MoYc0BE\n7hSRO+ft+sfAC8aY+fdEe4DXReRdYBfwO2PM75MXvvP4AyHKi/K4sCYzOomUcpIWj5sTI9MMT8wm\ntL8xBl8gxKbVleRkSH0fIKGVBIwxzwHPnfLc46c8/inw01OeCwLrzivCDOMLhNjUmFkXkVJOMdfB\n2947mtDiR12hCY6PTGXcHfbau5hGRwYm6B6czIi5PpRyorkhnYlO3eALxEqzmdYnp4k/jeY6iTLt\nIlLKKVaUFVJSkJtwnd8XCFFTWkhjVXGKI0svTfxp5Av0U+0uYHV1idWhKJWVRASvpyShFr8xhp2B\nEFetrkQks0qzmvjTZK6T6KqmzLuIlHKSlprYyB5jzBn3az8xRmh8JiNH4GniT5NA3zi9o9Na5lHK\nYs3L3AxOzNI/NnPG/TK1vg+a+NNmbpKnTGw9KOUkc4uyLFbn9wVC1FcUUbe0KB1hpZUm/jTxB/qp\nLV9CfUXmXURKOcnJOXvOUOePRA1vBkMZN4xzjib+NIhGDf4M7SRSymmqSvKpKM4/Y4v//aMjjEyF\nM3botSb+NGg7McrgxGxG1gqVcpq5kT1nSvxz9X1t8atz5svASZ6UcrLYnD1jpx3Z4w+GWF1dzLLS\nwjRHlh6a+NPAH+insaqY5WVLrA5FKUUs8Y9Nhzk6PPWJbbORKLsODmTMalsL0cSfYuFIlDeDA9ra\nV8pGTo7sWaCD973uISZmIhldmtXEn2IHjo4wOh3O2FqhUk40N8XyQnPz+zpjpdmNGfyZ1cSfYnP1\nfV1YXSn7KCvKw1NasGAHrz8Y4sLlpVQU51sQWXpo4k8xfzBEi8dNtbvA6lCUUvMstCjL1GyEPYcG\nM7rMA5r4U2omHGX3Qa3vK2VHLR43HSfGiEQ/Gtnz9uFBZsLRjC/NauJPoXe7h5icjWjiV8qGvDVu\npsNRDg9MnHxuZyBEjsCGpsUXaXEyTfwp5OsMIQKbGjXxK2U3LQtM3eALhLikrpzSwjyrwkoLTfwp\n5A/2c/GKMsqKMvsiUsqJ1iyLrYvREa/zj0+H2XtkKOPr+6CJP2WmZiO8fWhIyzxK2VRxQS4rK5ac\nHNK559Ag4ajJ+Po+aOJPmbcODTITiWriV8rGWuaN7PEF+slzCa0NSy2OKvU08aeIPxAiN0e4siGz\nO4mUcjKvx02wb5yZcBR/IMRlK5dSlJ9rdVgpp4k/RXyBfi6tK6OkIPMvIqWcqqXGTThqeLd7iP09\nw2zKkm/omvhTYGw6zLvdwxk9yZNSmaA5PnXDz3ceImoyc5nFhSSU+EXkJhFpE5FOEfneAtv/SkT2\nxv/sF5GIiFQkcmwm2t01QCRqsuYiUsqpmqqLceUIv3vvGAW5OVxWX251SGmxaOIXERfwGHAzsBa4\nTUTWzt/HGPO/jDHrjTHrgb8GXjHGDCRybCbyB0Lku3K4fFXmdxIp5WSFeS4aKosIRw2tDUspyHVZ\nHVJaJNLi3wB0GmOCxpgZ4CngljPsfxvwi3M8NiP4Av1cvqqcwrzsuIiUcrK5KZqzqTSbSM9jLXBk\n3uNuYONCO4pIEXATcPfZHpsMn//B60zNRlL18gnr7Bvj3i1eq8NQSiXA63Hz3L7jWTX0OtlDTj4P\nvGGMGTjbA0XkDuAOgPr6+nN689XVxcxEoud0bDKtXVHKFy+vtToMpVQCvnhZHbORKOvqsqO+D4kl\n/h5g5bzHdfHnFnIrH5V5zupYY8wTwBMAra2tCy+EuYiHb73sXA5TSmWx+soi/uozF1gdRlolUuPf\nDTSLSKOI5BNL7s+eupOIlAF/APz72R6rlFIqfRZt8RtjwiJyN/A84AKeNMYcEJE749sfj+/6x8AL\nxpjxxY5N9g+hlFIqcWLMOVVVUqq1tdXs2bPH6jCUUsoxROQtY0xrIvvqnbtKKZVlNPErpVSW0cSv\nlFJZRhO/UkplGU38SimVZWw5qkdE+oBD53h4FdCfxHCcTM/Fx+n5+Dg9Hx/JhHOxyhhTnciOtkz8\n50NE9iQ6pCnT6bn4OD0fH6fn4yPZdi601KOUUllGE79SSmWZTEz8T1gdgI3oufg4PR8fp+fjI1l1\nLjKuxq+UUurMMrHFr5RS6gwyJvFn46Lu84nIShF5SUTeF5EDInJP/PkKEdkmIh3xv7NmIWARcYnI\nOyLyH/HH2XwuykXkX0XkQxH5QESuyvLzcW/8c7JfRH4hIoXZdD4yIvFn66LupwgD9xtj1gKbgLvi\n5+B7wA5jTDOwI/44W9wDfDDvcTafi0eA3xtjLgDWETsvWXk+RKQW+A7Qaoy5mNiU8beSRecjIxI/\nWbqo+3zGmGPGmLfj/x4l9sGuJXYefhbf7WfAF6yJML1EpA74HPAP857O1nNRBlwL/COAMWbGGDNE\nlp6PuFxgiYjkAkXAUbLofGRK4l9oUfesXfRWRBqAy4A3AY8x5lh803HAY1FY6fYw8H8D8xdhztZz\n0Qj0AT+Jl77+QUSKydLzYYzpAf4OOAwcA4aNMS+QRecjUxK/ihOREuDfgO8aY0bmbzOxIVwZP4xL\nRP4Q6DXGvHW6fbLlXMTlApcDPzTGXAaMc0oZI5vOR7x2fwuxX4grgGIR+cr8fTL9fGRK4j+bBeEz\nlojkEUv6/2yMeTr+9AkRWR7fvhzotSq+NPoU8Eci0kWs7He9iPyc7DwXEPsG3G2MeTP++F+J/SLI\n1vOxBThojOkzxswCTwNXk0XnI1MSf9Yv6i4iQqyG+4Ex5sF5m54F/iL+778A/j3dsaWbMeavjTF1\nxpgGYtfCi8aYr5CF5wLAGHMcOCIiLfGnbgDeJ0vPB7ESzyYRKYp/bm4g1ieWNecjY27gEpHPEqvr\nzi3q/v9ZHFJaicg1wGvAPj6qa/8NsTr/r4B6YjOe/qkxZsCSIC0gIpuB/2qM+UMRqSRLz4WIrCfW\n0Z0PBIHbiTX8svV8/L/Al4mNhnsH+EughCw5HxmT+JVSSiUmU0o9SimlEqSJXymlsowmfqWUyjKa\n+JVSKsto4ldKqSyjiV8ppbKMJn6llMoymviVUirL/P83L5hchdhx4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107501828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x * plot_every for x in range(len(validation_plot_losses))], validation_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1 = None\n",
      "R2 = None\n",
      "R3 = None\n",
      "R4 = None\n",
      "R5 = None\n",
      "R6 = None\n",
      "R7 = None\n",
      "\n",
      "?? = ??(??, ??)\n",
      "?? = ??(??, ??)\n",
      "?? = ??(??, ??)\n",
      "?? = ??(??, ??)\n",
      "?? = ??(??, ??)\n",
      "?? = ??(??, ??)\n",
      "?? = ??(??, ??)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0dbe054d591f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mprintProgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mcompareOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# Original Add Program\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-0dbe054d591f>\u001b[0m in \u001b[0;36mcompareOutput\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0msoftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mgetBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0morig_register\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mmatch_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# print(controller.first_arg)\n",
    "# print(controller.second_arg)\n",
    "# print(controller.output)\n",
    "# print(controller.instruction)\n",
    "# print(controller.registers)\n",
    "\n",
    "cutoff = 0.7\n",
    "\n",
    "def getBest(vec):\n",
    "    maxVal, index = torch.max(vec, 0)\n",
    "    if maxVal.data[0] > cutoff:\n",
    "        return index.data[0]\n",
    "\n",
    "def bestRegister(vec):\n",
    "    index = getBest(vec)\n",
    "    if index is not None:\n",
    "        return \"R\" + str(1 + index)\n",
    "    return \"??\"\n",
    "    \n",
    "def bestInstruction(vec):\n",
    "    ops = [ \n",
    "        \"STOP\",\n",
    "        \"ZERO\",\n",
    "        \"INC\",\n",
    "        \"ADD\",\n",
    "        \"SUB\",\n",
    "        \"DEC\",\n",
    "        \"MIN\",\n",
    "        \"MAX\",\n",
    "        \"READ\",\n",
    "        \"WRITE\",\n",
    "        \"JEZ\"\n",
    "    ]\n",
    "    index = getBest(vec)\n",
    "    if index is not None:\n",
    "        return ops[index]\n",
    "    return \"??\"\n",
    "\n",
    "# registers = controller.registers\n",
    "# Increment task\n",
    "\n",
    "orig_register = torch.IntTensor([6,0,0,0,0,0,0])\n",
    "orig_output = torch.IntTensor([5,1,1,5,5,4,6])\n",
    "orig_instruction = torch.IntTensor([6,0,6,3,6,2,6])\n",
    "orig_first = torch.IntTensor([1,6,3,6,5,6,6])\n",
    "orig_second = torch.IntTensor([8,10,2,9,2,10,0])\n",
    "\n",
    "#orig_register = [6,2,0,1,0,0]\n",
    "#orig_output = [4,3,5,3,4,5,5,5]\n",
    "#orig_instruction = [8,8,10,5,2,10,9,0]\n",
    "#orig_first = [4,3,3,3,4,2,2,5]\n",
    "#orig_second = [5,5,0,5,5,1,4,5]\n",
    "\n",
    "# print(controller.output)\n",
    "# print(controller.first_arg)\n",
    "# output = controller.output\n",
    "# first = controller.first_arg\n",
    "# second = controller.second_arg\n",
    "instruction = controller.instruction\n",
    "_, R, M = controller.registers.size()\n",
    "    \n",
    "def printProgram():   \n",
    "    # Print registers\n",
    "    for i in range(R):\n",
    "        print(\"R\" + str(i + 1) + \" = \" + str(getBest(controller.registers[0, i,:])))\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Print the actual program\n",
    "    for i in range (M):\n",
    "        print(bestRegister(controller.output[0, :, i]) + \" = \" + \n",
    "              bestInstruction(controller.instruction[0, :, i]) + \"(\" +\n",
    "              bestRegister(controller.first_arg[0, :, i]) + \", \" +\n",
    "              bestRegister(controller.second_arg[0, :, i]) + \")\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def compareOutput():\n",
    "    # compare our output to theirs\n",
    "    # we get one point for every matching number\n",
    "    match_count = 0\n",
    "    softmax = nn.Softmax(1)\n",
    "    for i in range(R):\n",
    "        if getBest(controller.registers[0, i,:]) == orig_register[i]:\n",
    "            match_count += 1\n",
    "    for i in range (M):\n",
    "        if getBest(softmax(controller.output)[0, :, i]) == orig_output[i]:\n",
    "            match_count += 1\n",
    "        if getBest(softmax(controller.instruction)[0, :, i]) == orig_instruction[i]:\n",
    "            match_count += 1\n",
    "        if getBest(softmax(controller.first_arg)[0, :, i]) == orig_first[i]:\n",
    "            match_count += 1\n",
    "        if getBest(softmax(controller.second_arg)[0, :, i]) == orig_second[i]:\n",
    "            match_count += 1\n",
    "\n",
    "    percent_orig = match_count / (len(orig_register) + len(orig_output) + \n",
    "                                           len(orig_instruction) + len(orig_first) + len(orig_second))\n",
    "    return percent_orig\n",
    "#     print(\"PERCENT MATCH\", percent_orig)\n",
    "    \n",
    "printProgram()\n",
    "compareOutput()\n",
    "\n",
    "# Original Add Program   \n",
    "# R1 = 6\n",
    "# R2 = 2\n",
    "# R3 = 0\n",
    "# R4 = 1\n",
    "# R5 = 0\n",
    "# R6 = 0\n",
    "\n",
    "\n",
    "# R5 = READ(R5, R6)\n",
    "# R4 = READ(R4, R6)\n",
    "# R6 = JEZ(R4, R1)\n",
    "# R4 = DEC(R4, R6)\n",
    "# R5 = INC(R5, R6)\n",
    "# R6 = JEZ(R3, R2)\n",
    "# R6 = WRITE(R3, R5)\n",
    "# R6 = STOP(R6, R6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "LR is set to 0.001\n",
      "\n",
      "Training complete in 0m 0s\n",
      "Best loss:  inf\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6aadc51ac90e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mforward_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         batch_size = 5) # In the paper, they used batch sizes of 1 or 5\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpercent_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompareOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpercent_orig\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mnum_original_convergences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-0dbe054d591f>\u001b[0m in \u001b[0;36mcompareOutput\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0msoftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mgetBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0morig_register\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mmatch_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Test a bunch of times\n",
    "num_trials = 20\n",
    "\n",
    "num_original_convergences = 0\n",
    "num_0_losses = 0\n",
    "num_better_convergences = 0\n",
    "for i in range(num_trials):\n",
    "    # print(\"Trial \", i)\n",
    "    best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "        controller, \n",
    "        data_loader,  \n",
    "        optimizer, \n",
    "        num_epochs = 1, \n",
    "        # print_every = 10000, \n",
    "        plot_every = plot_every, \n",
    "        deep_copy_desired = False, \n",
    "        validation_criterion = anc_validation_criterion, \n",
    "        forward_train = True, \n",
    "        batch_size = 5) # In the paper, they used batch sizes of 1 or 5\n",
    "    percent_orig = compareOutput()\n",
    "    if percent_orig > .99:\n",
    "        num_original_convergences += 1\n",
    "    end_losses = validation_plot_losses[-2:]\n",
    "    if sum(end_losses) < .01:\n",
    "        num_0_losses += 1\n",
    "    if percent_orig < .99 and sum(end_losses) < .01:\n",
    "        num_\n",
    "print(\"LOSS CONVERGENCES\", num_0_losses * 1.0 / num_trials)\n",
    "print(\"ORIG CONVERGENCES\", num_original_convergences * 1.0 / num_trials)\n",
    "print(\"BETTER CONVERGENCES\", num_better_convergences * 1.0 / num_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
