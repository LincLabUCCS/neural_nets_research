{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/oliviawatkins/Documents/Schoolwork/NN/neural_nets_research\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets_library import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    Contains the two learnable parts of the model in four independent, fully connected layers.\n",
    "    First the initial values for the registers and instruction registers and second the \n",
    "    parameters that computes the required distributions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 first_arg = None, \n",
    "                 second_arg = None, \n",
    "                 output = None, \n",
    "                 instruction = None, \n",
    "                 initial_registers = None, \n",
    "                 stop_threshold = .99, \n",
    "                 blur = 3, \n",
    "                 correctness_weight = .25, \n",
    "                 halting_weight = .25, \n",
    "                 confidence_weight = .25, \n",
    "                 efficiency_weight = .25,\n",
    "                 t_max = 100):\n",
    "        #TODO: Read over ANC paper, check if there are more reasonable default initial values.\n",
    "        \"\"\"\n",
    "        Initialize a bunch of constants and pass in matrices defining a program.\n",
    "        \n",
    "        :param first_arg: Matrix with the 1st register argument for each timestep stored in the columns (1xRxM)\n",
    "        :param second_arg: Matrix with the 2nd register argument for each timestep stored in the columns (1xRxM)\n",
    "        :param output: Matrix with the output register for each timestep stored in the columns (1xRxM)\n",
    "        :param instruction: Matrix with the instruction for each timestep stored in the columns (1xNxM)\n",
    "        :param initial_registers: Matrix where each row is a distribution over the value in one register (1xRxM)\n",
    "        :param stop_threshold: The stop probability threshold at which the controller should stop running\n",
    "        :param blur: The factor our one-hot vectors are be multiplied by before they're softmaxed to add blur\n",
    "        :param correctness_weight: Weight given to the correctness component of the loss function\n",
    "        :param halting_weight: Weight given to the halting component of the loss function\n",
    "        :param confidence_weight: Weight given to the confidence component of the loss function\n",
    "        :param efficiency_weight: Weight given to the efficiency component of the loss function\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Controller, self).__init__()\n",
    "        \n",
    "        # Initialize dimension constants\n",
    "        B, R, M = initial_registers.size()\n",
    "        self.M = M\n",
    "        self.R = R\n",
    "        \n",
    "        # Initialize loss function weights\n",
    "        # In the ANC paper, these scalars are called, alpha, beta, gamma, and delta\n",
    "        self.correctness_weight = correctness_weight\n",
    "        self.halting_weight = halting_weight\n",
    "        self.confidence_weight = confidence_weight\n",
    "        self.efficiency_weight = efficiency_weight\n",
    "        \n",
    "        # And yet more initialized constants... yeah, there are a bunch, I know.\n",
    "        self.t_max = t_max\n",
    "        self.blur_factor = blur\n",
    "        self.stop_threshold = stop_threshold\n",
    "        \n",
    "        # Blur matrices - i.e. give each different operation/argument/register value some nonzero probability\n",
    "        if blur is not None:\n",
    "            first_arg = self.blur(first_arg, blur, 1)\n",
    "            second_arg = self.blur(second_arg, blur, 1)\n",
    "            output = self.blur(output, blur, 1)\n",
    "            instruction = self.blur(instruction, blur, 1)\n",
    "            initial_registers = self.blur(initial_registers, blur, 2)\n",
    "            \n",
    "        # Initialize parameters.  These are the things that are going to be optimized. \n",
    "        self.first_arg = nn.Parameter(first_arg.data)\n",
    "        self.second_arg = nn.Parameter(second_arg.data)\n",
    "        self.output = nn.Parameter(output.data)\n",
    "        self.instruction = nn.Parameter(instruction.data) \n",
    "        self.registers = nn.Parameter(initial_registers.data)\n",
    "        \n",
    "        # Machine initialization\n",
    "        self.machine = Machine(B, M, R)\n",
    "    \n",
    "    def blur(self, matrix, scale_factor, dimension):\n",
    "        \"\"\"\n",
    "        Takes a matrix, each row (or column) of which is a one-hot vector.\n",
    "        Multiply each 1 by a constant and then softmax it, which \n",
    "        effectively \"blurs\" the matrix a little bit.\n",
    "        \n",
    "        :param matrix: Matrix to blur\n",
    "        :param scale_factor: Constant to multiply the matrix by before it's softmaxed\n",
    "        :param dimension: Dimension to softmax over\n",
    "        \n",
    "        :return: Blurred matrix\n",
    "        \"\"\"\n",
    "        matrix = scale_factor * matrix\n",
    "        softmax = nn.Softmax(dimension)\n",
    "        return softmax(Variable(matrix))    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def forward(self, input, train):\n",
    "        \"\"\"\n",
    "        Runs the controller on a certain input memory matrix.\n",
    "        It either returns the loss or the output memory.\n",
    "        \n",
    "        :param input: A three-tuple of three MxM matrices: (memory matrix, output_memory, output_mask)\n",
    "        \n",
    "        :return: If train is true, return the loss. Otherwise, return the output matrix\n",
    "        \"\"\"\n",
    "        # Program's initial memory\n",
    "        self.memory = input[0]\n",
    "        # Desired output memory\n",
    "        self.output_memory = input[1]\n",
    "        # Mask with 1's in the rows of the output memory matrix which actually contain the answer.\n",
    "        self.output_mask = input[2]\n",
    "        \n",
    "        # Initialize instruction regiser (1xMx1)\n",
    "        self.register_buffer('IR', torch.zeros(1, M, 1))\n",
    "        self.IR[0, 0, 0] = 1\n",
    "        \n",
    "        # Blur memory and instruction register\n",
    "        if self.blur_factor is not None:\n",
    "            self.memory = self.blur(self.memory, self.blur_factor, 2) \n",
    "            IR = self.blur(self.IR, self.blur_factor, 1)\n",
    "        \n",
    "        efficiency_loss = 0\n",
    "        confidence_loss = 0\n",
    "        self.stop_probability = 0\n",
    "        \n",
    "        # Copy registers so we aren't using the values from the previous iteration.\n",
    "        registers = self.registers\n",
    "        \n",
    "        t = 0 \n",
    "        # Run the program, one timestep at a time, until the program terminates or whe time out\n",
    "        while t < self.t_max and self.stop_probability < self.stop_threshold: \n",
    "            a = torch.bmm(self.first_arg, IR)\n",
    "            b = torch.bmm(self.second_arg, IR)\n",
    "            o = torch.bmm(self.output, IR)\n",
    "            e = torch.bmm(self.instruction, IR)\n",
    "            \n",
    "            # Update memory, registers, and IR after machine operation\n",
    "            self.memory, registers, IR, stop_prob = self.machine(e, a, b, o, self.memory, registers, IR)\n",
    "            self.stop_probability = self.stop_probability + stop_prob[0]            \n",
    "            \n",
    "            # If we're training, calculate loss\n",
    "            if train:\n",
    "                new_efficiency, new_confidence = self.timestep_loss()\n",
    "                efficiency_loss += new_efficiency\n",
    "                confidence_loss += new_confidence\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        # If we're training, return loss.  Otherwise return memory.\n",
    "        if train:\n",
    "            correctness_loss, halting_loss = self.final_loss()\n",
    "            total_loss  = (\n",
    "                self.correctness_weight * correctness_loss + \n",
    "                self.halting_weight * halting_loss + \n",
    "                self.confidence_weight * confidence_loss + \n",
    "                self.efficiency_weight * efficiency_loss)\n",
    "            return (self.memory, torch.sum(self.registers)) #total_loss #TODO: Aditya @ Rakia - implement loss, then replace this\n",
    "        else:\n",
    "            return (self.memory, None)\n",
    "        \n",
    "        \n",
    "    def timestep_loss(self):\n",
    "        \"\"\"\n",
    "        @ Rakia @ Aditya feel free to use this function definition or not.  My main thought was that this would \n",
    "        compute the types of loss which get updated every timestep.\n",
    "        \"\"\"\n",
    "        # TODO: Insert losses\n",
    "        return (1,1)\n",
    "    \n",
    "    def final_loss(self):\n",
    "        \"\"\"\n",
    "        @ Rakia @ Aditya feel free to use this function definition or not.  My main thought was that this would \n",
    "        compute the types of loss which get updated every timestep.\n",
    "        \"\"\"\n",
    "        # TODO: insert losses\n",
    "        return (1,1)\n",
    "   \n",
    "\n",
    "    def lossfunctions(self, cmatrix, tjmatrix):\n",
    "        \"\"\" compute four diferent loss functions and return a weighted average of the four measuring correctness, \n",
    "        halting, efficiency, and confidence\"\"\"\n",
    "        \n",
    "        self.matrix1 = ((cmatrix)*(self.program_matrix4 - self.memory)^2)\n",
    "        self.correctness += self.matrix1\n",
    "        \n",
    "        \n",
    "            \n",
    "        self.efficiency += (1-self.stop_probability[self.t])\n",
    "        \n",
    "        self.confidence += torch.matmult(self.stop_probability[self.t] - self.stop_probability[self.t -1], matrix1)\n",
    "        \n",
    "      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(nn.Module):\n",
    "    \"\"\"\n",
    "    Parent class for our binary operations\n",
    "    \"\"\"\n",
    "    def __init__(self, M):\n",
    "        \"\"\"\n",
    "        Initialize the memory length (needed so we can mod our answer in case it exceeds the range 0-M-1)\n",
    "        Also calculate the output matrix for the operation\n",
    "        \n",
    "        :param M: Memory length\n",
    "        \"\"\"\n",
    "        super(Operation, self).__init__()\n",
    "        self.M = M\n",
    "        \n",
    "        # Create a MxMxM matrix where the (i,j,k) cell is 1 iff operation(i,j) = k.\n",
    "        self.outputs = torch.zeros(M, M, M)\n",
    "        for i in range(M):\n",
    "            for j in range(M):\n",
    "                val = self.compute(i, j)\n",
    "                self.outputs[val][i][j] = 1\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        \"\"\" \n",
    "        Perform the binary operation.  The arguments may or may not be used.\n",
    "        \n",
    "        :param x: First argument\n",
    "        :param y: Second argument\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        :return: The output matrix\n",
    "        \"\"\"\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Operation):\n",
    "\n",
    "    def __init__(self, M):\n",
    "        super(Add, self).__init__(M)\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        return (x + y) % self.M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stop(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Stop, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jump(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Jump, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0 # Actual jump happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decrement(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Decrement, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x - 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Increment(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Increment, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return (x + 1) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Max, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return max(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Min(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Min, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return min(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Read, self).__init__(M)\n",
    "        self.outputs = torch.zeros(M, M, M) # Leave output matrix blank since we're gonna do the reading elsewhere\n",
    "\n",
    "    def compute(self, x, _):\n",
    "        return 0 # Actual reading happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtract(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Subtract, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return (x - y) % self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Write(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Write, self).__init__(M)\n",
    "\n",
    "    def compute(self, x, y):\n",
    "        return 0 # Actual write happens in the Machine class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zero(Operation):\n",
    "    \n",
    "    def __init__(self, M):\n",
    "        super(Zero, self).__init__(M)\n",
    "\n",
    "    def compute(self, _1, _2):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine(nn.Module):\n",
    "    \"\"\"\n",
    "    The Machine executes assembly instructions passed to it by the Controller.\n",
    "    It updates the given memory, registers, and instruction pointer.\n",
    "    The Machine doesn't have any learnable parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, B, M, R):\n",
    "        \"\"\"\n",
    "        Initializes dimensions, operations, and counters\n",
    "        \n",
    "        :param B: Batch size (meant to be 1)\n",
    "        :param M: Memory length.  Integer values also take on values 0-M-1.  M is also the program length.\n",
    "        :param R: Number of registers\n",
    "        \"\"\"\n",
    "        super(Machine, self).__init__()\n",
    "        \n",
    "        # Store parameters as class variables\n",
    "        self.R = R # Number of registers\n",
    "        self.M = M # Memory length (also largest number)\n",
    "        self.B = B # Batch size\n",
    "        \n",
    "        # Start off with 0 probability of stopping\n",
    "        self.stop_probability = torch.zeros(B)\n",
    "        \n",
    "        # List of ops (must be in same order as the original ANC paper so compilation works right)\n",
    "        self.ops = [ \n",
    "            Stop(M),\n",
    "            Zero(M),\n",
    "            Increment(M),\n",
    "            Add(M),\n",
    "            Subtract(M),\n",
    "            Decrement(M),\n",
    "            Min(M),\n",
    "            Max(M),\n",
    "            Read(M),\n",
    "            Write(M),\n",
    "            Jump(M)\n",
    "        ]\n",
    "        \n",
    "        # Number of instructions\n",
    "        self.N = len(self.ops)\n",
    "        \n",
    "        # Create a 4D matrix composed of the output matrices of each of the ops\n",
    "        self.outputs = Variable(torch.zeros(self.N, M, M, M))\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            op = self.ops[i]\n",
    "            self.outputs[i] = op()\n",
    "            \n",
    "        # Add an extra batch dimension\n",
    "        self.outputs = torch.unsqueeze(self.outputs, 0)\n",
    "        self.outputs = self.outputs.expand(B, -1, -1, -1, -1)\n",
    "        \n",
    "        # Keep track of ops which will be handled specially\n",
    "        self.jump_index = 10\n",
    "        self.stop_index = 0\n",
    "        self.write_index = 9\n",
    "        self.read_index = 8\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, e, a, b, o, memory, registers, IR):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run the Machine for one timestep (corresponding to the execution of one line of Assembly).\n",
    "        The first four parameter names correspond to the vector names used in the original ANC paper\n",
    "        \n",
    "        :param e: Probability distribution over the instruction being executed (BxMx1)\n",
    "        :param a: Probability distribution over the first argument register (length BxRx1)\n",
    "        :param b: Probability distribution over the second argument register (length BxRx1)\n",
    "        :param o: Probability distribution over the first argument register (length BxRx1)\n",
    "        :param memory: Memory matrix (size BxMxM)\n",
    "        :param registers: Register matrix (size BxRxM)\n",
    "        :param IR: Instruction Register (length BxM)\n",
    "        \n",
    "        :return: The memory, registers, and instruction register after the timestep\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dimensions B x 1 x R -> B x 1 x R\n",
    "        a = torch.transpose(a, 1, 2)\n",
    "        b = torch.transpose(b, 1, 2)\n",
    "        \n",
    "        # Calculate distributions over the two argument values by multiplying each \n",
    "        # register by the probability that register is being used.\n",
    "        arg1 = torch.bmm(a, registers)\n",
    "        arg2 = torch.bmm(b, registers)\n",
    "        \n",
    "        # Multiply the output matrix by the arg1 and arg2 vectors to take into account\n",
    "        # Before we do this, we're going to have to do a bunch of dimension squishing.\n",
    "        \n",
    "        # arg1_long dimensions: B x 1 x M --> B x 1 x 1 x 1 x M\n",
    "        arg1_long = torch.unsqueeze(arg1, 1)\n",
    "        arg1_long = torch.unsqueeze(arg1_long, 1)\n",
    "        \n",
    "        outputs_x_arg1 = torch.matmul(arg1_long, self.outputs)\n",
    "        \n",
    "        # outputs_x_arg1 dimensions: B x N x M x 1 x M -> B x N x M x M\n",
    "        outputs_x_arg1 = torch.squeeze(outputs_x_arg1, 3)\n",
    "        \n",
    "        # arg2_long dimensions: B x 1 x M --> B x 1 x M x 1\n",
    "        arg2_long = torch.unsqueeze(arg2, 3)\n",
    "        \n",
    "        outputs_x_args = torch.matmul(outputs_x_arg1, arg2_long)\n",
    "        \n",
    "        # outputs_x_args dimensions: B x N x M x 1 -> B x N x M\n",
    "        outputs_x_args = torch.squeeze(outputs_x_args, 3)\n",
    "        \n",
    "        # e dimensions B x N x 1 -> B x 1 x N\n",
    "        e = torch.transpose(e, 1, 2)\n",
    "        \n",
    "        # read_vec dimensions B x 1 -> B x 1 x 1\n",
    "        read_vec =  e[:, :, self.read_index]\n",
    "        read_vec = read_vec.unsqueeze(1)\n",
    "        \n",
    "        # Length Bx1xM vector over the output of the operation\n",
    "        out_vec = torch.matmul(e, outputs_x_args)\n",
    "        \n",
    "        # Deal with memory reads separately\n",
    "        out_vec = out_vec + read_vec * torch.matmul(arg1, memory)        \n",
    "        \n",
    "        # Update our memory, registers, instruction register, and stopping probability\n",
    "        memory = self.writeMemory(e, o, memory, arg1, arg2)\n",
    "        registers = self.writeRegisters(out_vec, o, registers)\n",
    "        IR = self.updateIR(e, IR, arg1, arg2)\n",
    "        stop_prob = self.getStop(e)\n",
    "        \n",
    "        return(memory, registers, IR, stop_prob)\n",
    "        \n",
    "        \n",
    "    def writeRegisters(self, out, o, registers):\n",
    "        \"\"\"\n",
    "        Write the result of our operation to our registers.\n",
    "        \n",
    "        :param out: Probability distribution over the output value (Bx1xM)\n",
    "        :param o: Probability distribution over the output register (BxRx1)\n",
    "        :param Registers: register matrix (BxRxM)\n",
    "        \n",
    "        :return: The updated registers (BxRxM)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Multiply probability of writing to each output register by the distribution over the value we're writing there.\n",
    "        new_register_vals = torch.matmul(o, out)\n",
    "        \n",
    "        # Multiply each original register cell by the probabilty of not writing to that register\n",
    "        old_register_vals = (1-o).expand(self.B, self.R, self.M) * registers\n",
    "        \n",
    "        # Take a weighted sum over the old and new register values\n",
    "        registers =  new_register_vals + old_register_vals\n",
    "        \n",
    "        return registers\n",
    "    \n",
    "    def updateIR(self, e, IR, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the instruction register\n",
    "        \n",
    "        :param e: Distribution over the current instruction (BxNx1)\n",
    "        :param IR: Instruction register (length BxMx1)\n",
    "        :param arg1: Distribution over the first argument value (length BxMx1)\n",
    "        :param arg2: Distribution over the second argument value (length BxMx1)\n",
    "        \n",
    "        :return: The updated instruction register (BxMx1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dimensions B x 1 x M -> B x M x 1\n",
    "        arg2 = arg2.transpose(1, 2)\n",
    "        \n",
    "        # Probability that we're on the jump instruction\n",
    "        jump_probability = e[:, :, self.jump_index]\n",
    "        \n",
    "        # Probability that the first argument is 0\n",
    "        is_zero = arg1[:, :, 0]\n",
    "        \n",
    "        # Slicing lost a dimension.  Let's add it back\n",
    "        jump_probability = torch.unsqueeze(jump_probability, 1)\n",
    "        is_zero = torch.unsqueeze(is_zero, 1)\n",
    "        \n",
    "        # If we're not jumping, just shift IR by one slot\n",
    "        wraparound = IR[:, -1]\n",
    "        normal_instructions = IR[:, :-1]\n",
    "        \n",
    "        # For whatever reason, when you chop off one row/column, that dimension disappears.  Add it back.\n",
    "        wraparound = wraparound.unsqueeze(1)\n",
    "        IR_no_jump = torch.cat([wraparound, normal_instructions], 1)\n",
    "        \n",
    "        # If we are on a jump instruction, check whether the argument's 0.\n",
    "        # If it is, jump to the location specified by arg2.  Otherwise, increment like normal.\n",
    "        IR_jump = arg2 * is_zero + (1 - is_zero) * IR_no_jump\n",
    "        \n",
    "        # Take a weighted sum of the instruction register with and without jumping\n",
    "        IR = IR_no_jump * (1 - jump_probability) + IR_jump * jump_probability\n",
    "        \n",
    "        return IR\n",
    "    \n",
    "    def writeMemory(self, e, o, mem_orig, arg1, arg2):\n",
    "        \"\"\"\n",
    "        Update the memory\n",
    "        \n",
    "        :param e: Distribution over the current instruction (B x1xM)\n",
    "        :param mem_orig: Current memory matrix (BxMxM)\n",
    "        :param arg1: Distribution over the first argument value (Bx1xM)\n",
    "        :param arg2: Distribution over the second argument value (Bx1xM)\n",
    "        \n",
    "        :return: The updated memory matrix (BxMxM)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Probability that we're on the write instruction\n",
    "        write_probability = e[:,:, self.write_index]\n",
    "        \n",
    "        # write_probability dimensions: Bx1 -> B x 1 x 1\n",
    "        write_probability = torch.unsqueeze(write_probability, 1)\n",
    "        \n",
    "        # arg1 dimensions: B x 1 x M -> B x M x 1\n",
    "        arg1 = torch.transpose(arg1, 1, 2)\n",
    "        \n",
    "        # If we are on a write instruction, write the value arg2 in register arg1. Otherwise, leave memory as is.\n",
    "        mem_changed = torch.bmm(arg1, arg2)\n",
    "        mem_unchanged = mem_orig * (1-arg1).expand(-1, -1, self.M)\n",
    "        mem_write = mem_changed + mem_unchanged\n",
    "        \n",
    "        # Take a weighted sum over the new memory and old memory\n",
    "        memory = mem_orig * (1 - write_probability) + mem_write * write_probability\n",
    "\n",
    "        return memory\n",
    "        \n",
    "    def getStop(self, e):\n",
    "        \"\"\"\n",
    "        Obtain the probability that we will stop at this timestep based on the probability that we are running the STOP op.\n",
    "        \n",
    "        :param e: distribution over the current instruction (length Bx1xM)\n",
    "        \n",
    "        :return: probability representing whether the controller should stop.\n",
    "        \"\"\"\n",
    "        return e[:, :, self.stop_index].data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hotify(vec, length, dimension):\n",
    "    \"\"\"\n",
    "    Turn a tensor of integers into a matrix of one-hot vectors.\n",
    "    \n",
    "    :param vec: The vector to be converted.\n",
    "    :param length: One dimension of the matrix (the other is the length of vec)\n",
    "    :param dimension: Which dimension stores the elements of vec.  If 0, they're stored in the rows.  If 1, the columns.\n",
    "    \n",
    "    :return A matrix of one-hot vectors, each row or column corresponding to one element of vec\n",
    "    \"\"\"\n",
    "    x = vec.size()[0]\n",
    "    if dimension == 0:\n",
    "        binary_vec = torch.zeros(x, length)\n",
    "        for i in range(x):\n",
    "            binary_vec[i][vec[i]] = 1\n",
    "        return binary_vec\n",
    "    elif dimension == 1:\n",
    "        binary_vec = torch.zeros(length, x)\n",
    "        for i in range(x):\n",
    "            binary_vec[vec[i]][i] = 1\n",
    "        return binary_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition task\n",
    "# Generate this by running the instructions here (but with the addition program file): https://github.com/aditya-khant/neural-assembly-compiler\n",
    "# Then get rid of the .cuda in each of the tensors since we (or at least I) don't have cuda\n",
    "init_registers = torch.IntTensor([6,2,0,1,0,0]) # Length R, should be RxM\n",
    "first_arg = torch.IntTensor([4,3,3,3,4,2,2,5]) # Length M, should be RxM\n",
    "second_arg = torch.IntTensor([5,5,0,5,5,1,4,5]) # Length M, should be RxM\n",
    "target = torch.IntTensor([4,3,5,3,4,5,5,5]) # Length M, should be RxM\n",
    "instruction = torch.IntTensor([8,8,10,5,2,10,9,0]) # Length M, should be NxM\n",
    "\n",
    "# Get dimensions we'll need\n",
    "M = first_arg.size()[0]\n",
    "R = init_registers.size()[0]\n",
    "N = 11\n",
    "\n",
    "# Turn the given tensors into matrices of one-hot vectors.\n",
    "init_registers = one_hotify(init_registers, M, 0)\n",
    "first_arg = one_hotify(first_arg, R, 1)\n",
    "second_arg = one_hotify(second_arg, R, 1)\n",
    "target = one_hotify(target, R, 1)\n",
    "instruction = one_hotify(instruction, N, 1)\n",
    "\n",
    "# Add a fake first batch\n",
    "init_registers = init_registers.unsqueeze(0)\n",
    "first_arg = first_arg.unsqueeze(0)\n",
    "second_arg = second_arg.unsqueeze(0)\n",
    "target = target.unsqueeze(0)\n",
    "instruction = instruction.unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# An example starting memory.  In this case, we're adding 3+4 and expect 7.\n",
    "initial_memory = torch.IntTensor([3,4,0,0,0,0,0,0])\n",
    "initial_memory = one_hotify(initial_memory, M, 0)\n",
    "output_memory = torch.zeros(M, M)\n",
    "output_memory[0][7] = 1\n",
    "\n",
    "# Output mask has ones in the rows of the memory matrix where the answer will be stored.\n",
    "output_mask = torch.zeros(M, M)\n",
    "output_mask[0] = torch.ones(M)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTaskDataset(data.Dataset):\n",
    "    def __init__(self, M, num_examples):\n",
    "        \"\"\"\n",
    "        Generate a dataset for the addition task by randomly choosing two numbers in the allowed range\n",
    "        and creating the initial/final matrices for adding them.\n",
    "        \n",
    "        :param M: The allowable range of integers (from 0 to M-1)\n",
    "        :param num_examples: The number of training examples to be generated\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_list = []\n",
    "        \n",
    "        for i in range(num_examples):\n",
    "            # An example starting memory.  In this case, we're adding 3+4 and expect 7.\n",
    "            first_addend = random.randint(0, M-1)\n",
    "            second_addend = random.randint(0, M-1)\n",
    "            initial_memory = torch.zeros(M, M)\n",
    "            initial_memory[0][first_addend] = 1\n",
    "            initial_memory[1][second_addend] = 1\n",
    "            \n",
    "            output_memory = torch.zeros(M, M)\n",
    "            output_memory[0][(first_addend + second_addend) % M] = 1\n",
    "\n",
    "            # Output mask has ones in the rows of the memory matrix where the answer will be stored.\n",
    "            output_mask = torch.zeros(M, M)\n",
    "            output_mask[0] = torch.ones(M)\n",
    "            \n",
    "            self.input_list.append((initial_memory, output_memory, output_mask))\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Get the i^th element of the dataset.\n",
    "        \n",
    "        :param i: The index of the element to be returned.\n",
    "        :return A tuple containing i^th element of the dataset.\n",
    "        \"\"\"\n",
    "        return self.input_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "LR is set to 0.001\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bmm(): argument 'mat2' (position 1) must be Variable, not torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-cd02aad0419b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mvalidation_criterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manc_validation_criterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mforward_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     batch_size=10)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#kangaroo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Schoolwork/NN/neural_nets_research/neural_nets_library/training.py\u001b[0m in \u001b[0;36mtrain_model_anc\u001b[0;34m(model, dset_loader, optimizer, lr_scheduler, num_epochs, print_every, plot_every, deep_copy_desired, validation_criterion, forward_train, batch_size)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalidation_criterion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-91ca9d73be1d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, train)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Run the program, one timestep at a time, until the program terminates or whe time out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_max\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_probability\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecond_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bmm(): argument 'mat2' (position 1) must be Variable, not torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "M = 8 # Don't change this (as long as we're using the add-task)\n",
    "num_examples = 5\n",
    "\n",
    "dataset = AddTaskDataset(M, num_examples)\n",
    "data_loader = data.DataLoader(dataset, batch_size = 1) # Don't change this batch size.  You have been warned.\n",
    "\n",
    "def anc_validation_criterion(output, label):\n",
    "    target_memory = label[1]\n",
    "    target_mask = label[2]\n",
    "    \n",
    "    output = output.data * target_mask\n",
    "    target_memory = target_memory * target_mask\n",
    "    _, target_indices = torch.max(target_memory, 2)\n",
    "    _, output_indices = torch.max(output, 2)\n",
    "    return torch.equal(output_indices, target_indices)\n",
    "\n",
    "# Initialize our controller\n",
    "controller = Controller(first_arg = first_arg, \n",
    "                        second_arg = second_arg, \n",
    "                        output = target, \n",
    "                        instruction = instruction, \n",
    "                        initial_registers = init_registers, \n",
    "                        stop_threshold = .9, \n",
    "                        blur = 1, \n",
    "                        correctness_weight = .3, \n",
    "                        halting_weight = .3, \n",
    "                        confidence_weight = .3, \n",
    "                        efficiency_weight = .1, \n",
    "                        t_max = 10) \n",
    "\n",
    "# Learning rate is a tunable hyperparameter\n",
    "# The paper didn't mention which one they used\n",
    "optimizer = optim.Adam(controller.parameters(), lr = 0.01)\n",
    "\n",
    "plot_every = 1\n",
    "\n",
    "# TODO: Choose better initial values by checking what the paper did.\n",
    "# TODO: Think about whether there's a better validation criterion than just the loss.\n",
    "best_model, train_plot_losses, validation_plot_losses = training.train_model_anc(\n",
    "    controller, \n",
    "    data_loader,  \n",
    "    optimizer, \n",
    "    num_epochs=10, \n",
    "    print_every=1, \n",
    "    plot_every=plot_every, \n",
    "    deep_copy_desired=False, \n",
    "    validation_criterion=anc_validation_criterion, \n",
    "    forward_train=True, \n",
    "    batch_size=10)\n",
    "    \n",
    "    #kangaroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113d4e198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x * plot_every for x in range(len(train_plot_losses))], train_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114312358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x * plot_every for x in range(len(validation_plot_losses))], validation_plot_losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
