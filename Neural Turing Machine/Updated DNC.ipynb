{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of inspiration from https://github.com/loudinthecloud/pytorch-ntm. Hyperparameters were chosen based\n",
    "# upon his experiments.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seed(seed=None):\n",
    "    \"\"\"Seed the RNGs for predicatability/reproduction purposes.\"\"\"\n",
    "    if seed is None:\n",
    "        seed = int(get_ms() // 1000)\n",
    "\n",
    "    print(\"Using seed=%d\", seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC_Memory(nn.Module):\n",
    "    def __init__(self, address_count, address_dimension):\n",
    "        super(DNC_Memory, self).__init__()\n",
    "        self.initial_memory = nn.Parameter(torch.zeros(1, address_count, address_dimension))\n",
    "        reset_parameters()\n",
    "        initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        _, N, M = self.initial_memory.size()\n",
    "        stdev = 1 / np.sqrt(N + M)\n",
    "        nn.init.uniform(self.initial_memory, -stdev, stdev)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.memory = self.initial_memory.repeat(self.batch_size, 1, 1)\n",
    "    \n",
    "    def content_address_memory(self, key_vec, prev_address_vec, β, γ, sharp_on):\n",
    "        result = F.cosine_similarity(key_vec.unsqueeze(1).expand_as(self.memory), \n",
    "                                     self.memory, dim = 2)\n",
    "        result = β * result\n",
    "        result = result.exp() \n",
    "        result = result / result.sum()\n",
    "        \n",
    "        if sharp_on:\n",
    "            result = result ** γ\n",
    "            result = result / result.sum()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def read_memory(self, address_vec):\n",
    "        return torch.bmm(self.memory.transpose(1,2), address_vec.unsqueeze(2)).squeeze()\n",
    "    \n",
    "    def update_memory(self, address_vec, erase_vec, add_vec):\n",
    "        self.memory = self.memory * (1 - torch.bmm(address_vec.unsqueeze(2), erase_vec.unsqueeze(1)))\n",
    "        self.memory += torch.bmm(address_vec.unsqueeze(2), add_vec.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC_Usage(nn.Module):\n",
    "    def __init__(self, address_count, batch_size):\n",
    "        super(DNC_Usage, self).__init__()\n",
    "        self.initial_usage = Variable(torch.zeros(1, address_count))\n",
    "        self.batch_size = batch_size\n",
    "        self.initialize_state()\n",
    "        \n",
    "    def initialize_state(self):\n",
    "        self.usage = self.initial_usage.repeat(self.batch_size, 1)\n",
    "    \n",
    "    def read_update_usage(self, address_vec, rfree_weights):\n",
    "        self.usage *= 1 - rfree_weights * address_vec\n",
    "    \n",
    "    def write_update_usage(self, address_vec):\n",
    "        self.usage += (1 - self.usage) * address_vec\n",
    "    \n",
    "    def allocation_weights(self):\n",
    "        sorted_usage, indices_usage = torch.sort(self.usage)\n",
    "        prod_sorted_usage = torch.cumprod(torch.cat((Variable(torch.ones(self.batch_size, 1)), \n",
    "                                                     sorted_usage), dim=1), dim=1)[:, :-1]\n",
    "        sorted_allocation = (1 - sorted_usage) * prod_sorted_usage\n",
    "        return sorted_allocation.gather(1, indices_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_cols(mat, lengths):\n",
    "    \"\"\"Split a 2D matrix to variable length columns.\"\"\"\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results += [mat[:, s:e]]\n",
    "    return results\n",
    "\n",
    "class DNC_Head(nn.Module):\n",
    "    def __init__(self, address_count, address_dimension, \n",
    "                 controller_output_size):\n",
    "        super(DNC_Head, self).__init__()\n",
    "        \n",
    "        self.controller_output_size = controller_output_size\n",
    "        self.N = address_count\n",
    "        self.M = address_dimension\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC_Read_Head(DNC_Head):\n",
    "    def __init__(self, controller_output_size, batch_size, num_write_heads):\n",
    "        super(DNC_Read_Head, self).__init__(controller_output_size)\n",
    "        # key_vec, β, γ, read_mode, rfree_gate\n",
    "        #self.M is the number of rows\n",
    "        self.num_write_heads = num_write_heads\n",
    "        self.read_parameters_lengths = [self.M, 1, 1, 2 * self.num_write_heads + 1, self.N]\n",
    "        self.fc_read_parameters = nn.Linear(controller_output_size, sum(self.read_parameters_lengths))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_read_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_read_parameters.bias, std=0.01)\n",
    "        \n",
    "        self.initial_address_vector = Variable(1, torch.zeros(self.N))\n",
    "        self.initial_read = nn.Parameter(torch.randn(1, self.M) * 0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.prev_address_vec = self.initial_address_vec.repeat(self.batch_size, 1)\n",
    "        self.prev_read = self.initial_read.repeat(self.batch_size, 1)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "    \n",
    "    def forward(self, x, memory, usage, linked_matrices):\n",
    "        read_parameters = self.fc_read_parameters(x)\n",
    "        \n",
    "        key_vec, β, γ, read_modes, rfree_gate = _split_cols(read_parameters, self.read_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "        rfree_gate = F.sigmoid(rfree_gate)\n",
    "        read_modes = F.softmax(read_modes)\n",
    "        content_address_vec = memory.content_address_memory(key_vec, self.prev_address_vec, β, γ, False)\n",
    "        \n",
    "        forward, backward = []\n",
    "        \n",
    "        address_vec = content_address_vec * read_modes[0]\n",
    "        \n",
    "        for i, linked_matrix in enumerate(linked_matrices):\n",
    "            address_vec += read_modes[i+1] + torch.bmm(linked_matrix, self.prev_address_vec.unsquueze(2)).squeeze()\n",
    "            address_vec += read_modes[i+self.num_write_heads+1] * torch.bmm(linked_matrix.transpose(1,2), \n",
    "                                                                            self.prev_address_vec.unsqueeze(2)).squeeze()\n",
    "        \n",
    "        new_read = memory.read_memory(address_vec)\n",
    "        self.prev_read = new_read\n",
    "        return new_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC_Write_Head(DNC_Head):\n",
    "    def __init__(self, controller_output_size, batch_size):\n",
    "        super(DNC_Write_Head, self).__init__(controller_output_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.write_parameters_lengths = [self.M, 1, 1, 1, self.M, self.M]\n",
    "        self.fc_write_parameters = nn.Linear(controller_output_size, sum(self.write_parameters_lengths))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "        \n",
    "        #initialize the linked matrix\n",
    "        self.linked_matrix = torch.zeros(batch_size, self.M, self.M)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_write_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_write_parameters.bias, std=0.01)\n",
    "        \n",
    "        self.initial_address_vec = nn.Parameter(torch.zeros(self.N))\n",
    "    \n",
    "    def initialize_state(self):       \n",
    "        self.prev_address_vec = self.initial_address_vec\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return False\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        write_parameters = self.fc_write_parameters(x)\n",
    "                                       \n",
    "        key_vec, β, g, γ, erase_vec, add_vec = _split_cols(write_parameters, self.write_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "        erase_vec = F.sigmoid(erase_vec)\n",
    "                                               \n",
    "        address_vec = memory.address_memory(key_vec, self.prev_address_vec, β, g, γ)\n",
    "        memory.upate_memory(address_vec, erase_vec, add_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC(nn.Module):\n",
    "    def __init__(self, batch_size, controller, controller_output_size, \n",
    "                 output_size, address_count, address_dimension, heads):\n",
    "        super(DNC, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize controller\n",
    "        self.controller = controller\n",
    "        \n",
    "        # Create output gate. No activation function is used with it because\n",
    "        # I used BCEWithLogitsLoss which deals with the sigmoid in a more\n",
    "        # numerically stable manner.\n",
    "        self.outputGate = nn.Linear(controller_output_size, output_size)\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = DNC_Memory(address_count, address_dimension)\n",
    "\n",
    "        # Construct the heads.\n",
    "        self.heads = nn.ModuleList()\n",
    "        \n",
    "        # Initialize usage vector, might not need batch size\n",
    "        self.usage = torch.zeros(batch_size, address_count)\n",
    "        num_writes = heads.count(1)\n",
    "        \n",
    "        for head_id in heads:\n",
    "            if head_id == 0:\n",
    "                self.heads.append(DNC_Read_Head(self, controller_output_size, batch_size, num_writes))\n",
    "            else:\n",
    "                self.heads.append(DNC_Write_Head(self, controller_output_size))\n",
    "        \n",
    "        self.initialize_state()\n",
    "        \n",
    "    def initialize_state(self):\n",
    "        self.prev_reads = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            head.initialize_state()\n",
    "            \n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head.prev_read)\n",
    "        \n",
    "        self.memory.initialize_state()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.outputGate.weight)\n",
    "        nn.init.normal(self.outputGate.bias, std=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        \n",
    "        for current_observation in x.transpose(0,1):\n",
    "            self.prev_reads.append(current_observation)\n",
    "            controller_input = torch.cat(self.prev_reads, 1)\n",
    "            controller_output = self.controller(controller_input).squeeze()\n",
    "\n",
    "            self.prev_reads = []\n",
    "\n",
    "            for head in self.heads:                \n",
    "                if head.is_read_head():\n",
    "                    self.prev_reads.append(head(controller_output, self.memory))\n",
    "                else:\n",
    "                    head(controller_output, self.memory)\n",
    "                    \n",
    "            current_output = self.outputGate(controller_output)\n",
    "            outputs.append(current_output)\n",
    "        \n",
    "        return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Have the dataset also return labels.\n",
    "\n",
    "class CopyTaskDataset(data.Dataset):\n",
    "    def __init__(self, num_batches, batch_size, lower, upper, seq_size):\n",
    "        self.inputs_list = []\n",
    "        self.label_list = []\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            \n",
    "            self.input_list.append()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batch(self, batch_size, lower, upper, seq_size):\n",
    "        seq_length = random.randint(lower, upper)\n",
    "        seq = torch.from_numpy(\n",
    "                np.random.binomial(1, 0.5, (seq_length, batch_size, seq_size)))\n",
    "        end_marker = torch.zeros(seq_length, batch_size, 1)\n",
    "        seq = torch.cat((seq.float(), end_marker), 2)\n",
    "        delimiter_column = torch.zeros(1, batch_size, seq_size+1)\n",
    "        delimiter_column[0, :, seq_size] = 1\n",
    "        seq = torch.cat((seq, delimiter_column), 0)\n",
    "        output_time = torch.zeros(seq_length, batch_size, seq_size+1)\n",
    "        seq = torch.cat((seq, output_time), 0)\n",
    "        return seq\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)*self.batch_size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.input_list[i//self.batch_size][:, i % self.batch_size, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncapsulatedLSTM(nn.Module):\n",
    "    def __init__(self, batch_size, all_hiddens, *args, **kwargs):\n",
    "        super(EncapsulatedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(*args, **kwargs)\n",
    "        self.all_hiddens = all_hiddens\n",
    "        self.batch_size = batch_size\n",
    "                \n",
    "        self.num_inputs = args[0]\n",
    "        self.hidden_size = args[1]\n",
    "        self.num_layers = args[2]\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "          \n",
    "    def initialize_state(self):\n",
    "        self.state_tuple = (self.initial_hidden_state.repeat(1, self.batch_size, 1), \n",
    "                            self.initial_cell_state.repeat(1, self.batch_size, 1))\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.initial_hidden_state = nn.Parameter(torch.randn(self.num_layers, 1, self.hidden_size) * 0.05)\n",
    "        self.initial_cell_state = nn.Parameter(torch.randn(self.num_layers, 1, self.hidden_size) * 0.05)\n",
    "        \n",
    "        for p in self.lstm.parameters():\n",
    "            if p.dim() == 1:\n",
    "                nn.init.constant(p, 0)\n",
    "            else:\n",
    "                stdev = 5 / (np.sqrt(self.num_inputs +  self.hidden_size))\n",
    "                nn.init.uniform(p, -stdev, stdev)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output, self.state_tuple = self.lstm(input.unsqueeze(0), self.state_tuple)\n",
    "        \n",
    "        if self.all_hiddens:\n",
    "            return self.state_tuple[0]\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the loss function. Just use BCE logits form and cut out the right part of the outputs.\n",
    "def copy_task_loss(output, label):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generalize the train_model function a bit to accomodate gradient clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hidden_size = 100\n",
    "num_layers = 3\n",
    "seq_size = 8\n",
    "address_size = 20\n",
    "controller = EncapsulatedLSTM(batch_size, False, # all hiddens\n",
    "                              seq_size + address_size + 1, hidden_size, \n",
    "                              num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cannot assign parameters before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0faab8f7d5e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m dnc = DNC(batch_size, controller, controller_output_size, \n\u001b[0;32m----> 5\u001b[0;31m           seq_size, address_count, address_size, [0, 1])   \n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-64a8d21ca798>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, controller, controller_output_size, output_size, address_count, address_dimension, heads)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Initialize memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDNC_Memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress_dimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Construct the heads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e0e662195137>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, address_count, address_dimension)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDNC_Memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress_dimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress_dimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0minitialize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 raise AttributeError(\n\u001b[0;32m--> 309\u001b[0;31m                     \"cannot assign parameters before Module.__init__() call\")\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: cannot assign parameters before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "address_count = 128\n",
    "controller_output_size = hidden_size\n",
    "\n",
    "dnc = DNC(batch_size, controller, controller_output_size, \n",
    "          seq_size, address_count, address_size, [0, 1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_seq_length = 3\n",
    "upper_seq_length = 10\n",
    "num_batches = 600\n",
    "\n",
    "dataset = CopyTaskDataset(num_batches, batch_size, lower_seq_length, upper_seq_length, seq_size)\n",
    "data_loader = data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(dnc.parameters(), momentum=0.9,\n",
    "                          alpha=0.95, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAll(num_batches, batch_size=64, hidden_size=100, \n",
    "             num_layers=3, lower_seq_length=3, upper_seq_length=10, seq_size=8,\n",
    "             address_count=128, address_size=20):\n",
    "    # controller, controller_output_size, output_size, \n",
    "    # address_count, address_dimension, heads\n",
    "    controller = EncapsulatedLSTM(batch_size, False, # all hiddens\n",
    "                                  seq_size + address_size + 1, hidden_size, \n",
    "                                  num_layers)\n",
    "    controller_output_size = hidden_size\n",
    "    \n",
    "    dnc = DNC(batch_size, controller, controller_output_size, \n",
    "              seq_size, address_count, address_size, [0, 1])   \n",
    "    dataset = CopyTaskDataset(num_batches, batch_size, lower_seq_length, upper_seq_length, seq_size)\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        print(dnc(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.5173  0.5048  0.4749  ...   0.5030  0.5214  0.5072\n",
      "  0.5027  0.5128  0.4643  ...   0.4834  0.5122  0.5119\n",
      "  0.4870  0.5245  0.4737  ...   0.4905  0.4966  0.5319\n",
      "           ...             ⋱             ...          \n",
      "  0.4862  0.5271  0.4792  ...   0.4905  0.5020  0.5173\n",
      "  0.4963  0.5169  0.4738  ...   0.5036  0.4933  0.5348\n",
      "  0.4966  0.5282  0.4860  ...   0.4939  0.5077  0.5253\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.4688  0.5047  0.4738  ...   0.5163  0.5263  0.5115\n",
      "  0.4828  0.5057  0.4833  ...   0.4944  0.5436  0.5089\n",
      "  0.4584  0.5407  0.5048  ...   0.5077  0.5040  0.5155\n",
      "           ...             ⋱             ...          \n",
      "  0.4573  0.5416  0.5043  ...   0.5027  0.5165  0.5056\n",
      "  0.4559  0.5399  0.5036  ...   0.5106  0.4725  0.5086\n",
      "  0.4671  0.5350  0.4996  ...   0.5046  0.5082  0.5121\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.4073  0.5189  0.5012  ...   0.5110  0.5611  0.4872\n",
      "  0.4238  0.4922  0.4849  ...   0.4829  0.5555  0.5075\n",
      "  0.4273  0.5215  0.5224  ...   0.4918  0.5050  0.5312\n",
      "           ...             ⋱             ...          \n",
      "  0.4074  0.5517  0.5221  ...   0.5011  0.5064  0.5047\n",
      "  0.4318  0.5293  0.5121  ...   0.5106  0.4731  0.5162\n",
      "  0.4153  0.5450  0.5082  ...   0.5075  0.5017  0.5018\n",
      "...\n",
      "\n",
      "(4 ,.,.) = \n",
      "  0.3936  0.5275  0.5391  ...   0.4703  0.5861  0.5079\n",
      "  0.4290  0.4931  0.5039  ...   0.4820  0.5605  0.5168\n",
      "  0.3946  0.5498  0.5071  ...   0.4700  0.4991  0.5266\n",
      "           ...             ⋱             ...          \n",
      "  0.4151  0.5739  0.5971  ...   0.4770  0.5019  0.4750\n",
      "  0.4106  0.5605  0.4680  ...   0.5287  0.4717  0.6082\n",
      "  0.4150  0.5598  0.5583  ...   0.5402  0.5113  0.5174\n",
      "\n",
      "(5 ,.,.) = \n",
      "  0.4167  0.5266  0.5009  ...   0.4679  0.5928  0.5501\n",
      "  0.4560  0.4826  0.5001  ...   0.4892  0.5572  0.5047\n",
      "  0.4013  0.5689  0.4751  ...   0.4811  0.5036  0.5367\n",
      "           ...             ⋱             ...          \n",
      "  0.4582  0.5905  0.5547  ...   0.4737  0.5402  0.5047\n",
      "  0.4538  0.5698  0.4310  ...   0.5670  0.4929  0.6337\n",
      "  0.4515  0.5436  0.5661  ...   0.5534  0.5228  0.5037\n",
      "\n",
      "(6 ,.,.) = \n",
      "  0.4373  0.5296  0.4875  ...   0.4723  0.6080  0.5801\n",
      "  0.4730  0.4615  0.4977  ...   0.4882  0.5421  0.4628\n",
      "  0.4171  0.5878  0.4500  ...   0.5078  0.5078  0.5447\n",
      "           ...             ⋱             ...          \n",
      "  0.4869  0.6153  0.5207  ...   0.5117  0.5774  0.5157\n",
      "  0.4992  0.5845  0.4394  ...   0.6060  0.5112  0.6165\n",
      "  0.4787  0.5297  0.5752  ...   0.5570  0.5215  0.4773\n",
      "[torch.FloatTensor of size 7x64x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainAll(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CopyTaskDataset(1, 16, 3, 8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "   0   1   0   0\n",
      "   1   1   1   0\n",
      "   1   0   1   0\n",
      "   1   0   1   0\n",
      "   0   0   0   0\n",
      "   1   1   0   0\n",
      "   1   1   1   0\n",
      "   0   1   0   0\n",
      "   1   1   0   0\n",
      "   1   1   1   0\n",
      "   0   0   0   0\n",
      "   0   1   1   0\n",
      "   0   1   0   0\n",
      "   1   1   1   0\n",
      "   0   0   0   0\n",
      "   0   1   1   0\n",
      "\n",
      "(1 ,.,.) = \n",
      "   1   0   0   0\n",
      "   0   1   0   0\n",
      "   1   0   0   0\n",
      "   1   1   0   0\n",
      "   0   0   0   0\n",
      "   1   1   0   0\n",
      "   1   0   0   0\n",
      "   0   0   1   0\n",
      "   1   1   1   0\n",
      "   1   1   0   0\n",
      "   0   0   0   0\n",
      "   0   1   1   0\n",
      "   0   1   0   0\n",
      "   1   0   1   0\n",
      "   1   0   1   0\n",
      "   0   0   0   0\n",
      "\n",
      "(2 ,.,.) = \n",
      "   1   0   0   0\n",
      "   1   0   1   0\n",
      "   1   0   1   0\n",
      "   1   1   1   0\n",
      "   0   1   0   0\n",
      "   0   1   0   0\n",
      "   0   1   1   0\n",
      "   1   1   0   0\n",
      "   1   0   1   0\n",
      "   1   0   0   0\n",
      "   1   1   1   0\n",
      "   0   1   1   0\n",
      "   1   1   1   0\n",
      "   1   1   1   0\n",
      "   1   1   1   0\n",
      "   1   0   1   0\n",
      "\n",
      "(3 ,.,.) = \n",
      "   1   1   0   0\n",
      "   1   0   1   0\n",
      "   1   1   1   0\n",
      "   0   0   0   0\n",
      "   1   1   1   0\n",
      "   0   0   0   0\n",
      "   0   1   0   0\n",
      "   0   1   0   0\n",
      "   0   1   1   0\n",
      "   0   1   0   0\n",
      "   1   1   0   0\n",
      "   0   0   1   0\n",
      "   0   0   0   0\n",
      "   0   0   1   0\n",
      "   0   0   1   0\n",
      "   1   1   0   0\n",
      "\n",
      "(4 ,.,.) = \n",
      "   1   1   0   0\n",
      "   1   1   0   0\n",
      "   0   1   1   0\n",
      "   0   1   0   0\n",
      "   1   0   0   0\n",
      "   1   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   1   0\n",
      "   1   1   1   0\n",
      "   1   1   0   0\n",
      "   1   0   1   0\n",
      "   0   0   1   0\n",
      "   0   0   0   0\n",
      "   1   0   1   0\n",
      "   0   0   0   0\n",
      "   1   0   0   0\n",
      "\n",
      "(5 ,.,.) = \n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "   0   0   0   1\n",
      "\n",
      "(6 ,.,.) = \n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "\n",
      "(7 ,.,.) = \n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "\n",
      "(8 ,.,.) = \n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "\n",
      "(9 ,.,.) = \n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "\n",
      "(10,.,.) = \n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "   0   0   0   0\n",
      "[torch.FloatTensor of size 11x16x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset.input_list[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
