{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of inspiration from https://github.com/loudinthecloud/pytorch-ntm. Hyperparameters were chosen based\n",
    "# upon his experiments.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data as d\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_cols(mat, lengths):\n",
    "    \"\"\"Split a 2D matrix to variable length columns.\"\"\"\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results += [mat[:, s:e]]\n",
    "    return results\n",
    "\n",
    "class NTM_Head(nn.Module):\n",
    "    def __init__(self, memory, controller_output_size):\n",
    "        super(NTM_Head, self).__init__()\n",
    "        \n",
    "        self.memory = memory\n",
    "        self.controller_output_size = controller_output_size\n",
    "        _, self.N, self.M = memory.size()\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Read_Head(NTM_Head):\n",
    "    def __init__(self, memory, controller_output_size, batch_size):\n",
    "        super(NTM_Read_Head, self).__init__(memory, controller_output_size)\n",
    "        #TODO: Get rid of g in read and write mode, to be replaced with usage vec\n",
    "        # key_vec, β, g, γ, read_mode, rfree_gate\n",
    "        #self.M is the number of rows\n",
    "        self.read_parameters_lengths = [self.M, 1, 1, 1, 3, self.M]\n",
    "        self.fc_read_parameters = nn.Linear(controller_output_size, sum(self.read_parameters_lengths))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.initial_address_vector = nn.Parameter(torch.zeros(self.N))\n",
    "        self.initial_read = nn.Parameter(torch.zeros(1, self.M))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_read_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_read_parameters.bias, std=0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.prev_address_vector = self.initial_address_vector\n",
    "        self.prev_read = self.initial_read.repeat(self.batch_size, 1)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        read_parameters = self.fc_read_parameters(x)\n",
    "        # key_vec, β, g, γ, read_mode, rfree_gate\n",
    "        key_vec, β, g, γ, read_mode, rfree_gate = _split_cols(read_parameters, self.read_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        read_mode = F.softmax(read_mode)\n",
    "        rfree_gate = F.sigmoid(rfree_gate)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "        \n",
    "        print('H')\n",
    "        print(key_vec)\n",
    "        print('I')\n",
    "        print(β)\n",
    "        print('J')\n",
    "        print(g)\n",
    "        print('K1')\n",
    "        print(read_mode)\n",
    "        print('K2')\n",
    "        print(rfree_gate)\n",
    "        print('L')\n",
    "        print(γ)\n",
    "                                               \n",
    "        address_vec = self.address_memory(key_vec, β, g, γ, read_mode, rfree_gate)\n",
    "        \n",
    "        print('M')\n",
    "        print(address_vec)\n",
    "        new_read = torch.bmm(self.memory.transpose(1,2), address_vec.unsqueeze(2)).squeeze()\n",
    "        \n",
    "        print('N')\n",
    "        print(new_read)\n",
    "        self.prev_read = new_read\n",
    "        return new_read\n",
    "    \n",
    "    def address_memory(self, key_vec, β, g, γ, read_mode, rfree_gate):\n",
    "        result = F.cosine_similarity(key_vec.unsqueeze(1).expand_as(self.memory), self.memory, dim = 2)\n",
    "        \n",
    "        print('R')\n",
    "        print(result)\n",
    "        \n",
    "        result = β * result\n",
    "        \n",
    "        print('S')\n",
    "        print(result)\n",
    "        \n",
    "        result = result.exp() / result.sum()\n",
    "        \n",
    "        print('T')\n",
    "        print(result)\n",
    "        \n",
    "#        result = g * result + (1 - g) * self.prev_address_vector\n",
    "        print('U')\n",
    "        print(result)\n",
    "        \n",
    "\n",
    "        print('V')\n",
    "        print(result)\n",
    "        \n",
    "        result = result ** γ\n",
    "        print('W')\n",
    "        print(result)\n",
    "        \n",
    "        result = result / result.sum()\n",
    "        print('X')\n",
    "        print(result)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Write_Head(NTM_Head):\n",
    "    def __init__(self, memory, controller_output_size):\n",
    "        super(NTM_Write_Head, self).__init__(memory, controller_output_size)\n",
    "        #write_parameters are, in order: key_vec, β, g, write_gate, γ, erase_vec, add_vec\n",
    "        self.write_parameters_lengths = [self.M, 1, 1, 1, 1,  self.M, self.M]\n",
    "        self.fc_write_parameters = nn.Linear(controller_output_size, sum(self.write_parameters_lengths))\n",
    "        self.initial_address_vector = nn.Parameter(torch.zeros(self.N))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_write_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_write_parameters.bias, std=0.01)\n",
    "    \n",
    "    def initialize_state(self):       \n",
    "        self.prev_address_vector = Variable(self.initial_address_vector.data)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        write_parameters = self.fc_write_parameters(x)\n",
    "                                       \n",
    "        key_vec, β, g, write_gate, γ, erase_vec, add_vec = _split_cols(write_parameters, self.write_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        write_gate = F.softmax(write_gate)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "        erase_vec = F.sigmoid(erase_vec)\n",
    "        \n",
    "        print('A')\n",
    "        print(key_vec)\n",
    "        print('B')\n",
    "        print(β)\n",
    "        print('C')\n",
    "        print(g)\n",
    "        print('D')\n",
    "        print(s)\n",
    "        print('E')\n",
    "        print(γ)\n",
    "        print('F')\n",
    "        print(erase_vec)\n",
    "        print(key_vec)\n",
    "        print(β)\n",
    "        print(g)\n",
    "        print(write_gate)\n",
    "        print(γ)\n",
    "        print('G')\n",
    "        print(add_vec)\n",
    "                                               \n",
    "        address_vec = self.address_memory(key_vec, β, g, write_gate, γ)\n",
    "        self.memory = self.memory * (1 - torch.bmm(address_vec.unsqueeze(2), erase_vec.unsqueeze(1)))\n",
    "        self.memory += torch.bmm(address_vec.unsqueeze(2), add_vec.unsqueeze(1))\n",
    "        \n",
    "        print('O')\n",
    "        print(self.memory)\n",
    "        print('P')\n",
    "        print(address_vec)\n",
    "        \n",
    "    def address_memory(self, key_vec, β, g, write_gate, γ):\n",
    "        result = F.cosine_similarity(key_vec.unsqueeze(1).expand_as(self.memory), self.memory, dim = 2)\n",
    "        \n",
    "        print('R')\n",
    "        print(result)\n",
    "        \n",
    "        result = β * result\n",
    "        \n",
    "        print('S')\n",
    "        print(result)\n",
    "        \n",
    "        result = result.exp()\n",
    "        result = result / result.sum()\n",
    "        \n",
    "        print('T')\n",
    "        print(result)\n",
    "        \n",
    "        result = g * result + (1 - g) * self.prev_address_vector\n",
    "        print('U')\n",
    "        print(result)\n",
    "        \n",
    "        result = torch.cat((result[:, 1:], result[:, :1]), 1) * s[:, 0:1] + result * s[:, 1:2] + \\\n",
    "                 torch.cat((result[:, -1:], result[:, :-1]), 1) * s[:, 2:3]\n",
    "        print('V')\n",
    "        print(result)\n",
    "        \n",
    "        result = result ** γ\n",
    "        print('W')\n",
    "        print(result)\n",
    "        \n",
    "        result = result / result.sum()\n",
    "        print('X')\n",
    "        print(result)\n",
    "        ######## WHAT I CHANGED ############\n",
    "        result = result * write_gate\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, batch_size, controller, controller_output_size, \n",
    "                 output_size, address_count, address_dimension, heads):\n",
    "        super(NTM, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.initial_memory = nn.Parameter(torch.zeros(1, address_count, address_dimension))\n",
    "        self.memory = self.initial_memory.repeat(batch_size, 1, 1)\n",
    "        \n",
    "        # Construct the heads.\n",
    "        self.heads = nn.ModuleList()\n",
    "        \n",
    "        for head_id in heads:\n",
    "            if head_id == 0:\n",
    "                self.heads.append(NTM_Read_Head(self.memory, controller_output_size, batch_size))\n",
    "            else:\n",
    "                self.heads.append(NTM_Write_Head(self.memory, controller_output_size))\n",
    "        \n",
    "        # Initialize controller\n",
    "        self.controller = controller\n",
    "        self.outputGate = nn.Linear(controller_output_size, output_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.initialize_state()\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.prev_reads = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            head.initialize_state()\n",
    "            \n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head.prev_read)\n",
    "        \n",
    "        self.memory = self.initial_memory.repeat(self.batch_size, 1, 1)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        _, N, M = self.initial_memory.size()\n",
    "        stdev = 1 / np.sqrt(N + M)\n",
    "        nn.init.uniform(self.initial_memory, -stdev, stdev)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.prev_reads.append(x)\n",
    "        controller_output = self.controller(torch.cat(self.prev_reads, 1)).squeeze()\n",
    "        print('Q')\n",
    "        print(controller_output)\n",
    "                \n",
    "        self.prev_reads = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head(controller_output))\n",
    "            else:\n",
    "                head(controller_output)\n",
    "        \n",
    "        return self.softmax(self.outputGate(controller_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyTaskDataset(d.Dataset):\n",
    "    def __init__(self, num_batches, batch_size, lower, upper, seq_size):\n",
    "        self.input_list = []\n",
    "        for _ in range(num_batches):\n",
    "            self.input_list.append(self.generate_batch(batch_size, lower, upper, seq_size))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batch(self, batch_size, lower, upper, seq_size):\n",
    "        seq_length = random.randint(lower, upper)\n",
    "        batch = torch.from_numpy(\n",
    "            np.random.binomial(1, 0.5, (seq_length, batch_size, seq_size)))\n",
    "        end_marker = torch.zeros(seq_length - 1, batch_size, 1)\n",
    "        end_row = torch.ones(1, batch_size, 1)\n",
    "        end_marker = torch.cat((end_marker, end_row), 0)\n",
    "        batch = torch.cat((batch.float(), end_marker), 2)\n",
    "        return batch \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)*self.batch_size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.input_list[i//self.batch_size][:, i % self.batch_size, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncapsulatedLSTM(nn.Module):\n",
    "    def __init__(self, batch_size, all_hiddens, *args, **kwargs):\n",
    "        super(EncapsulatedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(*args, **kwargs)\n",
    "        self.all_hiddens = all_hiddens\n",
    "        self.batch_size = batch_size\n",
    "                \n",
    "        hidden_size = args[1]\n",
    "        num_layers = args[2]\n",
    "        \n",
    "        self.initial_hidden_state = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n",
    "        self.initial_cell_state = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n",
    "        self.initialize_state()\n",
    "          \n",
    "    def initialize_state(self):\n",
    "        self.state_tuple = (self.initial_hidden_state.repeat(1, self.batch_size, 1), \n",
    "                            self.initial_cell_state.repeat(1, self.batch_size, 1))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output, self.state_tuple = self.lstm(input.unsqueeze(0), self.state_tuple)\n",
    "        \n",
    "        if self.all_hiddens:\n",
    "            return self.state_tuple[0]\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAll(num_batches, batch_size=64, hidden_size=100, \n",
    "             num_layers=3, lower_seq_length=3, upper_seq_length=10, seq_size=8,\n",
    "             address_count=128, address_size=20):\n",
    "    # controller, controller_output_size, output_size, \n",
    "    # address_count, address_dimension, heads\n",
    "    controller = EncapsulatedLSTM(batch_size, False, # all hiddens\n",
    "                                  seq_size + address_size + 1, hidden_size, \n",
    "                                  num_layers)\n",
    "    controller_output_size = hidden_size\n",
    "    \n",
    "    ntm = NTM(batch_size, controller, controller_output_size, \n",
    "              seq_size, address_count, address_size, [0, 1])   \n",
    "    dataset = CopyTaskDataset(num_batches, batch_size, lower_seq_length, upper_seq_length, seq_size)\n",
    "    \n",
    "    data_loader = d.DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        batch = batch.squeeze()\n",
    "        sequence_length = batch.size()[1]\n",
    "\n",
    "        # Pass in one element of the sequence per time step\n",
    "        for time_step in range(sequence_length):\n",
    "            ntm(batch[:, time_step])\n",
    "            break\n",
    "                \n",
    "#             # Now, don't pass in any elements.\n",
    "#             for time_step in range(len(batch)):\n",
    "#                 ntm(None, batch[time_step][:,:-1])\n",
    "#     print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      "Variable containing:\n",
      " 5.0101e-03  4.3148e-02 -3.0836e-05  ...   3.3804e-01 -1.5142e-01  1.5596e-01\n",
      " 5.0960e-03  4.2850e-02  3.4896e-06  ...   3.3847e-01 -1.5143e-01  1.5555e-01\n",
      " 5.1468e-03  4.2708e-02  1.0764e-04  ...   3.3880e-01 -1.5143e-01  1.5553e-01\n",
      "                ...                   ⋱                   ...                \n",
      " 5.2387e-03  4.2841e-02  1.4981e-04  ...   3.3864e-01 -1.5182e-01  1.5579e-01\n",
      " 5.2937e-03  4.2686e-02 -3.2832e-05  ...   3.3874e-01 -1.5121e-01  1.5601e-01\n",
      " 5.1746e-03  4.2826e-02 -4.3817e-05  ...   3.3818e-01 -1.5140e-01  1.5634e-01\n",
      "[torch.FloatTensor of size 64x100]\n",
      "\n",
      "H\n",
      "Variable containing:\n",
      "-0.2219 -0.1801  1.0618  ...   0.0314  0.2972  0.3293\n",
      "-0.2217 -0.1812  1.0620  ...   0.0311  0.2957  0.3295\n",
      "-0.2212 -0.1812  1.0619  ...   0.0313  0.2959  0.3291\n",
      "          ...             ⋱             ...          \n",
      "-0.2213 -0.1808  1.0618  ...   0.0312  0.2973  0.3289\n",
      "-0.2215 -0.1807  1.0610  ...   0.0303  0.2945  0.3287\n",
      "-0.2217 -0.1802  1.0605  ...   0.0308  0.2963  0.3289\n",
      "[torch.FloatTensor of size 64x20]\n",
      "\n",
      "I\n",
      "Variable containing:\n",
      " 0.5199\n",
      " 0.5197\n",
      " 0.5196\n",
      " 0.5201\n",
      " 0.5195\n",
      " 0.5203\n",
      " 0.5195\n",
      " 0.5201\n",
      " 0.5195\n",
      " 0.5201\n",
      " 0.5191\n",
      " 0.5199\n",
      " 0.5198\n",
      " 0.5193\n",
      " 0.5197\n",
      " 0.5198\n",
      " 0.5194\n",
      " 0.5200\n",
      " 0.5197\n",
      " 0.5195\n",
      " 0.5198\n",
      " 0.5195\n",
      " 0.5198\n",
      " 0.5194\n",
      " 0.5192\n",
      " 0.5198\n",
      " 0.5201\n",
      " 0.5199\n",
      " 0.5201\n",
      " 0.5195\n",
      " 0.5198\n",
      " 0.5198\n",
      " 0.5198\n",
      " 0.5197\n",
      " 0.5199\n",
      " 0.5198\n",
      " 0.5201\n",
      " 0.5201\n",
      " 0.5201\n",
      " 0.5195\n",
      " 0.5197\n",
      " 0.5196\n",
      " 0.5194\n",
      " 0.5195\n",
      " 0.5199\n",
      " 0.5196\n",
      " 0.5192\n",
      " 0.5197\n",
      " 0.5198\n",
      " 0.5198\n",
      " 0.5195\n",
      " 0.5200\n",
      " 0.5195\n",
      " 0.5202\n",
      " 0.5198\n",
      " 0.5194\n",
      " 0.5195\n",
      " 0.5196\n",
      " 0.5195\n",
      " 0.5196\n",
      " 0.5196\n",
      " 0.5200\n",
      " 0.5196\n",
      " 0.5199\n",
      "[torch.FloatTensor of size 64x1]\n",
      "\n",
      "J\n",
      "Variable containing:\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4402\n",
      " 0.4402\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4402\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4402\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4404\n",
      " 0.4404\n",
      " 0.4404\n",
      " 0.4402\n",
      " 0.4402\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4402\n",
      " 0.4403\n",
      " 0.4404\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4402\n",
      " 0.4404\n",
      " 0.4404\n",
      " 0.4403\n",
      " 0.4401\n",
      " 0.4403\n",
      " 0.4404\n",
      " 0.4404\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4401\n",
      " 0.4402\n",
      " 0.4401\n",
      " 0.4404\n",
      " 0.4403\n",
      " 0.4405\n",
      " 0.4404\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4404\n",
      " 0.4404\n",
      " 0.4403\n",
      " 0.4402\n",
      " 0.4404\n",
      " 0.4401\n",
      " 0.4403\n",
      " 0.4402\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4403\n",
      " 0.4402\n",
      " 0.4402\n",
      " 0.4404\n",
      " 0.4403\n",
      "[torch.FloatTensor of size 64x1]\n",
      "\n",
      "K\n",
      "Variable containing:\n",
      " 0.2328  0.4077  0.3595\n",
      " 0.2326  0.4079  0.3595\n",
      " 0.2326  0.4080  0.3594\n",
      " 0.2328  0.4078  0.3595\n",
      " 0.2328  0.4079  0.3593\n",
      " 0.2327  0.4077  0.3596\n",
      " 0.2328  0.4081  0.3591\n",
      " 0.2329  0.4076  0.3595\n",
      " 0.2327  0.4077  0.3596\n",
      " 0.2328  0.4077  0.3595\n",
      " 0.2328  0.4081  0.3591\n",
      " 0.2328  0.4080  0.3592\n",
      " 0.2326  0.4079  0.3595\n",
      " 0.2329  0.4079  0.3592\n",
      " 0.2326  0.4081  0.3593\n",
      " 0.2330  0.4077  0.3593\n",
      " 0.2328  0.4080  0.3592\n",
      " 0.2328  0.4078  0.3594\n",
      " 0.2328  0.4078  0.3595\n",
      " 0.2327  0.4077  0.3596\n",
      " 0.2328  0.4078  0.3594\n",
      " 0.2329  0.4075  0.3596\n",
      " 0.2328  0.4079  0.3593\n",
      " 0.2328  0.4078  0.3594\n",
      " 0.2329  0.4080  0.3591\n",
      " 0.2328  0.4078  0.3594\n",
      " 0.2328  0.4079  0.3593\n",
      " 0.2326  0.4078  0.3596\n",
      " 0.2327  0.4077  0.3596\n",
      " 0.2330  0.4078  0.3592\n",
      " 0.2328  0.4076  0.3596\n",
      " 0.2328  0.4076  0.3596\n",
      " 0.2328  0.4079  0.3593\n",
      " 0.2328  0.4078  0.3595\n",
      " 0.2329  0.4076  0.3595\n",
      " 0.2327  0.4077  0.3596\n",
      " 0.2327  0.4078  0.3595\n",
      " 0.2327  0.4077  0.3596\n",
      " 0.2328  0.4077  0.3594\n",
      " 0.2330  0.4078  0.3592\n",
      " 0.2328  0.4079  0.3593\n",
      " 0.2329  0.4078  0.3593\n",
      " 0.2328  0.4077  0.3594\n",
      " 0.2329  0.4080  0.3591\n",
      " 0.2329  0.4078  0.3594\n",
      " 0.2327  0.4080  0.3593\n",
      " 0.2330  0.4078  0.3592\n",
      " 0.2328  0.4079  0.3592\n",
      " 0.2329  0.4077  0.3594\n",
      " 0.2328  0.4079  0.3593\n",
      " 0.2328  0.4076  0.3595\n",
      " 0.2328  0.4078  0.3593\n",
      " 0.2330  0.4079  0.3591\n",
      " 0.2328  0.4076  0.3596\n",
      " 0.2328  0.4079  0.3593\n",
      " 0.2330  0.4076  0.3595\n",
      " 0.2329  0.4077  0.3594\n",
      " 0.2329  0.4079  0.3591\n",
      " 0.2329  0.4075  0.3596\n",
      " 0.2328  0.4079  0.3593\n",
      " 0.2330  0.4077  0.3593\n",
      " 0.2327  0.4079  0.3593\n",
      " 0.2327  0.4080  0.3593\n",
      " 0.2329  0.4075  0.3595\n",
      "[torch.FloatTensor of size 64x3]\n",
      "\n",
      "L\n",
      "Variable containing:\n",
      " 1.4036\n",
      " 1.4035\n",
      " 1.4034\n",
      " 1.4032\n",
      " 1.4038\n",
      " 1.4030\n",
      " 1.4035\n",
      " 1.4032\n",
      " 1.4032\n",
      " 1.4033\n",
      " 1.4036\n",
      " 1.4031\n",
      " 1.4032\n",
      " 1.4035\n",
      " 1.4037\n",
      " 1.4034\n",
      " 1.4036\n",
      " 1.4030\n",
      " 1.4037\n",
      " 1.4032\n",
      " 1.4032\n",
      " 1.4034\n",
      " 1.4033\n",
      " 1.4037\n",
      " 1.4038\n",
      " 1.4032\n",
      " 1.4038\n",
      " 1.4032\n",
      " 1.4031\n",
      " 1.4034\n",
      " 1.4034\n",
      " 1.4035\n",
      " 1.4038\n",
      " 1.4037\n",
      " 1.4033\n",
      " 1.4032\n",
      " 1.4036\n",
      " 1.4031\n",
      " 1.4035\n",
      " 1.4034\n",
      " 1.4033\n",
      " 1.4032\n",
      " 1.4034\n",
      " 1.4037\n",
      " 1.4036\n",
      " 1.4033\n",
      " 1.4035\n",
      " 1.4032\n",
      " 1.4035\n",
      " 1.4033\n",
      " 1.4035\n",
      " 1.4035\n",
      " 1.4036\n",
      " 1.4033\n",
      " 1.4032\n",
      " 1.4033\n",
      " 1.4036\n",
      " 1.4038\n",
      " 1.4034\n",
      " 1.4033\n",
      " 1.4034\n",
      " 1.4033\n",
      " 1.4033\n",
      " 1.4032\n",
      "[torch.FloatTensor of size 64x1]\n",
      "\n",
      "R\n",
      "Variable containing:\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "S\n",
      "Variable containing:\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "T\n",
      "Variable containing:\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "       ...          ⋱          ...       \n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "U\n",
      "Variable containing:\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "       ...          ⋱          ...       \n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "V\n",
      "Variable containing:\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "       ...          ⋱          ...       \n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "W\n",
      "Variable containing:\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "       ...          ⋱          ...       \n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "X\n",
      "Variable containing:\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "       ...          ⋱          ...       \n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "M\n",
      "Variable containing:\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "       ...          ⋱          ...       \n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "N\n",
      "Variable containing:\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "       ...          ⋱          ...       \n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "[torch.FloatTensor of size 64x20]\n",
      "\n",
      "A\n",
      "Variable containing:\n",
      "-0.5304 -0.3641 -0.0862  ...  -0.1557 -0.7464  0.3702\n",
      "-0.5307 -0.3633 -0.0865  ...  -0.1557 -0.7454  0.3698\n",
      "-0.5307 -0.3637 -0.0861  ...  -0.1553 -0.7454  0.3702\n",
      "          ...             ⋱             ...          \n",
      "-0.5302 -0.3630 -0.0852  ...  -0.1554 -0.7456  0.3705\n",
      "-0.5313 -0.3644 -0.0864  ...  -0.1542 -0.7455  0.3699\n",
      "-0.5305 -0.3645 -0.0862  ...  -0.1543 -0.7467  0.3700\n",
      "[torch.FloatTensor of size 64x20]\n",
      "\n",
      "B\n",
      "Variable containing:\n",
      " 0.7365\n",
      " 0.7367\n",
      " 0.7365\n",
      " 0.7362\n",
      " 0.7368\n",
      " 0.7363\n",
      " 0.7361\n",
      " 0.7365\n",
      " 0.7363\n",
      " 0.7366\n",
      " 0.7363\n",
      " 0.7361\n",
      " 0.7366\n",
      " 0.7366\n",
      " 0.7368\n",
      " 0.7364\n",
      " 0.7364\n",
      " 0.7361\n",
      " 0.7368\n",
      " 0.7363\n",
      " 0.7360\n",
      " 0.7366\n",
      " 0.7361\n",
      " 0.7367\n",
      " 0.7366\n",
      " 0.7360\n",
      " 0.7368\n",
      " 0.7367\n",
      " 0.7362\n",
      " 0.7362\n",
      " 0.7367\n",
      " 0.7368\n",
      " 0.7365\n",
      " 0.7368\n",
      " 0.7364\n",
      " 0.7365\n",
      " 0.7367\n",
      " 0.7362\n",
      " 0.7366\n",
      " 0.7362\n",
      " 0.7361\n",
      " 0.7359\n",
      " 0.7366\n",
      " 0.7364\n",
      " 0.7365\n",
      " 0.7365\n",
      " 0.7364\n",
      " 0.7361\n",
      " 0.7364\n",
      " 0.7361\n",
      " 0.7366\n",
      " 0.7366\n",
      " 0.7362\n",
      " 0.7366\n",
      " 0.7361\n",
      " 0.7364\n",
      " 0.7366\n",
      " 0.7364\n",
      " 0.7366\n",
      " 0.7364\n",
      " 0.7361\n",
      " 0.7363\n",
      " 0.7365\n",
      " 0.7363\n",
      "[torch.FloatTensor of size 64x1]\n",
      "\n",
      "C\n",
      "Variable containing:\n",
      " 0.6012\n",
      " 0.6013\n",
      " 0.6013\n",
      " 0.6012\n",
      " 0.6013\n",
      " 0.6013\n",
      " 0.6012\n",
      " 0.6013\n",
      " 0.6016\n",
      " 0.6012\n",
      " 0.6014\n",
      " 0.6013\n",
      " 0.6014\n",
      " 0.6015\n",
      " 0.6013\n",
      " 0.6013\n",
      " 0.6014\n",
      " 0.6013\n",
      " 0.6013\n",
      " 0.6016\n",
      " 0.6013\n",
      " 0.6016\n",
      " 0.6013\n",
      " 0.6013\n",
      " 0.6014\n",
      " 0.6013\n",
      " 0.6011\n",
      " 0.6014\n",
      " 0.6013\n",
      " 0.6012\n",
      " 0.6015\n",
      " 0.6014\n",
      " 0.6012\n",
      " 0.6013\n",
      " 0.6013\n",
      " 0.6015\n",
      " 0.6011\n",
      " 0.6013\n",
      " 0.6011\n",
      " 0.6012\n",
      " 0.6012\n",
      " 0.6013\n",
      " 0.6015\n",
      " 0.6011\n",
      " 0.6013\n",
      " 0.6015\n",
      " 0.6015\n",
      " 0.6013\n",
      " 0.6012\n",
      " 0.6013\n",
      " 0.6015\n",
      " 0.6011\n",
      " 0.6012\n",
      " 0.6012\n",
      " 0.6012\n",
      " 0.6016\n",
      " 0.6014\n",
      " 0.6012\n",
      " 0.6016\n",
      " 0.6015\n",
      " 0.6013\n",
      " 0.6011\n",
      " 0.6015\n",
      " 0.6014\n",
      "[torch.FloatTensor of size 64x1]\n",
      "\n",
      "D\n",
      "Variable containing:\n",
      " 0.2616  0.4144  0.3240\n",
      " 0.2617  0.4142  0.3241\n",
      " 0.2615  0.4144  0.3241\n",
      " 0.2617  0.4143  0.3240\n",
      " 0.2618  0.4138  0.3244\n",
      " 0.2617  0.4143  0.3240\n",
      " 0.2617  0.4144  0.3239\n",
      " 0.2618  0.4141  0.3240\n",
      " 0.2616  0.4143  0.3241\n",
      " 0.2615  0.4144  0.3241\n",
      " 0.2620  0.4139  0.3241\n",
      " 0.2618  0.4142  0.3241\n",
      " 0.2615  0.4144  0.3241\n",
      " 0.2619  0.4138  0.3243\n",
      " 0.2616  0.4141  0.3243\n",
      " 0.2618  0.4140  0.3242\n",
      " 0.2616  0.4141  0.3243\n",
      " 0.2617  0.4144  0.3239\n",
      " 0.2614  0.4144  0.3242\n",
      " 0.2616  0.4143  0.3241\n",
      " 0.2618  0.4144  0.3238\n",
      " 0.2617  0.4141  0.3242\n",
      " 0.2615  0.4144  0.3241\n",
      " 0.2617  0.4142  0.3241\n",
      " 0.2620  0.4138  0.3242\n",
      " 0.2618  0.4144  0.3238\n",
      " 0.2617  0.4141  0.3243\n",
      " 0.2616  0.4142  0.3242\n",
      " 0.2615  0.4145  0.3240\n",
      " 0.2618  0.4144  0.3238\n",
      " 0.2616  0.4141  0.3243\n",
      " 0.2613  0.4144  0.3243\n",
      " 0.2617  0.4142  0.3241\n",
      " 0.2614  0.4144  0.3242\n",
      " 0.2614  0.4145  0.3240\n",
      " 0.2616  0.4142  0.3242\n",
      " 0.2617  0.4142  0.3240\n",
      " 0.2615  0.4145  0.3240\n",
      " 0.2614  0.4145  0.3241\n",
      " 0.2618  0.4144  0.3238\n",
      " 0.2616  0.4146  0.3237\n",
      " 0.2616  0.4146  0.3238\n",
      " 0.2615  0.4143  0.3241\n",
      " 0.2618  0.4142  0.3240\n",
      " 0.2617  0.4141  0.3242\n",
      " 0.2618  0.4139  0.3243\n",
      " 0.2616  0.4140  0.3243\n",
      " 0.2620  0.4141  0.3240\n",
      " 0.2615  0.4146  0.3239\n",
      " 0.2615  0.4144  0.3241\n",
      " 0.2614  0.4144  0.3242\n",
      " 0.2616  0.4144  0.3240\n",
      " 0.2617  0.4142  0.3241\n",
      " 0.2614  0.4145  0.3242\n",
      " 0.2618  0.4144  0.3238\n",
      " 0.2615  0.4143  0.3242\n",
      " 0.2616  0.4143  0.3242\n",
      " 0.2616  0.4143  0.3241\n",
      " 0.2617  0.4141  0.3242\n",
      " 0.2615  0.4141  0.3243\n",
      " 0.2616  0.4145  0.3239\n",
      " 0.2616  0.4145  0.3239\n",
      " 0.2618  0.4139  0.3243\n",
      " 0.2617  0.4143  0.3240\n",
      "[torch.FloatTensor of size 64x3]\n",
      "\n",
      "E\n",
      "Variable containing:\n",
      " 1.6583\n",
      " 1.6578\n",
      " 1.6577\n",
      " 1.6576\n",
      " 1.6584\n",
      " 1.6574\n",
      " 1.6582\n",
      " 1.6577\n",
      " 1.6574\n",
      " 1.6579\n",
      " 1.6579\n",
      " 1.6577\n",
      " 1.6575\n",
      " 1.6581\n",
      " 1.6581\n",
      " 1.6582\n",
      " 1.6579\n",
      " 1.6573\n",
      " 1.6582\n",
      " 1.6574\n",
      " 1.6577\n",
      " 1.6578\n",
      " 1.6580\n",
      " 1.6582\n",
      " 1.6583\n",
      " 1.6577\n",
      " 1.6586\n",
      " 1.6575\n",
      " 1.6576\n",
      " 1.6580\n",
      " 1.6577\n",
      " 1.6579\n",
      " 1.6586\n",
      " 1.6582\n",
      " 1.6580\n",
      " 1.6573\n",
      " 1.6582\n",
      " 1.6576\n",
      " 1.6582\n",
      " 1.6580\n",
      " 1.6578\n",
      " 1.6576\n",
      " 1.6579\n",
      " 1.6586\n",
      " 1.6584\n",
      " 1.6577\n",
      " 1.6581\n",
      " 1.6577\n",
      " 1.6583\n",
      " 1.6580\n",
      " 1.6579\n",
      " 1.6582\n",
      " 1.6584\n",
      " 1.6579\n",
      " 1.6576\n",
      " 1.6578\n",
      " 1.6580\n",
      " 1.6586\n",
      " 1.6578\n",
      " 1.6577\n",
      " 1.6580\n",
      " 1.6578\n",
      " 1.6577\n",
      " 1.6578\n",
      "[torch.FloatTensor of size 64x1]\n",
      "\n",
      "F\n",
      "Variable containing:\n",
      " 0.4502  0.5786  0.4623  ...   0.6272  0.5198  0.5845\n",
      " 0.4503  0.5785  0.4621  ...   0.6272  0.5194  0.5849\n",
      " 0.4503  0.5784  0.4620  ...   0.6270  0.5195  0.5850\n",
      "          ...             ⋱             ...          \n",
      " 0.4502  0.5784  0.4621  ...   0.6269  0.5196  0.5849\n",
      " 0.4505  0.5785  0.4619  ...   0.6271  0.5197  0.5850\n",
      " 0.4503  0.5785  0.4622  ...   0.6271  0.5199  0.5846\n",
      "[torch.FloatTensor of size 64x20]\n",
      "\n",
      "Variable containing:\n",
      "-0.5304 -0.3641 -0.0862  ...  -0.1557 -0.7464  0.3702\n",
      "-0.5307 -0.3633 -0.0865  ...  -0.1557 -0.7454  0.3698\n",
      "-0.5307 -0.3637 -0.0861  ...  -0.1553 -0.7454  0.3702\n",
      "          ...             ⋱             ...          \n",
      "-0.5302 -0.3630 -0.0852  ...  -0.1554 -0.7456  0.3705\n",
      "-0.5313 -0.3644 -0.0864  ...  -0.1542 -0.7455  0.3699\n",
      "-0.5305 -0.3645 -0.0862  ...  -0.1543 -0.7467  0.3700\n",
      "[torch.FloatTensor of size 64x20]\n",
      "\n",
      "Variable containing:\n",
      " 0.7365\n",
      " 0.7367\n",
      " 0.7365\n",
      " 0.7362\n",
      " 0.7368\n",
      " 0.7363\n",
      " 0.7361\n",
      " 0.7365\n",
      " 0.7363\n",
      " 0.7366\n",
      " 0.7363\n",
      " 0.7361\n",
      " 0.7366\n",
      " 0.7366\n",
      " 0.7368\n",
      " 0.7364\n",
      " 0.7364\n",
      " 0.7361\n",
      " 0.7368\n",
      " 0.7363\n",
      " 0.7360\n",
      " 0.7366\n",
      " 0.7361\n",
      " 0.7367\n",
      " 0.7366\n",
      " 0.7360\n",
      " 0.7368\n",
      " 0.7367\n",
      " 0.7362\n",
      " 0.7362\n",
      " 0.7367\n",
      " 0.7368\n",
      " 0.7365\n",
      " 0.7368\n",
      " 0.7364\n",
      " 0.7365\n",
      " 0.7367\n",
      " 0.7362\n",
      " 0.7366\n",
      " 0.7362\n",
      " 0.7361\n",
      " 0.7359\n",
      " 0.7366\n",
      " 0.7364\n",
      " 0.7365\n",
      " 0.7365\n",
      " 0.7364\n",
      " 0.7361\n",
      " 0.7364\n",
      " 0.7361\n",
      " 0.7366\n",
      " 0.7366\n",
      " 0.7362\n",
      " 0.7366\n",
      " 0.7361\n",
      " 0.7364\n",
      " 0.7366\n",
      " 0.7364\n",
      " 0.7366\n",
      " 0.7364\n",
      " 0.7361\n",
      " 0.7363\n",
      " 0.7365\n",
      " 0.7363\n",
      "[torch.FloatTensor of size 64x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.6012\n",
      " 0.6013\n",
      " 0.6013\n",
      " 0.6012\n",
      " 0.6013\n",
      " 0.6013\n",
      " 0.6012\n",
      " 0.6013\n",
      " 0.6016\n",
      " 0.6012\n",
      " 0.6014\n",
      " 0.6013\n",
      " 0.6014\n",
      " 0.6015\n",
      " 0.6013\n",
      " 0.6013\n",
      " 0.6014\n",
      " 0.6013\n",
      " 0.6013\n",
      " 0.6016\n",
      " 0.6013\n",
      " 0.6016\n",
      " 0.6013\n",
      " 0.6013\n",
      " 0.6014\n",
      " 0.6013\n",
      " 0.6011\n",
      " 0.6014\n",
      " 0.6013\n",
      " 0.6012\n",
      " 0.6015\n",
      " 0.6014\n",
      " 0.6012\n",
      " 0.6013\n",
      " 0.6013\n",
      " 0.6015\n",
      " 0.6011\n",
      " 0.6013\n",
      " 0.6011\n",
      " 0.6012\n",
      " 0.6012\n",
      " 0.6013\n",
      " 0.6015\n",
      " 0.6011\n",
      " 0.6013\n",
      " 0.6015\n",
      " 0.6015\n",
      " 0.6013\n",
      " 0.6012\n",
      " 0.6013\n",
      " 0.6015\n",
      " 0.6011\n",
      " 0.6012\n",
      " 0.6012\n",
      " 0.6012\n",
      " 0.6016\n",
      " 0.6014\n",
      " 0.6012\n",
      " 0.6016\n",
      " 0.6015\n",
      " 0.6013\n",
      " 0.6011\n",
      " 0.6015\n",
      " 0.6014\n",
      "[torch.FloatTensor of size 64x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.2616  0.4144  0.3240\n",
      " 0.2617  0.4142  0.3241\n",
      " 0.2615  0.4144  0.3241\n",
      " 0.2617  0.4143  0.3240\n",
      " 0.2618  0.4138  0.3244\n",
      " 0.2617  0.4143  0.3240\n",
      " 0.2617  0.4144  0.3239\n",
      " 0.2618  0.4141  0.3240\n",
      " 0.2616  0.4143  0.3241\n",
      " 0.2615  0.4144  0.3241\n",
      " 0.2620  0.4139  0.3241\n",
      " 0.2618  0.4142  0.3241\n",
      " 0.2615  0.4144  0.3241\n",
      " 0.2619  0.4138  0.3243\n",
      " 0.2616  0.4141  0.3243\n",
      " 0.2618  0.4140  0.3242\n",
      " 0.2616  0.4141  0.3243\n",
      " 0.2617  0.4144  0.3239\n",
      " 0.2614  0.4144  0.3242\n",
      " 0.2616  0.4143  0.3241\n",
      " 0.2618  0.4144  0.3238\n",
      " 0.2617  0.4141  0.3242\n",
      " 0.2615  0.4144  0.3241\n",
      " 0.2617  0.4142  0.3241\n",
      " 0.2620  0.4138  0.3242\n",
      " 0.2618  0.4144  0.3238\n",
      " 0.2617  0.4141  0.3243\n",
      " 0.2616  0.4142  0.3242\n",
      " 0.2615  0.4145  0.3240\n",
      " 0.2618  0.4144  0.3238\n",
      " 0.2616  0.4141  0.3243\n",
      " 0.2613  0.4144  0.3243\n",
      " 0.2617  0.4142  0.3241\n",
      " 0.2614  0.4144  0.3242\n",
      " 0.2614  0.4145  0.3240\n",
      " 0.2616  0.4142  0.3242\n",
      " 0.2617  0.4142  0.3240\n",
      " 0.2615  0.4145  0.3240\n",
      " 0.2614  0.4145  0.3241\n",
      " 0.2618  0.4144  0.3238\n",
      " 0.2616  0.4146  0.3237\n",
      " 0.2616  0.4146  0.3238\n",
      " 0.2615  0.4143  0.3241\n",
      " 0.2618  0.4142  0.3240\n",
      " 0.2617  0.4141  0.3242\n",
      " 0.2618  0.4139  0.3243\n",
      " 0.2616  0.4140  0.3243\n",
      " 0.2620  0.4141  0.3240\n",
      " 0.2615  0.4146  0.3239\n",
      " 0.2615  0.4144  0.3241\n",
      " 0.2614  0.4144  0.3242\n",
      " 0.2616  0.4144  0.3240\n",
      " 0.2617  0.4142  0.3241\n",
      " 0.2614  0.4145  0.3242\n",
      " 0.2618  0.4144  0.3238\n",
      " 0.2615  0.4143  0.3242\n",
      " 0.2616  0.4143  0.3242\n",
      " 0.2616  0.4143  0.3241\n",
      " 0.2617  0.4141  0.3242\n",
      " 0.2615  0.4141  0.3243\n",
      " 0.2616  0.4145  0.3239\n",
      " 0.2616  0.4145  0.3239\n",
      " 0.2618  0.4139  0.3243\n",
      " 0.2617  0.4143  0.3240\n",
      "[torch.FloatTensor of size 64x3]\n",
      "\n",
      "Variable containing:\n",
      " 1.6583\n",
      " 1.6578\n",
      " 1.6577\n",
      " 1.6576\n",
      " 1.6584\n",
      " 1.6574\n",
      " 1.6582\n",
      " 1.6577\n",
      " 1.6574\n",
      " 1.6579\n",
      " 1.6579\n",
      " 1.6577\n",
      " 1.6575\n",
      " 1.6581\n",
      " 1.6581\n",
      " 1.6582\n",
      " 1.6579\n",
      " 1.6573\n",
      " 1.6582\n",
      " 1.6574\n",
      " 1.6577\n",
      " 1.6578\n",
      " 1.6580\n",
      " 1.6582\n",
      " 1.6583\n",
      " 1.6577\n",
      " 1.6586\n",
      " 1.6575\n",
      " 1.6576\n",
      " 1.6580\n",
      " 1.6577\n",
      " 1.6579\n",
      " 1.6586\n",
      " 1.6582\n",
      " 1.6580\n",
      " 1.6573\n",
      " 1.6582\n",
      " 1.6576\n",
      " 1.6582\n",
      " 1.6580\n",
      " 1.6578\n",
      " 1.6576\n",
      " 1.6579\n",
      " 1.6586\n",
      " 1.6584\n",
      " 1.6577\n",
      " 1.6581\n",
      " 1.6577\n",
      " 1.6583\n",
      " 1.6580\n",
      " 1.6579\n",
      " 1.6582\n",
      " 1.6584\n",
      " 1.6579\n",
      " 1.6576\n",
      " 1.6578\n",
      " 1.6580\n",
      " 1.6586\n",
      " 1.6578\n",
      " 1.6577\n",
      " 1.6580\n",
      " 1.6578\n",
      " 1.6577\n",
      " 1.6578\n",
      "[torch.FloatTensor of size 64x1]\n",
      "\n",
      "G\n",
      "Variable containing:\n",
      "-0.1666 -0.0150 -0.6899  ...  -0.1797 -0.4102  0.1552\n",
      "-0.1671 -0.0160 -0.6895  ...  -0.1807 -0.4089  0.1553\n",
      "-0.1669 -0.0160 -0.6893  ...  -0.1805 -0.4092  0.1555\n",
      "          ...             ⋱             ...          \n",
      "-0.1666 -0.0161 -0.6888  ...  -0.1796 -0.4094  0.1558\n",
      "-0.1669 -0.0151 -0.6900  ...  -0.1801 -0.4092  0.1569\n",
      "-0.1661 -0.0143 -0.6904  ...  -0.1792 -0.4104  0.1565\n",
      "[torch.FloatTensor of size 64x20]\n",
      "\n",
      "R\n",
      "Variable containing:\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "S\n",
      "Variable containing:\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "T\n",
      "Variable containing:\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "       ...          ⋱          ...       \n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "U\n",
      "Variable containing:\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "       ...          ⋱          ...       \n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "V\n",
      "Variable containing:\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "       ...          ⋱          ...       \n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "W\n",
      "Variable containing:\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "       ...          ⋱          ...       \n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "  inf   inf   inf  ...    inf   inf   inf\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "X\n",
      "Variable containing:\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "       ...          ⋱          ...       \n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n",
      "O\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      "     ...       ⋱       ...    \n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      "\n",
      "( 1 ,.,.) = \n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      "     ...       ⋱       ...    \n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      "\n",
      "( 2 ,.,.) = \n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      "     ...       ⋱       ...    \n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      "... \n",
      "\n",
      "(61 ,.,.) = \n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      "     ...       ⋱       ...    \n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      "\n",
      "(62 ,.,.) = \n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      "     ...       ⋱       ...    \n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      "\n",
      "(63 ,.,.) = \n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      "     ...       ⋱       ...    \n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      " nan nan nan  ...  nan nan nan\n",
      "[torch.FloatTensor of size 64x128x20]\n",
      "\n",
      "P\n",
      "Variable containing:\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "       ...          ⋱          ...       \n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "  nan   nan   nan  ...    nan   nan   nan\n",
      "[torch.FloatTensor of size 64x128]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainAll(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
