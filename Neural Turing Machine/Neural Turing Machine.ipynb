{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets_library import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of inspiration from https://github.com/loudinthecloud/pytorch-ntm. Hyperparameters were chosen based\n",
    "# upon his experiments.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seed(seed=None):\n",
    "    \"\"\"Seed the RNGs for predicatability/reproduction purposes.\"\"\"\n",
    "    if seed is None:\n",
    "        seed = int(get_ms() // 1000)\n",
    "\n",
    "    print(\"Using seed=%d\", seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Memory(nn.Module):\n",
    "    def __init__(self, address_count, address_dimension, batch_size):\n",
    "        super(NTM_Memory, self).__init__()\n",
    "        self.initial_memory = nn.Parameter(torch.zeros(1, address_count, address_dimension))\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        _, N, M = self.initial_memory.size()\n",
    "        stdev = 1 / np.sqrt(N + M)\n",
    "        nn.init.uniform(self.initial_memory, -stdev, stdev)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.memory = self.initial_memory.repeat(self.batch_size, 1, 1)\n",
    "    \n",
    "    def address_memory(self, key_vec, prev_address_vec, β, g, s, γ):\n",
    "        EPSILON = 1e-16\n",
    "        result = F.cosine_similarity((key_vec+EPSILON).unsqueeze(1).expand_as(self.memory), \n",
    "                                     self.memory+EPSILON, dim = 2)\n",
    "        result = F.softmax(β * result, dim=1)\n",
    "        result = g * result + (1 - g) * prev_address_vec\n",
    "        result = torch.cat((result[:, 1:], result[:, :1]), 1) * s[:, 0:1] + result * s[:, 1:2] + \\\n",
    "                 torch.cat((result[:, -1:], result[:, :-1]), 1) * s[:, 2:3]\n",
    "\n",
    "#         result = result ** γ\n",
    "#         result = result / (result.sum(1, keepdim=True) + EPSILON)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def read_memory(self, address_vec):\n",
    "        return torch.bmm(self.memory.transpose(1,2), address_vec.unsqueeze(2)).squeeze(2)\n",
    "    \n",
    "    def update_memory(self, address_vec, erase_vec, add_vec):\n",
    "        self.memory = self.memory * (1 - torch.bmm(address_vec.unsqueeze(2), erase_vec.unsqueeze(1)))\n",
    "        self.memory += torch.bmm(address_vec.unsqueeze(2), add_vec.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_cols(mat, lengths):\n",
    "    \"\"\"Split a 2D matrix to variable length columns.\"\"\"\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results += [mat[:, s:e]]\n",
    "    return results\n",
    "\n",
    "class NTM_Head(nn.Module):\n",
    "    def __init__(self, address_count, address_dimension, \n",
    "                 controller_output_size):\n",
    "        super(NTM_Head, self).__init__()\n",
    "        \n",
    "        self.controller_output_size = controller_output_size\n",
    "        self.N = address_count\n",
    "        self.M = address_dimension\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Read_Head(NTM_Head):\n",
    "    def __init__(self, address_count, address_dimension, controller_output_size, batch_size):\n",
    "        super(NTM_Read_Head, self).__init__(address_count, address_dimension, controller_output_size)\n",
    "        \n",
    "        self.read_parameters_lengths = [self.M, 1, 1, 3, 1]\n",
    "        self.fc_read_parameters = nn.Linear(controller_output_size, sum(self.read_parameters_lengths))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_read_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_read_parameters.bias, std=0.01)\n",
    "        \n",
    "        self.initial_address_vec = nn.Parameter(torch.zeros(self.N))\n",
    "        self.initial_read = nn.Parameter(torch.randn(1, self.M) * 0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.prev_address_vec = self.initial_address_vec.clone()\n",
    "        self.prev_read = self.initial_read.repeat(self.batch_size, 1)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        read_parameters = self.fc_read_parameters(x)\n",
    "        \n",
    "        key_vec, β, g, s, γ = _split_cols(read_parameters, self.read_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "        \n",
    "        self.prev_address_vec = memory.address_memory(key_vec, self.prev_address_vec, β, g, s, γ)\n",
    "        new_read = memory.read_memory(self.prev_address_vec)        \n",
    "        self.prev_read = new_read\n",
    "        return new_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Write_Head(NTM_Head):\n",
    "    def __init__(self, address_count, address_dimension, controller_output_size):\n",
    "        super(NTM_Write_Head, self).__init__(address_count, address_dimension, controller_output_size)\n",
    "        \n",
    "        self.write_parameters_lengths = [self.M, 1, 1, 3, 1, self.M, self.M]\n",
    "        self.fc_write_parameters = nn.Linear(controller_output_size, sum(self.write_parameters_lengths))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_write_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_write_parameters.bias, std=0.01)\n",
    "        \n",
    "        self.initial_address_vec = nn.Parameter(torch.zeros(self.N))\n",
    "    \n",
    "    def initialize_state(self):       \n",
    "        self.prev_address_vec = self.initial_address_vec.clone()\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return False\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        write_parameters = self.fc_write_parameters(x)\n",
    "                                       \n",
    "        key_vec, β, g, s, γ, erase_vec, add_vec = _split_cols(write_parameters, self.write_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "        erase_vec = F.sigmoid(erase_vec)\n",
    "                                               \n",
    "        self.prev_address_vec = memory.address_memory(key_vec, self.prev_address_vec, β, g, s, γ)\n",
    "        memory.update_memory(self.prev_address_vec, erase_vec, add_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, batch_size, controller, controller_output_size, \n",
    "                 output_size, address_count, address_dimension, heads):\n",
    "        super(NTM, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize controller\n",
    "        self.controller = controller\n",
    "        \n",
    "        # Create output gate. No activation function is used with it because\n",
    "        # I used BCEWithLogitsLoss which deals with the sigmoid in a more\n",
    "        # numerically stable manner.\n",
    "        self.outputGate = nn.Linear(controller_output_size, output_size)\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = NTM_Memory(address_count, address_dimension, batch_size)\n",
    "\n",
    "        # Construct the heads.\n",
    "        self.heads = nn.ModuleList()\n",
    "        \n",
    "        for head_id in heads:\n",
    "            if head_id == 0:\n",
    "                self.heads.append(NTM_Read_Head(address_count, address_dimension, \n",
    "                                                controller_output_size, batch_size))\n",
    "            else:\n",
    "                self.heads.append(NTM_Write_Head(address_count, address_dimension,\n",
    "                                                 controller_output_size))\n",
    "        self.initialize_state()\n",
    "        \n",
    "    def initialize_state(self):\n",
    "        self.prev_reads = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            head.initialize_state()\n",
    "            \n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head.prev_read)\n",
    "        \n",
    "        self.memory.initialize_state()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.outputGate.weight)\n",
    "        nn.init.normal(self.outputGate.bias, std=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.initialize_state()\n",
    "        outputs = []\n",
    "        \n",
    "        for current_observation in x.transpose(0,1):\n",
    "            self.prev_reads.append(current_observation)\n",
    "            controller_input = torch.cat(self.prev_reads, 1)\n",
    "            controller_output = self.controller(controller_input)\n",
    "            self.prev_reads = []\n",
    "\n",
    "            for head in self.heads:                \n",
    "                if head.is_read_head():\n",
    "                    self.prev_reads.append(head(controller_output, self.memory))\n",
    "                else:\n",
    "                    head(controller_output, self.memory)\n",
    "                    \n",
    "            current_output = self.outputGate(controller_output)\n",
    "            outputs.append(current_output)\n",
    "        \n",
    "        return torch.stack(outputs).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyTaskDataset(data.Dataset):\n",
    "    def __init__(self, num_batches, batch_size, lower, upper, seq_size):\n",
    "        self.input_list = []\n",
    "        self.label_list = []\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            data, label = self.generate_batch(batch_size, lower, upper, seq_size)\n",
    "            self.input_list.append(data)\n",
    "            self.label_list.append(label)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batch(self, batch_size, lower, upper, seq_size):\n",
    "        seq_length = random.randint(lower, upper)\n",
    "        label = torch.from_numpy(\n",
    "                np.random.binomial(1, 0.5, (seq_length, batch_size, seq_size))).float()\n",
    "        end_marker = torch.zeros(seq_length, batch_size, 1)\n",
    "        seq = torch.cat((label, end_marker), 2)\n",
    "        delimiter_column = torch.zeros(1, batch_size, seq_size+1)\n",
    "        delimiter_column[0, :, seq_size] = 1\n",
    "        seq = torch.cat((seq, delimiter_column), 0)\n",
    "        output_time = torch.zeros(seq_length, batch_size, seq_size+1)\n",
    "        seq = torch.cat((seq, output_time), 0)\n",
    "        return seq, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)*self.batch_size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        batch_index = i//self.batch_size\n",
    "        index_in_batch = i % self.batch_size\n",
    "        return self.input_list[batch_index][:, index_in_batch, :], self.label_list[batch_index][:, index_in_batch, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionTaskDataset(data.Dataset):\n",
    "    def __init__(self, num_batches, batch_size, lower, upper, seq_size):\n",
    "        self.input_list = []\n",
    "        self.label_list = []\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            data, label = self.generate_batch(batch_size, lower, upper, 0)\n",
    "            self.input_list.append(data)\n",
    "            self.label_list.append(label)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batch(self, batch_size, min_int, max_int, think_time):\n",
    "        seq_length = think_time + 2\n",
    "        inputs = [random.randint(min_int, max_int - 1) for _ in range(batch_size)]\n",
    "        outputs = [(inputs[i] + 2) % max_int for i in range(len(inputs))]\n",
    "        input_indices = torch.LongTensor(inputs).unsqueeze(1)\n",
    "        output_indices = torch.LongTensor(outputs).unsqueeze(1)\n",
    "        input_one_hot = torch.zeros(batch_size,max_int)\n",
    "        input_one_hot.scatter_(1, input_indices, 1)\n",
    "        input_one_hot = input_one_hot.unsqueeze(0)\n",
    "        output_one_hot = torch.zeros(batch_size,max_int)\n",
    "        output_one_hot.scatter_(1, output_indices, 1)\n",
    "        output_one_hot = output_one_hot.unsqueeze(0)\n",
    "        label = output_one_hot\n",
    "        seq = torch.cat((input_one_hot, torch.zeros(seq_length - 1, batch_size, max_int)), 0)\n",
    "        return seq, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)*self.batch_size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        batch_index = i//self.batch_size\n",
    "        index_in_batch = i % self.batch_size\n",
    "        return self.input_list[batch_index][:, index_in_batch, :], self.label_list[batch_index][:, index_in_batch, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncapsulatedLSTM(nn.Module):\n",
    "    def __init__(self, batch_size, all_hiddens, *args, **kwargs):\n",
    "        super(EncapsulatedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(*args, **kwargs)\n",
    "        self.all_hiddens = all_hiddens\n",
    "        self.batch_size = batch_size\n",
    "                \n",
    "        self.num_inputs = args[0]\n",
    "        self.hidden_size = args[1]\n",
    "        self.num_layers = args[2]\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "          \n",
    "    def initialize_state(self):\n",
    "        self.state_tuple = (self.initial_hidden_state.repeat(1, self.batch_size, 1), \n",
    "                            self.initial_cell_state.repeat(1, self.batch_size, 1))\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.initial_hidden_state = nn.Parameter(torch.randn(self.num_layers, 1, self.hidden_size) * 0.05)\n",
    "        self.initial_cell_state = nn.Parameter(torch.randn(self.num_layers, 1, self.hidden_size) * 0.05)\n",
    "        \n",
    "        for p in self.lstm.parameters():\n",
    "            if p.dim() == 1:\n",
    "                nn.init.constant(p, 0)\n",
    "            else:\n",
    "                stdev = 5 / (np.sqrt(self.num_inputs +  self.hidden_size))\n",
    "                nn.init.uniform(p, -stdev, stdev)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.initialize_state()\n",
    "        output, self.state_tuple = self.lstm(input.unsqueeze(0), self.state_tuple)\n",
    "        \n",
    "        if self.all_hiddens:\n",
    "            return self.state_tuple[0].transpose(0,1).view(self.batch_size, -1)\n",
    "        else:\n",
    "            return output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition_task_loss(output, label):\n",
    "    return F.binary_cross_entropy_with_logits(output[:, -1:,:], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_task_loss(output, label):\n",
    "    _, seq_length, _ = label.size()\n",
    "    return F.binary_cross_entropy_with_logits(output[:, -seq_length:, :], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_bits_per_sequence(output, label):\n",
    "    batch_size, seq_length, _ = label.size()\n",
    "    binarized_output = output[:, -seq_length:, :].sign()/2 + 0.5\n",
    "    \n",
    "    # The cost is the number of error bits per sequence\n",
    "    return torch.sum(torch.abs(binarized_output - label))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_clipped_optimizer(optimizer_type):\n",
    "#     class ClippedOptimizer(optimizer_type):\n",
    "#         def step(self, closure=None):\n",
    "#             for group in self.param_groups:\n",
    "#                 for p in group['params']:\n",
    "#                     if p.grad is not None:\n",
    "#                         p.grad = p.grad.data.clamp(-10,10)\n",
    "            \n",
    "#             super().step(closure)\n",
    "    \n",
    "#     return ClippedOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClippedRMSprop = construct_clipped_optimizer(optim.RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hidden_size = 100\n",
    "num_layers = 3\n",
    "seq_size = 4\n",
    "address_size = 20\n",
    "#Initialized for addition task\n",
    "controller = EncapsulatedLSTM(batch_size, False, # all hiddens\n",
    "                              10 + address_size, hidden_size, \n",
    "                              num_layers)\n",
    "\n",
    "#Uncomment for copy task\n",
    "#controller = EncapsulatedLSTM(batch_size, False, # all hiddens\n",
    "#                              10 + address_size, hidden_size, \n",
    "#                              num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_count = 128\n",
    "controller_output_size = hidden_size\n",
    "#Initialized for Addition task\n",
    "ntm = NTM(batch_size, controller, controller_output_size, \n",
    "          10, address_count, address_size, [0, 1])\n",
    "\n",
    "#Uncomment for copy task\n",
    "#ntm = NTM(batch_size, controller, controller_output_size, \n",
    "#          seq_size, address_count, address_size, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ntm = ntm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_seq_length = 3\n",
    "upper_seq_length = 10\n",
    "num_batches = 2000\n",
    "\n",
    "dataset = CopyTaskDataset(num_batches, batch_size, lower_seq_length, upper_seq_length, seq_size)\n",
    "data_loader = data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_int = 0\n",
    "\n",
    "dataset = AdditionTaskDataset(num_batches, batch_size, min_int, max_int, think_time)\n",
    "data_loader = data.DataLoader(dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(ntm.parameters(), momentum=0.9,\n",
    "                          alpha=0.95, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, train_plot_losses, validation_plot_losses = training.train_model(ntm, data_loader, addition_task_loss, optimizer, None, num_epochs=1, print_every=25, deep_copy_desired=False, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 2000, 20), train_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 10000, 100), validation_plot_losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
