{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "n_input = 8 # Copy task (pg 10, https://arxiv.org/pdf/1410.5401.pdf)\n",
    "n_hidden = 50 # Kinda random...\n",
    "n_output = 8 # Copy task\n",
    "\n",
    "class NTM(nn.Module):\n",
    "    def __init__(self, mem_length, mem_width, read_heads, write_heads, key_strength):\n",
    "        super(NTM, self).__init__()\n",
    "        \n",
    "        # Save useful params\n",
    "        self.read_heads = read_heads\n",
    "        self.write_heads = write_heads\n",
    "        self.key_strength = key_strength      \n",
    "        self.M = mem_length\n",
    "        self.N = mem_width\n",
    "        \n",
    "        # Initialize controller\n",
    "        self.lstm = myLSTM(n_input, [n_hidden, n_hidden], n_output, 2)\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = Variable(torch.zeros(mem_length, mem_width))\n",
    "        # Initialize read_weights\n",
    "        self.read_weights = Variable(torch.zeros(mem_length))\n",
    "        \n",
    "        # Functions we'll need later\n",
    "        self.cosine_similarity = nn.CosineSimilarity()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "    # Equation 2\n",
    "    def read(self, weight_vec):\n",
    "        # weight_vec is N; memory is NxM\n",
    "        return torch.matmul(self.weight_vec * self.memory)\n",
    "        \n",
    "    def write(self, weight_vec, erase_vec, add_vec):\n",
    "        erase_matrix = erase(weight_vec, erase_vec)\n",
    "        self.add(weight_vec, add_vec, erase_matrix)\n",
    "    \n",
    "    # Equation 3\n",
    "    def erase(self, weight_vec, erase_vec):\n",
    "        # weight_vec is size N; erase_vec is M\n",
    "        erase_weighting = torch.matmul(weight_vec, erase_vec.unsqueeze(0))\n",
    "        # erase_wighting is NxM; memory is NxM\n",
    "        erase_weighting = torch.ones(self.N, self.M) - erase_weighting\n",
    "        return self.memory * erase_weighting\n",
    "    \n",
    "    # Equation 4\n",
    "    def add(self, weight_vec, add_vec, erase_matrix):\n",
    "        # weight_vec is size N; add_vec is M\n",
    "        add_weighting = torch.matmul(weight_vec, add_vec.unsqueeze(0))\n",
    "        # add_wighting is NxM; erase_matrix is NxM\n",
    "        return self.erase_matrix + add_weighting\n",
    "      \n",
    "    def read_head(self, key_vecs, interpolation_gates, gammas, shift_weights):  \n",
    "        read_vecs = torch.Tensor(self.read_heads)\n",
    "        for i in range(self.read_heads):\n",
    "            content_weights = self.content_focus(key_vecs[i], self.key_strength)\n",
    "            combined_weights = self.location_focus(interpolation_gates[i], \n",
    "                                                   gammas[i], \n",
    "                                                   self.read_weights[i], \n",
    "                                                   self.content_weights[i], \n",
    "                                                   shift_weights[i])\n",
    "            self.read_weights[i] = combined_weights\n",
    "            read_vecs[i] = self.read(self.read_weights[i])\n",
    "        return read_vecs\n",
    "    \n",
    "    def write_head(self, key_vecs, interpolation_gates, gammas, shift_weights, erase_vecs, add_vecs):\n",
    "        for i in range(self.write_heads):\n",
    "            content_weights = self.content_focus(key_vecs[i], self.key_strength)\n",
    "            combined_weights = self.location_focus(interpolation_gates[i], \n",
    "                                                   gammas[i], \n",
    "                                                   self.write_weights[i], \n",
    "                                                   self.content_weights[i], \n",
    "                                                   shift_weights[i])\n",
    "            self.write_weights[i] = combined_weights\n",
    "            self.write(self.read_weights[i], erase_vec[i], add_vec[i])\n",
    "    \n",
    "    # Equations 5, 6\n",
    "    def content_focus(self, key_vec, key_strength):\n",
    "        # TODO - check that this operation is actually doing what we want!\n",
    "        # key_vec.unsqueeze(0) is 1xM; memory is NxM\n",
    "        sim_vec = self.cosine_similarity(key_vec.unsqueeze(0), self.memory)\n",
    "        # sim_vec is a matrix of length N\n",
    "        sim_vec = key_strength * sim_vec\n",
    "        return self.softmax(sim_vec)\n",
    "     \n",
    "    def location_focus(self, g, gamma, old_weight, content_weight, shift_weights):\n",
    "        # Equation 7\n",
    "        gated_weight = g * content_weight + (1- g) * self.old_weight\n",
    "        \n",
    "        # Equation 8\n",
    "        w_bar = torch.zeros(self.M)\n",
    "        for i in range(self.M):\n",
    "            for j in range(self.N):\n",
    "                w_bar[i] += gated_weight[j] * shift_weights[i-j]\n",
    "        \n",
    "        # Equation 9\n",
    "        weights_to_power = torch.pow(x, gamma)\n",
    "        weights_power_sum = torch.sum(weights_to_power)\n",
    "        return torch.mul(weights_to_power, 1/weights_power_sum)\n",
    "    \n",
    "    def parse_lstm_output(output):\n",
    "        pass\n",
    "        # TODO - get needed read/write head info from the controller. We need:\n",
    "        \n",
    "        #key_vecs (rh x M), interpolation_gates (rh), gammas (rh), shift_weights (rh x M)\n",
    "                                 \n",
    "        #key_vecs (wh x M), interpolation_gates (wh), gammas (wh), shift_weights (wh x M), erase_vecs (wh x M), add_vecs(wh x M),\n",
    "        \n",
    "        # rh=read_heads; wh=write_heads\n",
    "    \n",
    "    def backProp():\n",
    "        pass\n",
    "        \n",
    "    def forward(self, input, label):\n",
    "        \n",
    "        # Concatenate inputs\n",
    "        read_vec = torch.cat(self.read_weights)\n",
    "        combined_input = torch.cat(input, read_vec)\n",
    "        \n",
    "        # Call controller, get output\n",
    "        output = lstm(combined_input)\n",
    "        \n",
    "        # Parse output\n",
    "        self.parse_lstm_output(output)\n",
    "\n",
    "        # Write\n",
    "        self.write_head()\n",
    "        self.read_head() # We should throw params in here.\n",
    "        \n",
    "        \n",
    "# Create train function down here\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class MyLSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyLSTMLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.forgetGate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.incorporatePositionGate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.incorporateValueGate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.hiddenValueGate = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward_step(self, input, hidden, cell_state):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        \n",
    "        f = self.sigmoid(self.forgetGate(combined))\n",
    "        i = self.sigmoid(self.incorporatePositionGate(combined))\n",
    "        C_new = self.tanh(self.incorporateValueGate(combined))\n",
    "        \n",
    "        cell_state = f * cell_state + i * C_new\n",
    "        \n",
    "        hidden = self.relu(self.hiddenValueGate(cell_state))\n",
    "        \n",
    "        return hidden, cell_state\n",
    "    \n",
    "    def forward(self, input):\n",
    "        hidden, cell_state = self.initAll()\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(input.size()[0]):\n",
    "            hidden, cell_state = self.forward_step(input[i], hidden, cell_state)\n",
    "            outputs.append(hidden)\n",
    "        \n",
    "        return torch.stack(outputs)\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size)).cuda()\n",
    "    \n",
    "    def initCellState(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size)).cuda()\n",
    "    \n",
    "    def initAll(self):\n",
    "        return self.initHidden(), self.initCellState()\n",
    "\n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, layers):\n",
    "        super(myLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        input_sizes = [input_size] + hidden_sizes[:-1] \n",
    "        \n",
    "        for input_size, hidden_size in zip(input_sizes, hidden_sizes):\n",
    "            self.lstm_layers.append(MyLSTMLayer(input_size, hidden_size))\n",
    "        \n",
    "        self.outputGate = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input):\n",
    "        hiddens = input\n",
    "        \n",
    "        for i, lstm_layer in enumerate(self.lstm_layers):\n",
    "            hiddens = lstm_layer(hiddens)\n",
    "        \n",
    "        return self.softmax(self.outputGate(hiddens[-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "   0   1   1   0   0   0   1   0   0\n",
      "   1   1   1   0   0   0   0   1   0\n",
      "   0   0   1   1   0   1   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0\n",
      "\n",
      "(1 ,.,.) = \n",
      "   0   0   1   1   1   1   1   0   0\n",
      "   0   0   0   0   1   0   1   0   0\n",
      "   0   0   1   1   0   0   1   1   0\n",
      "   1   1   1   1   0   1   0   1   0\n",
      "\n",
      "(2 ,.,.) = \n",
      "   1   1   1   1   0   0   0   1   0\n",
      "   0   0   0   0   1   1   0   1   0\n",
      "   1   0   0   0   0   1   1   1   0\n",
      "   0   0   0   1   1   0   0   0   0\n",
      "\n",
      "(3 ,.,.) = \n",
      "   1   1   0   1   1   0   0   1   0\n",
      "   1   1   1   1   0   0   0   1   0\n",
      "   0   0   0   0   0   0   1   0   0\n",
      "   1   1   1   0   1   1   1   0   0\n",
      "\n",
      "(4 ,.,.) = \n",
      "   1   0   1   0   1   0   1   0   0\n",
      "   0   1   1   1   1   1   1   0   0\n",
      "   0   1   0   0   1   0   1   0   0\n",
      "   1   1   1   0   0   0   1   1   0\n",
      "\n",
      "(5 ,.,.) = \n",
      "   1   0   0   1   0   0   0   1   0\n",
      "   0   0   0   0   1   0   1   1   0\n",
      "   0   0   0   0   1   1   0   1   0\n",
      "   1   0   0   0   0   1   1   0   0\n",
      "\n",
      "(6 ,.,.) = \n",
      "   1   0   0   0   1   0   1   1   0\n",
      "   0   1   1   1   0   1   1   1   0\n",
      "   0   0   0   1   1   0   0   0   0\n",
      "   0   0   0   1   1   1   0   0   0\n",
      "\n",
      "(7 ,.,.) = \n",
      "   1   1   0   1   0   1   0   0   0\n",
      "   1   0   1   0   1   0   0   1   0\n",
      "   0   0   0   0   1   1   0   1   0\n",
      "   1   1   1   1   1   0   0   0   0\n",
      "\n",
      "(8 ,.,.) = \n",
      "   1   0   0   1   1   1   0   1   1\n",
      "   1   1   0   1   0   0   1   1   1\n",
      "   0   1   1   1   0   1   0   0   1\n",
      "   1   1   0   1   1   0   1   1   1\n",
      "[torch.FloatTensor of size 9x4x9]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "   0   1   0   1   0   0   1   0   0\n",
      "   1   1   1   1   0   0   1   1   0\n",
      "   0   1   0   1   1   1   1   1   0\n",
      "   0   1   1   0   0   1   1   1   0\n",
      "\n",
      "(1 ,.,.) = \n",
      "   1   1   0   1   1   1   1   1   0\n",
      "   1   0   1   0   1   1   1   1   0\n",
      "   0   0   1   0   0   0   0   1   0\n",
      "   1   0   0   0   0   1   0   1   0\n",
      "\n",
      "(2 ,.,.) = \n",
      "   0   1   0   1   1   0   1   1   0\n",
      "   1   1   1   1   0   1   0   1   0\n",
      "   0   0   1   0   0   0   0   0   0\n",
      "   1   0   0   1   0   0   1   0   0\n",
      "\n",
      "(3 ,.,.) = \n",
      "   0   0   1   0   1   1   0   1   0\n",
      "   1   0   0   0   1   1   1   0   0\n",
      "   0   0   0   0   0   1   1   0   0\n",
      "   0   1   1   0   1   0   0   0   0\n",
      "\n",
      "(4 ,.,.) = \n",
      "   1   0   1   1   0   0   1   0   0\n",
      "   0   1   1   1   0   0   1   0   0\n",
      "   1   0   1   0   1   0   1   1   0\n",
      "   0   0   1   0   1   1   0   0   0\n",
      "\n",
      "(5 ,.,.) = \n",
      "   0   0   0   0   0   0   1   1   0\n",
      "   0   0   1   1   0   0   1   1   0\n",
      "   0   1   1   1   0   1   0   0   0\n",
      "   0   0   1   1   0   0   0   0   0\n",
      "\n",
      "(6 ,.,.) = \n",
      "   0   0   1   1   1   0   1   1   1\n",
      "   0   1   1   1   0   0   0   1   1\n",
      "   0   1   1   0   0   1   0   0   1\n",
      "   0   0   0   1   0   1   1   0   1\n",
      "[torch.FloatTensor of size 7x4x9]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "   1   0   1   1   1   1   0   1   0\n",
      "   0   0   0   1   0   0   0   1   0\n",
      "   1   0   0   1   1   0   0   1   0\n",
      "   0   0   1   1   0   0   1   0   0\n",
      "\n",
      "(1 ,.,.) = \n",
      "   1   0   1   1   1   0   0   1   0\n",
      "   1   0   1   0   0   1   0   0   0\n",
      "   1   0   0   0   0   1   1   0   0\n",
      "   0   1   1   0   1   0   1   0   0\n",
      "\n",
      "(2 ,.,.) = \n",
      "   0   1   0   1   0   0   1   0   0\n",
      "   1   0   0   0   0   0   1   1   0\n",
      "   1   1   0   1   0   1   0   1   0\n",
      "   1   0   0   0   1   1   0   0   0\n",
      "\n",
      "(3 ,.,.) = \n",
      "   1   1   1   0   1   0   1   1   0\n",
      "   0   1   0   1   1   0   1   0   0\n",
      "   0   0   0   1   0   1   0   0   0\n",
      "   1   1   0   1   0   1   0   0   0\n",
      "\n",
      "(4 ,.,.) = \n",
      "   1   0   1   0   0   1   0   0   1\n",
      "   0   1   0   1   1   0   1   1   1\n",
      "   0   0   0   1   0   1   0   0   1\n",
      "   1   0   0   1   1   1   0   1   1\n",
      "[torch.FloatTensor of size 5x4x9]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "   0   1   1   0   1   0   0   1   0\n",
      "   1   1   1   0   0   1   0   0   0\n",
      "   1   0   0   1   1   0   0   1   0\n",
      "   0   1   1   0   0   1   0   1   0\n",
      "\n",
      "(1 ,.,.) = \n",
      "   0   0   0   1   0   1   0   0   0\n",
      "   1   0   0   1   0   1   0   0   0\n",
      "   0   1   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "(2 ,.,.) = \n",
      "   0   1   0   1   1   0   0   1   0\n",
      "   1   1   1   1   0   0   0   0   0\n",
      "   0   0   1   0   1   1   1   1   0\n",
      "   1   1   1   0   0   0   1   1   0\n",
      "\n",
      "(3 ,.,.) = \n",
      "   1   0   0   1   1   1   0   0   1\n",
      "   1   1   0   1   0   0   0   1   1\n",
      "   1   0   0   0   0   1   1   0   1\n",
      "   1   1   1   0   1   0   0   1   1\n",
      "[torch.FloatTensor of size 4x4x9]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cat received an invalid combination of arguments - got (NoneType, Variable), but expected one of:\n * (sequence[torch.FloatTensor] seq)\n * (sequence[torch.FloatTensor] seq, int dim)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mNoneType\u001b[0m, \u001b[31;1mVariable\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-79cc5ff7620f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtrainAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-79cc5ff7620f>\u001b[0m in \u001b[0;36mtrainAll\u001b[0;34m(num_batches, batch_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Now, don't pass in any elements.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mntm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DONE!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-094b71852a67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, label)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Concatenate inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mread_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mcombined_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# Call controller, get output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cat received an invalid combination of arguments - got (NoneType, Variable), but expected one of:\n * (sequence[torch.FloatTensor] seq)\n * (sequence[torch.FloatTensor] seq, int dim)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mNoneType\u001b[0m, \u001b[31;1mVariable\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as d\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "class CopyTaskDataset(d.Dataset):\n",
    "    def __init__(self, batch_size, lower, upper, seq_size):\n",
    "        self.input_list = []\n",
    "        for _ in range(batch_size):\n",
    "            self.input_list.append(self.generate_batch(batch_size, lower, upper, seq_size))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "    # OK\n",
    "    def generate_batch(self, batch_size, lower, upper, seq_size):\n",
    "        seq_length = random.randint(lower, upper)\n",
    "        batch = torch.from_numpy(\n",
    "            numpy.random.binomial(1, 0.5, (seq_length, batch_size, seq_size)))\n",
    "        end_marker = torch.zeros(seq_length - 1, batch_size, 1)\n",
    "        end_row = torch.ones(1, batch_size, 1)\n",
    "        end_marker = torch.cat((end_marker, end_row), 0)\n",
    "        batch = torch.cat((batch.float(), end_marker), 2)\n",
    "        print(batch)\n",
    "        return batch \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.input_list[i//self.batch_size][:, i % self.batch_size, :]\n",
    "\n",
    "def trainAll(num_batches, batch_size):\n",
    "    ntm = NTM(1,1,1,1,1)\n",
    "    dataset = CopyTaskDataset(batch_size, 3, 10, 8)\n",
    "    \n",
    "    data_loader = d.DataLoader(dataset, batch_size = batch_size)\n",
    "    for _ in range(num_batches):\n",
    "        for batch in data_loader:\n",
    "            batch = batch.squeeze()\n",
    "            # Pass in one element of the sequence per time step\n",
    "            for time_step in range(len(batch)):\n",
    "                #ntm(batch[time_step], None)\n",
    "                pass\n",
    "                \n",
    "            # Now, don't pass in any elements.\n",
    "            for time_step in range(len(batch)):\n",
    "                ntm(None, batch[time_step][:,:-1])\n",
    "    print(\"DONE!\")\n",
    "\n",
    "    \n",
    "trainAll(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class EncapsulatedLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(EncapsulatedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(*args, **kwargs)\n",
    "        self.hidden_size = args[1]\n",
    "        self.num_layers = args[2]\n",
    "        \n",
    "        initial_hidden_state = nn.Parameter(torch.randn(self.num_layers, 1, self.hidden_size))\n",
    "        initial_cell_state = nn.Parameter(torch.randn(self.num_layers, 1, self.hidden_size))\n",
    "        self.state_tuple = (initial_hidden_state, initial_cell_state)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden_state = self.state_tuple[0].expand(self.num_layers, input.size()[1], self.hidden_size)\n",
    "        cell_state = self.state_tuple[1].expand(self.num_layers, input.size()[1], self.hidden_size)\n",
    "        output, self.state_tuple = self.lstm(input, (hidden_state, cell_state))\n",
    "        return output\n",
    "    \n",
    "# Test to make sure there aren't obvious errors\n",
    "myLstm = EncapsulatedLSTM(3, 2, 1)\n",
    "out_1 = myLstm(torch.autograd.Variable(torch.zeros(4, 3)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
