{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# A lot of inspiration from https://github.com/loudinthecloud/pytorch-ntm. Hyperparameters were chosen based\n",
    "# upon his experiments.\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "n_input = 8 # Copy task (pg 10, https://arxiv.org/pdf/1410.5401.pdf)\n",
    "n_hidden = 50 # Kinda random...\n",
    "n_output = 8 # Copy task\n",
    "\n",
    "def _split_cols(mat, lengths):\n",
    "    \"\"\"Split a 2D matrix to variable length columns.\"\"\"\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results += [mat[:, s:e]]\n",
    "    return results\n",
    "\n",
    "class NTM_Head(nn.Module):\n",
    "    def __init__(self, memory, controller_output_size):\n",
    "        super(NTM_Head, self).__init__()\n",
    "        \n",
    "        self.memory = memory\n",
    "        self.controller_output_size = controller_output_size\n",
    "        self.N, self.M = memory.size()\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def address_memory(self, key_vec, β, g, s, γ):\n",
    "        result = F.cosine_similarity(key_vec.unsqueeze(0).expand(self.N, -1), self.memory, dim = 0)\n",
    "        result = β * result\n",
    "        result = result.exp() / result.sum()\n",
    "        result = g * result + (1 - g) * self.prev_address_vector\n",
    "        result = F.conv1d(torch.cat((result[:, -1], result, result[:, 1]), 1).view(-1, 1, self.N), \n",
    "                          s.view(1, 1, -1))\n",
    "        result = result ** γ\n",
    "        result = result / result.sum()\n",
    "        return result\n",
    "\n",
    "class NTM_Read_Head(NTM_Head):\n",
    "    def __init__(self, memory, controller_output_size):\n",
    "        super(NTM_Read_Head).__init__(memory, controller_output_size)\n",
    "        \n",
    "        self.read_parameters_lengths = [self.N, 1, 1, 3, 1]\n",
    "        self.fc_read_parameters = nn.Linear(controller_output_size, sum(read_parameters_lengths))\n",
    "        \n",
    "        initialize_parameters()\n",
    "        initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_read_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_read_parameters.bias, std=0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.initial_address_vector = nn.Parameter(torch.zeros(self.N))\n",
    "        self.initial_read = nn.Parameter(torch.zeros(self.M))\n",
    "        \n",
    "        self.prev_address_vector = Variable(self.initial_address_vector)\n",
    "        self.prev_read = Variable(self.initial_read)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        read_parameters = self.fc_read_parameters(x)\n",
    "        \n",
    "        key_vec, β, g, s, γ = _split_cols(read_parameters, self.read_parameters_lengths)                                        \n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "                                               \n",
    "        address_vec = address_memory(key_vec, β, g, s, γ)\n",
    "        new_read = self.M.transpose() * address_vec\n",
    "        self.prev_read = new_read\n",
    "        return new_read\n",
    "\n",
    "class NTM_Write_Head(NTM_Head):\n",
    "    def __init__(self, memory, controller_output_size):\n",
    "        super(NTM_Write_Head).__init__(memory, controller_output_size)\n",
    "        \n",
    "        self.write_parameters_lengths = [self.N, 1, 1, 3, 1, self.M, self.M]\n",
    "        self.fc_write_parameters = nn.Linear(controller_output_size, sum(write_parameters_size))\n",
    "        \n",
    "        reset_parameters()\n",
    "        initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_write_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_write_parameters.bias, std=0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.initial_address_vector = nn.Parameter(torch.zeros(self.N))       \n",
    "        self.prev_address_vector = Variable(self.initial_address_vector)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        write_parameters = self.fc_write_parameters(x)\n",
    "                                       \n",
    "        key_vec, β, g, s, γ, erase_vec, add_vec = _split_cols(write_parameters, self.write_parameters_size)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "        erase_vec = F.sigmoid(erase_vec)\n",
    "                                               \n",
    "        address_vec = address_memory(key_vec, β, g, s, γ)\n",
    "        self.M *= 1 - torch.bmm(address_vec.unsqueeze(2), erase_vec.unsquueze(1))\n",
    "        self.M += torch.bmm(address_vec.unsqueeze(2), add_vec.unsquueze(1))\n",
    "\n",
    "class NTM(nn.Module):\n",
    "    def __init__(self, controller, controller_output_size, output_size, \n",
    "                 address_count, address_dimension, heads):\n",
    "        super(NTM, self).__init__()\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = nn.Parameter(torch.zeros(address_count, address_dimension))\n",
    "        \n",
    "        # Construct the heads.\n",
    "        self.heads = nn.ModuleList()\n",
    "        \n",
    "        for head_id in heads:\n",
    "            if head_id = 0:\n",
    "                self.heads.append(NTM_Read_Head(memory, controller_output_size))\n",
    "            else:\n",
    "                self.heads.append(NTM_Write_Head(memory, controller_output_size))\n",
    "        \n",
    "        # Initialize controller\n",
    "        self.controller = controller\n",
    "        self.outputGate = nn.Linear(controller_output_size, output_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        initialize_state()\n",
    "        reset_parameters()\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.prev_reads = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head.prev_read)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdev = 1 / (np.sqrt(N + M))\n",
    "        nn.init.uniform(self.memory, -stdev, stdev)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        controller_output = controller(torch.cat(self.prev_reads.append(x), dim=1))\n",
    "        self.prev_reads = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head(controller_output))\n",
    "            else:\n",
    "                head(controller_output)\n",
    "        \n",
    "        return self.softmax(self.outputGate(controller_output))\n",
    "    \n",
    "# Create train function down here\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
