{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mehdi2277/Documents/HarveyMuddWork/Neural_Nets_Research/neural_nets_research\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets_library import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of inspiration from https://github.com/loudinthecloud/pytorch-ntm. Hyperparameters were chosen based\n",
    "# upon his experiments.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seed(seed=None):\n",
    "    \"\"\"Seed the RNGs for predicatability/reproduction purposes.\"\"\"\n",
    "    if seed is None:\n",
    "        seed = int(get_ms() // 1000)\n",
    "\n",
    "    print(\"Using seed=%d\", seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Memory(nn.Module):\n",
    "    def __init__(self, address_count, address_dimension, batch_size):\n",
    "        super(NTM_Memory, self).__init__()\n",
    "        self.initial_memory = nn.Parameter(torch.zeros(1, address_count, address_dimension))\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        _, N, M = self.initial_memory.size()\n",
    "        stdev = 1 / np.sqrt(N + M)\n",
    "        nn.init.uniform(self.initial_memory, -stdev, stdev)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.memory = self.initial_memory.repeat(self.batch_size, 1, 1)\n",
    "    \n",
    "    def address_memory(self, key_vec, prev_address_vec, β, g, s, γ):\n",
    "        EPSILON = 1e-16\n",
    "        result = F.cosine_similarity((key_vec+EPSILON).unsqueeze(1).expand_as(self.memory), \n",
    "                                     self.memory+EPSILON, dim = 2)\n",
    "        result = F.softmax(β * result, dim=1)\n",
    "        result = g * result + (1 - g) * prev_address_vec\n",
    "        result = torch.cat((result[:, 1:], result[:, :1]), 1) * s[:, 0:1] + result * s[:, 1:2] + \\\n",
    "                 torch.cat((result[:, -1:], result[:, :-1]), 1) * s[:, 2:3]\n",
    "\n",
    "#         result = result ** γ\n",
    "#         result = result / (result.sum(1, keepdim=True) + EPSILON)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def read_memory(self, address_vec):\n",
    "        return torch.bmm(self.memory.transpose(1,2), address_vec.unsqueeze(2)).squeeze(2)\n",
    "    \n",
    "    def update_memory(self, address_vec, erase_vec, add_vec):\n",
    "        self.memory = self.memory * (1 - torch.bmm(address_vec.unsqueeze(2), erase_vec.unsqueeze(1)))\n",
    "        self.memory += torch.bmm(address_vec.unsqueeze(2), add_vec.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_cols(mat, lengths):\n",
    "    \"\"\"Split a 2D matrix to variable length columns.\"\"\"\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results += [mat[:, s:e]]\n",
    "    return results\n",
    "\n",
    "class NTM_Head(nn.Module):\n",
    "    def __init__(self, address_count, address_dimension, \n",
    "                 controller_output_size):\n",
    "        super(NTM_Head, self).__init__()\n",
    "        \n",
    "        self.controller_output_size = controller_output_size\n",
    "        self.N = address_count\n",
    "        self.M = address_dimension\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Read_Head(NTM_Head):\n",
    "    def __init__(self, address_count, address_dimension, controller_output_size, batch_size):\n",
    "        super(NTM_Read_Head, self).__init__(address_count, address_dimension, controller_output_size)\n",
    "        \n",
    "        self.read_parameters_lengths = [self.M, 1, 1, 3, 1]\n",
    "        self.fc_read_parameters = nn.Linear(controller_output_size, sum(self.read_parameters_lengths))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_read_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_read_parameters.bias, std=0.01)\n",
    "        \n",
    "        self.initial_address_vec = nn.Parameter(torch.zeros(self.N))\n",
    "        self.initial_read = nn.Parameter(torch.randn(1, self.M) * 0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.prev_address_vec = self.initial_address_vec.clone()\n",
    "        self.prev_read = self.initial_read.repeat(self.batch_size, 1)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        read_parameters = self.fc_read_parameters(x)\n",
    "        \n",
    "        key_vec, β, g, s, γ = _split_cols(read_parameters, self.read_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "        \n",
    "        self.prev_address_vec = memory.address_memory(key_vec, self.prev_address_vec, β, g, s, γ)\n",
    "        new_read = memory.read_memory(self.prev_address_vec)        \n",
    "        self.prev_read = new_read\n",
    "        return new_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Write_Head(NTM_Head):\n",
    "    def __init__(self, address_count, address_dimension, controller_output_size):\n",
    "        super(NTM_Write_Head, self).__init__(address_count, address_dimension, controller_output_size)\n",
    "        \n",
    "        self.write_parameters_lengths = [self.M, 1, 1, 3, 1, self.M, self.M]\n",
    "        self.fc_write_parameters = nn.Linear(controller_output_size, sum(self.write_parameters_lengths))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_write_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_write_parameters.bias, std=0.01)\n",
    "        \n",
    "        self.initial_address_vec = nn.Parameter(torch.zeros(self.N))\n",
    "    \n",
    "    def initialize_state(self):       \n",
    "        self.prev_address_vec = self.initial_address_vec.clone()\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return False\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        write_parameters = self.fc_write_parameters(x)\n",
    "                                       \n",
    "        key_vec, β, g, s, γ, erase_vec, add_vec = _split_cols(write_parameters, self.write_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "        erase_vec = F.sigmoid(erase_vec)\n",
    "                                               \n",
    "        self.prev_address_vec = memory.address_memory(key_vec, self.prev_address_vec, β, g, s, γ)\n",
    "        memory.update_memory(self.prev_address_vec, erase_vec, add_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, batch_size, controller, controller_output_size, \n",
    "                 output_size, address_count, address_dimension, heads):\n",
    "        super(NTM, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize controller\n",
    "        self.controller = controller\n",
    "        \n",
    "        # Create output gate. No activation function is used with it because\n",
    "        # I used BCEWithLogitsLoss which deals with the sigmoid in a more\n",
    "        # numerically stable manner.\n",
    "        self.outputGate = nn.Linear(controller_output_size, output_size)\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = NTM_Memory(address_count, address_dimension, batch_size)\n",
    "\n",
    "        # Construct the heads.\n",
    "        self.heads = nn.ModuleList()\n",
    "        \n",
    "        for head_id in heads:\n",
    "            if head_id == 0:\n",
    "                self.heads.append(NTM_Read_Head(address_count, address_dimension, \n",
    "                                                controller_output_size, batch_size))\n",
    "            else:\n",
    "                self.heads.append(NTM_Write_Head(address_count, address_dimension,\n",
    "                                                 controller_output_size))\n",
    "        self.initialize_state()\n",
    "        \n",
    "    def initialize_state(self):\n",
    "        self.prev_reads = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            head.initialize_state()\n",
    "            \n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head.prev_read)\n",
    "        \n",
    "        self.memory.initialize_state()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.outputGate.weight)\n",
    "        nn.init.normal(self.outputGate.bias, std=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.initialize_state()\n",
    "        outputs = []\n",
    "        \n",
    "        for current_observation in x.transpose(0,1):\n",
    "            self.prev_reads.append(current_observation)\n",
    "            controller_input = torch.cat(self.prev_reads, 1)\n",
    "            controller_output = self.controller(controller_input)\n",
    "\n",
    "            self.prev_reads = []\n",
    "\n",
    "            for head in self.heads:                \n",
    "                if head.is_read_head():\n",
    "                    self.prev_reads.append(head(controller_output, self.memory))\n",
    "                else:\n",
    "                    head(controller_output, self.memory)\n",
    "                    \n",
    "            current_output = self.outputGate(controller_output)\n",
    "            outputs.append(current_output)\n",
    "        \n",
    "        return torch.stack(outputs).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyTaskDataset(data.Dataset):\n",
    "    def __init__(self, num_batches, batch_size, lower, upper, seq_size):\n",
    "        self.input_list = []\n",
    "        self.label_list = []\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            data, label = self.generate_batch(batch_size, lower, upper, seq_size)\n",
    "            self.input_list.append(data)\n",
    "            self.label_list.append(label)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batch(self, batch_size, lower, upper, seq_size):\n",
    "        seq_length = random.randint(lower, upper)\n",
    "        label = torch.from_numpy(\n",
    "                np.random.binomial(1, 0.5, (seq_length, batch_size, seq_size))).float()\n",
    "        end_marker = torch.zeros(seq_length, batch_size, 1)\n",
    "        seq = torch.cat((label, end_marker), 2)\n",
    "        delimiter_column = torch.zeros(1, batch_size, seq_size+1)\n",
    "        delimiter_column[0, :, seq_size] = 1\n",
    "        seq = torch.cat((seq, delimiter_column), 0)\n",
    "        output_time = torch.zeros(seq_length, batch_size, seq_size+1)\n",
    "        seq = torch.cat((seq, output_time), 0)\n",
    "        return seq, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)*self.batch_size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        batch_index = i//self.batch_size\n",
    "        index_in_batch = i % self.batch_size\n",
    "        return self.input_list[batch_index][:, index_in_batch, :], self.label_list[batch_index][:, index_in_batch, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncapsulatedLSTM(nn.Module):\n",
    "    def __init__(self, batch_size, all_hiddens, *args, **kwargs):\n",
    "        super(EncapsulatedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(*args, **kwargs)\n",
    "        self.all_hiddens = all_hiddens\n",
    "        self.batch_size = batch_size\n",
    "                \n",
    "        self.num_inputs = args[0]\n",
    "        self.hidden_size = args[1]\n",
    "        self.num_layers = args[2]\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "          \n",
    "    def initialize_state(self):\n",
    "        self.state_tuple = (self.initial_hidden_state.repeat(1, self.batch_size, 1), \n",
    "                            self.initial_cell_state.repeat(1, self.batch_size, 1))\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.initial_hidden_state = nn.Parameter(torch.randn(self.num_layers, 1, self.hidden_size) * 0.05)\n",
    "        self.initial_cell_state = nn.Parameter(torch.randn(self.num_layers, 1, self.hidden_size) * 0.05)\n",
    "        \n",
    "        for p in self.lstm.parameters():\n",
    "            if p.dim() == 1:\n",
    "                nn.init.constant(p, 0)\n",
    "            else:\n",
    "                stdev = 5 / (np.sqrt(self.num_inputs +  self.hidden_size))\n",
    "                nn.init.uniform(p, -stdev, stdev)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.initialize_state()\n",
    "        output, self.state_tuple = self.lstm(input.unsqueeze(0), self.state_tuple)\n",
    "        \n",
    "        if self.all_hiddens:\n",
    "            return self.state_tuple[0].transpose(0,1).view(self.batch_size, -1)\n",
    "        else:\n",
    "            return output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_task_loss(output, label):\n",
    "    _, seq_length, _ = label.size()\n",
    "    return F.binary_cross_entropy_with_logits(output[:, -seq_length:, :], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_bits_per_sequence(output, label):\n",
    "    batch_size, seq_length, _ = label.size()\n",
    "    binarized_output = output[:, -seq_length:, :].sign()/2 + 0.5\n",
    "    \n",
    "    # The cost is the number of error bits per sequence\n",
    "    return torch.sum(torch.abs(binarized_output - label))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_clipped_optimizer(optimizer_type):\n",
    "#     class ClippedOptimizer(optimizer_type):\n",
    "#         def step(self, closure=None):\n",
    "#             for group in self.param_groups:\n",
    "#                 for p in group['params']:\n",
    "#                     if p.grad is not None:\n",
    "#                         p.grad = p.grad.data.clamp(-10,10)\n",
    "            \n",
    "#             super().step(closure)\n",
    "    \n",
    "#     return ClippedOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClippedRMSprop = construct_clipped_optimizer(optim.RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hidden_size = 100\n",
    "num_layers = 3\n",
    "seq_size = 8\n",
    "address_size = 20\n",
    "controller = EncapsulatedLSTM(batch_size, False, # all hiddens\n",
    "                              seq_size + address_size + 1, hidden_size, \n",
    "                              num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_count = 128\n",
    "controller_output_size = hidden_size\n",
    "\n",
    "ntm = NTM(batch_size, controller, controller_output_size, \n",
    "          seq_size, address_count, address_size, [0, 1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm = ntm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_seq_length = 3\n",
    "upper_seq_length = 10\n",
    "num_batches = 10000\n",
    "\n",
    "dataset = CopyTaskDataset(num_batches, batch_size, lower_seq_length, upper_seq_length, seq_size)\n",
    "data_loader = data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(ntm.parameters(), momentum=0.9,\n",
    "                          alpha=0.95, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "Epoch Number: 0, Batch Number: 25, Training Loss: 0.6938, Validation Loss: 24.5575\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 0, Batch Number: 50, Training Loss: 0.6822, Validation Loss: 23.3525\n",
      "Time so far is 0m 9s\n",
      "Epoch Number: 0, Batch Number: 75, Training Loss: 0.6435, Validation Loss: 15.4519\n",
      "Time so far is 0m 13s\n",
      "Epoch Number: 0, Batch Number: 100, Training Loss: 0.6426, Validation Loss: 19.9481\n",
      "Time so far is 0m 17s\n",
      "Epoch Number: 0, Batch Number: 125, Training Loss: 0.6264, Validation Loss: 19.1938\n",
      "Time so far is 0m 22s\n",
      "Epoch Number: 0, Batch Number: 150, Training Loss: 0.6099, Validation Loss: 17.3738\n",
      "Time so far is 0m 27s\n",
      "Epoch Number: 0, Batch Number: 175, Training Loss: 0.6079, Validation Loss: 18.1269\n",
      "Time so far is 0m 30s\n",
      "Epoch Number: 0, Batch Number: 200, Training Loss: 0.6080, Validation Loss: 17.5956\n",
      "Time so far is 0m 34s\n",
      "Epoch Number: 0, Batch Number: 225, Training Loss: 0.6169, Validation Loss: 20.2437\n",
      "Time so far is 0m 38s\n",
      "Epoch Number: 0, Batch Number: 250, Training Loss: 0.5967, Validation Loss: 16.3806\n",
      "Time so far is 0m 41s\n",
      "Epoch Number: 0, Batch Number: 275, Training Loss: 0.5973, Validation Loss: 17.6969\n",
      "Time so far is 0m 46s\n",
      "Epoch Number: 0, Batch Number: 300, Training Loss: 0.4966, Validation Loss: 12.5000\n",
      "Time so far is 0m 50s\n",
      "Epoch Number: 0, Batch Number: 325, Training Loss: 0.2378, Validation Loss: 1.4263\n",
      "Time so far is 0m 56s\n",
      "Epoch Number: 0, Batch Number: 350, Training Loss: 0.1213, Validation Loss: 0.0581\n",
      "Time so far is 1m 0s\n",
      "Epoch Number: 0, Batch Number: 375, Training Loss: 0.0760, Validation Loss: 0.0006\n",
      "Time so far is 1m 5s\n",
      "Epoch Number: 0, Batch Number: 400, Training Loss: 0.0527, Validation Loss: 0.0000\n",
      "Time so far is 1m 9s\n",
      "Epoch Number: 0, Batch Number: 425, Training Loss: 0.0369, Validation Loss: 0.0000\n",
      "Time so far is 1m 13s\n",
      "Epoch Number: 0, Batch Number: 450, Training Loss: 0.0259, Validation Loss: 0.0000\n",
      "Time so far is 1m 17s\n",
      "Epoch Number: 0, Batch Number: 475, Training Loss: 0.0179, Validation Loss: 0.0000\n",
      "Time so far is 1m 20s\n",
      "Epoch Number: 0, Batch Number: 500, Training Loss: 0.0125, Validation Loss: 0.0000\n",
      "Time so far is 1m 22s\n",
      "Epoch Number: 0, Batch Number: 525, Training Loss: 0.0088, Validation Loss: 0.0000\n",
      "Time so far is 1m 24s\n",
      "Epoch Number: 0, Batch Number: 550, Training Loss: 0.0061, Validation Loss: 0.0000\n",
      "Time so far is 1m 28s\n",
      "Epoch Number: 0, Batch Number: 575, Training Loss: 0.0042, Validation Loss: 0.0000\n",
      "Time so far is 1m 33s\n",
      "Epoch Number: 0, Batch Number: 600, Training Loss: 0.0030, Validation Loss: 0.0000\n",
      "Time so far is 1m 37s\n",
      "Epoch Number: 0, Batch Number: 625, Training Loss: 0.0021, Validation Loss: 0.0000\n",
      "Time so far is 1m 42s\n",
      "Epoch Number: 0, Batch Number: 650, Training Loss: 0.0014, Validation Loss: 0.0000\n",
      "Time so far is 1m 47s\n",
      "Epoch Number: 0, Batch Number: 675, Training Loss: 0.0010, Validation Loss: 0.0000\n",
      "Time so far is 1m 52s\n",
      "Epoch Number: 0, Batch Number: 700, Training Loss: 0.0007, Validation Loss: 0.0000\n",
      "Time so far is 1m 57s\n",
      "Epoch Number: 0, Batch Number: 725, Training Loss: 0.0005, Validation Loss: 0.0000\n",
      "Time so far is 2m 2s\n",
      "Epoch Number: 0, Batch Number: 750, Training Loss: 0.0003, Validation Loss: 0.0000\n",
      "Time so far is 2m 7s\n",
      "Epoch Number: 0, Batch Number: 775, Training Loss: 0.0002, Validation Loss: 0.0000\n",
      "Time so far is 2m 13s\n",
      "Epoch Number: 0, Batch Number: 800, Training Loss: 0.0002, Validation Loss: 0.0000\n",
      "Time so far is 2m 18s\n",
      "Epoch Number: 0, Batch Number: 825, Training Loss: 0.0001, Validation Loss: 0.0000\n",
      "Time so far is 2m 23s\n",
      "Epoch Number: 0, Batch Number: 850, Training Loss: 0.0001, Validation Loss: 0.0000\n",
      "Time so far is 2m 28s\n",
      "Epoch Number: 0, Batch Number: 875, Training Loss: 0.0001, Validation Loss: 0.0000\n",
      "Time so far is 2m 33s\n",
      "Epoch Number: 0, Batch Number: 900, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 2m 37s\n",
      "Epoch Number: 0, Batch Number: 925, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 2m 43s\n",
      "Epoch Number: 0, Batch Number: 950, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 2m 48s\n",
      "Epoch Number: 0, Batch Number: 975, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 2m 53s\n",
      "Epoch Number: 0, Batch Number: 1000, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 2m 59s\n",
      "Epoch Number: 0, Batch Number: 1025, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 3m 4s\n",
      "Epoch Number: 0, Batch Number: 1050, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 3m 9s\n",
      "Epoch Number: 0, Batch Number: 1075, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 3m 14s\n",
      "Epoch Number: 0, Batch Number: 1100, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 3m 19s\n",
      "Epoch Number: 0, Batch Number: 1125, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 3m 24s\n",
      "Epoch Number: 0, Batch Number: 1150, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 3m 29s\n",
      "Epoch Number: 0, Batch Number: 1175, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 3m 34s\n",
      "Epoch Number: 0, Batch Number: 1200, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 3m 40s\n",
      "Epoch Number: 0, Batch Number: 1225, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 3m 46s\n",
      "Epoch Number: 0, Batch Number: 1250, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 3m 51s\n",
      "Epoch Number: 0, Batch Number: 1275, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 3m 56s\n",
      "Epoch Number: 0, Batch Number: 1300, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 4m 2s\n",
      "Epoch Number: 0, Batch Number: 1325, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 4m 7s\n",
      "Epoch Number: 0, Batch Number: 1350, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 4m 13s\n",
      "Epoch Number: 0, Batch Number: 1375, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 4m 18s\n",
      "Epoch Number: 0, Batch Number: 1400, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 4m 23s\n",
      "Epoch Number: 0, Batch Number: 1425, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 4m 28s\n",
      "Epoch Number: 0, Batch Number: 1450, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 4m 33s\n",
      "Epoch Number: 0, Batch Number: 1475, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 4m 39s\n",
      "Epoch Number: 0, Batch Number: 1500, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 4m 44s\n",
      "Epoch Number: 0, Batch Number: 1525, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 4m 49s\n",
      "Epoch Number: 0, Batch Number: 1550, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 4m 54s\n",
      "Epoch Number: 0, Batch Number: 1575, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 4m 59s\n",
      "Epoch Number: 0, Batch Number: 1600, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 4s\n",
      "Epoch Number: 0, Batch Number: 1625, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 9s\n",
      "Epoch Number: 0, Batch Number: 1650, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 14s\n",
      "Epoch Number: 0, Batch Number: 1675, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 19s\n",
      "Epoch Number: 0, Batch Number: 1700, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 24s\n",
      "Epoch Number: 0, Batch Number: 1725, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 29s\n",
      "Epoch Number: 0, Batch Number: 1750, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 35s\n",
      "Epoch Number: 0, Batch Number: 1775, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 39s\n",
      "Epoch Number: 0, Batch Number: 1800, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 44s\n",
      "Epoch Number: 0, Batch Number: 1825, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 49s\n",
      "Epoch Number: 0, Batch Number: 1850, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 53s\n",
      "Epoch Number: 0, Batch Number: 1875, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 56s\n",
      "Epoch Number: 0, Batch Number: 1900, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 5m 59s\n",
      "Epoch Number: 0, Batch Number: 1925, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 3s\n",
      "Epoch Number: 0, Batch Number: 1950, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0, Batch Number: 1975, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 9s\n",
      "Epoch Number: 0, Batch Number: 2000, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 12s\n",
      "Epoch Number: 0, Batch Number: 2025, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 15s\n",
      "Epoch Number: 0, Batch Number: 2050, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 21s\n",
      "Epoch Number: 0, Batch Number: 2075, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 26s\n",
      "Epoch Number: 0, Batch Number: 2100, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 31s\n",
      "Epoch Number: 0, Batch Number: 2125, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 37s\n",
      "Epoch Number: 0, Batch Number: 2150, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 42s\n",
      "Epoch Number: 0, Batch Number: 2175, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 46s\n",
      "Epoch Number: 0, Batch Number: 2200, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 52s\n",
      "Epoch Number: 0, Batch Number: 2225, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 6m 58s\n",
      "Epoch Number: 0, Batch Number: 2250, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 7m 3s\n",
      "Epoch Number: 0, Batch Number: 2275, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 7m 9s\n",
      "Epoch Number: 0, Batch Number: 2300, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 7m 14s\n",
      "Epoch Number: 0, Batch Number: 2325, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 7m 19s\n",
      "Epoch Number: 0, Batch Number: 2350, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 7m 24s\n",
      "Epoch Number: 0, Batch Number: 2375, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 7m 30s\n",
      "Epoch Number: 0, Batch Number: 2400, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 7m 35s\n",
      "Epoch Number: 0, Batch Number: 2425, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 7m 40s\n",
      "Epoch Number: 0, Batch Number: 2450, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 7m 45s\n",
      "Epoch Number: 0, Batch Number: 2475, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 7m 50s\n",
      "Epoch Number: 0, Batch Number: 2500, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 7m 55s\n",
      "Epoch Number: 0, Batch Number: 2525, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 8m 0s\n",
      "Epoch Number: 0, Batch Number: 2550, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 8m 5s\n",
      "Epoch Number: 0, Batch Number: 2575, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 8m 10s\n",
      "Epoch Number: 0, Batch Number: 2600, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 8m 15s\n",
      "Epoch Number: 0, Batch Number: 2625, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 8m 20s\n",
      "Epoch Number: 0, Batch Number: 2650, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 8m 25s\n",
      "Epoch Number: 0, Batch Number: 2675, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 8m 31s\n",
      "Epoch Number: 0, Batch Number: 2700, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 8m 36s\n",
      "Epoch Number: 0, Batch Number: 2725, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 8m 41s\n",
      "Epoch Number: 0, Batch Number: 2750, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 8m 47s\n",
      "Epoch Number: 0, Batch Number: 2775, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 8m 52s\n",
      "Epoch Number: 0, Batch Number: 2800, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 8m 57s\n",
      "Epoch Number: 0, Batch Number: 2825, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 2s\n",
      "Epoch Number: 0, Batch Number: 2850, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 7s\n",
      "Epoch Number: 0, Batch Number: 2875, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 12s\n",
      "Epoch Number: 0, Batch Number: 2900, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 18s\n",
      "Epoch Number: 0, Batch Number: 2925, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 23s\n",
      "Epoch Number: 0, Batch Number: 2950, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 29s\n",
      "Epoch Number: 0, Batch Number: 2975, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 34s\n",
      "Epoch Number: 0, Batch Number: 3000, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 39s\n",
      "Epoch Number: 0, Batch Number: 3025, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 44s\n",
      "Epoch Number: 0, Batch Number: 3050, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 47s\n",
      "Epoch Number: 0, Batch Number: 3075, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 49s\n",
      "Epoch Number: 0, Batch Number: 3100, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 52s\n",
      "Epoch Number: 0, Batch Number: 3125, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 54s\n",
      "Epoch Number: 0, Batch Number: 3150, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 57s\n",
      "Epoch Number: 0, Batch Number: 3175, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 9m 59s\n",
      "Epoch Number: 0, Batch Number: 3200, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 1s\n",
      "Epoch Number: 0, Batch Number: 3225, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 4s\n",
      "Epoch Number: 0, Batch Number: 3250, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 6s\n",
      "Epoch Number: 0, Batch Number: 3275, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 9s\n",
      "Epoch Number: 0, Batch Number: 3300, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 11s\n",
      "Epoch Number: 0, Batch Number: 3325, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 13s\n",
      "Epoch Number: 0, Batch Number: 3350, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 16s\n",
      "Epoch Number: 0, Batch Number: 3375, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 19s\n",
      "Epoch Number: 0, Batch Number: 3400, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 24s\n",
      "Epoch Number: 0, Batch Number: 3425, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 29s\n",
      "Epoch Number: 0, Batch Number: 3450, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 34s\n",
      "Epoch Number: 0, Batch Number: 3475, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 40s\n",
      "Epoch Number: 0, Batch Number: 3500, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 45s\n",
      "Epoch Number: 0, Batch Number: 3525, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 50s\n",
      "Epoch Number: 0, Batch Number: 3550, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 10m 56s\n",
      "Epoch Number: 0, Batch Number: 3575, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 11m 0s\n",
      "Epoch Number: 0, Batch Number: 3600, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 11m 6s\n",
      "Epoch Number: 0, Batch Number: 3625, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 11m 11s\n",
      "Epoch Number: 0, Batch Number: 3650, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 11m 16s\n",
      "Epoch Number: 0, Batch Number: 3675, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 11m 22s\n",
      "Epoch Number: 0, Batch Number: 3700, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 11m 27s\n",
      "Epoch Number: 0, Batch Number: 3725, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 11m 32s\n",
      "Epoch Number: 0, Batch Number: 3750, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 11m 37s\n",
      "Epoch Number: 0, Batch Number: 3775, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 11m 42s\n",
      "Epoch Number: 0, Batch Number: 3800, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 11m 48s\n",
      "Epoch Number: 0, Batch Number: 3825, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 11m 53s\n",
      "Epoch Number: 0, Batch Number: 3850, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 11m 59s\n",
      "Epoch Number: 0, Batch Number: 3875, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 3s\n",
      "Epoch Number: 0, Batch Number: 3900, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0, Batch Number: 3925, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 11s\n",
      "Epoch Number: 0, Batch Number: 3950, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 15s\n",
      "Epoch Number: 0, Batch Number: 3975, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 19s\n",
      "Epoch Number: 0, Batch Number: 4000, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 23s\n",
      "Epoch Number: 0, Batch Number: 4025, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 26s\n",
      "Epoch Number: 0, Batch Number: 4050, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 30s\n",
      "Epoch Number: 0, Batch Number: 4075, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 34s\n",
      "Epoch Number: 0, Batch Number: 4100, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 37s\n",
      "Epoch Number: 0, Batch Number: 4125, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 41s\n",
      "Epoch Number: 0, Batch Number: 4150, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 45s\n",
      "Epoch Number: 0, Batch Number: 4175, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 49s\n",
      "Epoch Number: 0, Batch Number: 4200, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 53s\n",
      "Epoch Number: 0, Batch Number: 4225, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 56s\n",
      "Epoch Number: 0, Batch Number: 4250, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 12m 59s\n",
      "Epoch Number: 0, Batch Number: 4275, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 2s\n",
      "Epoch Number: 0, Batch Number: 4300, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 4s\n",
      "Epoch Number: 0, Batch Number: 4325, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 7s\n",
      "Epoch Number: 0, Batch Number: 4350, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 9s\n",
      "Epoch Number: 0, Batch Number: 4375, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 12s\n",
      "Epoch Number: 0, Batch Number: 4400, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 14s\n",
      "Epoch Number: 0, Batch Number: 4425, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 16s\n",
      "Epoch Number: 0, Batch Number: 4450, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 19s\n",
      "Epoch Number: 0, Batch Number: 4475, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 21s\n",
      "Epoch Number: 0, Batch Number: 4500, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 26s\n",
      "Epoch Number: 0, Batch Number: 4525, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 29s\n",
      "Epoch Number: 0, Batch Number: 4550, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 34s\n",
      "Epoch Number: 0, Batch Number: 4575, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 37s\n",
      "Epoch Number: 0, Batch Number: 4600, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 42s\n",
      "Epoch Number: 0, Batch Number: 4625, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 47s\n",
      "Epoch Number: 0, Batch Number: 4650, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 52s\n",
      "Epoch Number: 0, Batch Number: 4675, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 13m 56s\n",
      "Epoch Number: 0, Batch Number: 4700, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 1s\n",
      "Epoch Number: 0, Batch Number: 4725, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 6s\n",
      "Epoch Number: 0, Batch Number: 4750, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 10s\n",
      "Epoch Number: 0, Batch Number: 4775, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 14s\n",
      "Epoch Number: 0, Batch Number: 4800, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 18s\n",
      "Epoch Number: 0, Batch Number: 4825, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 22s\n",
      "Epoch Number: 0, Batch Number: 4850, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 27s\n",
      "Epoch Number: 0, Batch Number: 4875, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 30s\n",
      "Epoch Number: 0, Batch Number: 4900, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 35s\n",
      "Epoch Number: 0, Batch Number: 4925, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 39s\n",
      "Epoch Number: 0, Batch Number: 4950, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 43s\n",
      "Epoch Number: 0, Batch Number: 4975, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 48s\n",
      "Epoch Number: 0, Batch Number: 5000, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 52s\n",
      "Epoch Number: 0, Batch Number: 5025, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 55s\n",
      "Epoch Number: 0, Batch Number: 5050, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 14m 59s\n",
      "Epoch Number: 0, Batch Number: 5075, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 4s\n",
      "Epoch Number: 0, Batch Number: 5100, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 8s\n",
      "Epoch Number: 0, Batch Number: 5125, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 11s\n",
      "Epoch Number: 0, Batch Number: 5150, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 14s\n",
      "Epoch Number: 0, Batch Number: 5175, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 17s\n",
      "Epoch Number: 0, Batch Number: 5200, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 20s\n",
      "Epoch Number: 0, Batch Number: 5225, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 22s\n",
      "Epoch Number: 0, Batch Number: 5250, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 26s\n",
      "Epoch Number: 0, Batch Number: 5275, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 30s\n",
      "Epoch Number: 0, Batch Number: 5300, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 34s\n",
      "Epoch Number: 0, Batch Number: 5325, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 38s\n",
      "Epoch Number: 0, Batch Number: 5350, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 42s\n",
      "Epoch Number: 0, Batch Number: 5375, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 46s\n",
      "Epoch Number: 0, Batch Number: 5400, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 50s\n",
      "Epoch Number: 0, Batch Number: 5425, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 54s\n",
      "Epoch Number: 0, Batch Number: 5450, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 15m 59s\n",
      "Epoch Number: 0, Batch Number: 5475, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 16m 5s\n",
      "Epoch Number: 0, Batch Number: 5500, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 16m 9s\n",
      "Epoch Number: 0, Batch Number: 5525, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 16m 15s\n",
      "Epoch Number: 0, Batch Number: 5550, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 16m 20s\n",
      "Epoch Number: 0, Batch Number: 5575, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 16m 25s\n",
      "Epoch Number: 0, Batch Number: 5600, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 16m 30s\n",
      "Epoch Number: 0, Batch Number: 5625, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 16m 35s\n",
      "Epoch Number: 0, Batch Number: 5650, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 16m 40s\n",
      "Epoch Number: 0, Batch Number: 5675, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 16m 44s\n",
      "Epoch Number: 0, Batch Number: 5700, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 16m 49s\n",
      "Epoch Number: 0, Batch Number: 5725, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 16m 54s\n",
      "Epoch Number: 0, Batch Number: 5750, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 16m 59s\n",
      "Epoch Number: 0, Batch Number: 5775, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 17m 4s\n",
      "Epoch Number: 0, Batch Number: 5800, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 17m 9s\n",
      "Epoch Number: 0, Batch Number: 5825, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 17m 14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0, Batch Number: 5850, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 17m 18s\n",
      "Epoch Number: 0, Batch Number: 5875, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 17m 23s\n",
      "Epoch Number: 0, Batch Number: 5900, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 17m 28s\n",
      "Epoch Number: 0, Batch Number: 5925, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 17m 33s\n",
      "Epoch Number: 0, Batch Number: 5950, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 17m 38s\n",
      "Epoch Number: 0, Batch Number: 5975, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 17m 43s\n",
      "Epoch Number: 0, Batch Number: 6000, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 17m 49s\n",
      "Epoch Number: 0, Batch Number: 6025, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 17m 54s\n",
      "Epoch Number: 0, Batch Number: 6050, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 17m 59s\n",
      "Epoch Number: 0, Batch Number: 6075, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 3s\n",
      "Epoch Number: 0, Batch Number: 6100, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 8s\n",
      "Epoch Number: 0, Batch Number: 6125, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 13s\n",
      "Epoch Number: 0, Batch Number: 6150, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 17s\n",
      "Epoch Number: 0, Batch Number: 6175, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 23s\n",
      "Epoch Number: 0, Batch Number: 6200, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 27s\n",
      "Epoch Number: 0, Batch Number: 6225, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 31s\n",
      "Epoch Number: 0, Batch Number: 6250, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 36s\n",
      "Epoch Number: 0, Batch Number: 6275, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 40s\n",
      "Epoch Number: 0, Batch Number: 6300, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 45s\n",
      "Epoch Number: 0, Batch Number: 6325, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 50s\n",
      "Epoch Number: 0, Batch Number: 6350, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 54s\n",
      "Epoch Number: 0, Batch Number: 6375, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 18m 59s\n",
      "Epoch Number: 0, Batch Number: 6400, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 3s\n",
      "Epoch Number: 0, Batch Number: 6425, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 8s\n",
      "Epoch Number: 0, Batch Number: 6450, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 13s\n",
      "Epoch Number: 0, Batch Number: 6475, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 17s\n",
      "Epoch Number: 0, Batch Number: 6500, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 22s\n",
      "Epoch Number: 0, Batch Number: 6525, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 27s\n",
      "Epoch Number: 0, Batch Number: 6550, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 31s\n",
      "Epoch Number: 0, Batch Number: 6575, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 36s\n",
      "Epoch Number: 0, Batch Number: 6600, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 40s\n",
      "Epoch Number: 0, Batch Number: 6625, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 44s\n",
      "Epoch Number: 0, Batch Number: 6650, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 48s\n",
      "Epoch Number: 0, Batch Number: 6675, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 52s\n",
      "Epoch Number: 0, Batch Number: 6700, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 19m 56s\n",
      "Epoch Number: 0, Batch Number: 6725, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 0s\n",
      "Epoch Number: 0, Batch Number: 6750, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 4s\n",
      "Epoch Number: 0, Batch Number: 6775, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 8s\n",
      "Epoch Number: 0, Batch Number: 6800, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 12s\n",
      "Epoch Number: 0, Batch Number: 6825, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 17s\n",
      "Epoch Number: 0, Batch Number: 6850, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 20s\n",
      "Epoch Number: 0, Batch Number: 6875, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 24s\n",
      "Epoch Number: 0, Batch Number: 6900, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 29s\n",
      "Epoch Number: 0, Batch Number: 6925, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 32s\n",
      "Epoch Number: 0, Batch Number: 6950, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 36s\n",
      "Epoch Number: 0, Batch Number: 6975, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 40s\n",
      "Epoch Number: 0, Batch Number: 7000, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 45s\n",
      "Epoch Number: 0, Batch Number: 7025, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 50s\n",
      "Epoch Number: 0, Batch Number: 7050, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 55s\n",
      "Epoch Number: 0, Batch Number: 7075, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 20m 59s\n",
      "Epoch Number: 0, Batch Number: 7100, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 3s\n",
      "Epoch Number: 0, Batch Number: 7125, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 5s\n",
      "Epoch Number: 0, Batch Number: 7150, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 8s\n",
      "Epoch Number: 0, Batch Number: 7175, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 11s\n",
      "Epoch Number: 0, Batch Number: 7200, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 13s\n",
      "Epoch Number: 0, Batch Number: 7225, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 16s\n",
      "Epoch Number: 0, Batch Number: 7250, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 19s\n",
      "Epoch Number: 0, Batch Number: 7275, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 23s\n",
      "Epoch Number: 0, Batch Number: 7300, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 27s\n",
      "Epoch Number: 0, Batch Number: 7325, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 32s\n",
      "Epoch Number: 0, Batch Number: 7350, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 36s\n",
      "Epoch Number: 0, Batch Number: 7375, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 41s\n",
      "Epoch Number: 0, Batch Number: 7400, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 45s\n",
      "Epoch Number: 0, Batch Number: 7425, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 48s\n",
      "Epoch Number: 0, Batch Number: 7450, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 52s\n",
      "Epoch Number: 0, Batch Number: 7475, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 21m 56s\n",
      "Epoch Number: 0, Batch Number: 7500, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 1s\n",
      "Epoch Number: 0, Batch Number: 7525, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 5s\n",
      "Epoch Number: 0, Batch Number: 7550, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 10s\n",
      "Epoch Number: 0, Batch Number: 7575, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 14s\n",
      "Epoch Number: 0, Batch Number: 7600, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 19s\n",
      "Epoch Number: 0, Batch Number: 7625, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 24s\n",
      "Epoch Number: 0, Batch Number: 7650, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 29s\n",
      "Epoch Number: 0, Batch Number: 7675, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 34s\n",
      "Epoch Number: 0, Batch Number: 7700, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 38s\n",
      "Epoch Number: 0, Batch Number: 7725, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 42s\n",
      "Epoch Number: 0, Batch Number: 7750, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 45s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0, Batch Number: 7775, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 49s\n",
      "Epoch Number: 0, Batch Number: 7800, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 52s\n",
      "Epoch Number: 0, Batch Number: 7825, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 55s\n",
      "Epoch Number: 0, Batch Number: 7850, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 22m 60s\n",
      "Epoch Number: 0, Batch Number: 7875, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 3s\n",
      "Epoch Number: 0, Batch Number: 7900, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 6s\n",
      "Epoch Number: 0, Batch Number: 7925, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 10s\n",
      "Epoch Number: 0, Batch Number: 7950, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 13s\n",
      "Epoch Number: 0, Batch Number: 7975, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 16s\n",
      "Epoch Number: 0, Batch Number: 8000, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 19s\n",
      "Epoch Number: 0, Batch Number: 8025, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 23s\n",
      "Epoch Number: 0, Batch Number: 8050, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 27s\n",
      "Epoch Number: 0, Batch Number: 8075, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 31s\n",
      "Epoch Number: 0, Batch Number: 8100, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 35s\n",
      "Epoch Number: 0, Batch Number: 8125, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 38s\n",
      "Epoch Number: 0, Batch Number: 8150, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 41s\n",
      "Epoch Number: 0, Batch Number: 8175, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 45s\n",
      "Epoch Number: 0, Batch Number: 8200, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 50s\n",
      "Epoch Number: 0, Batch Number: 8225, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 54s\n",
      "Epoch Number: 0, Batch Number: 8250, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 23m 57s\n",
      "Epoch Number: 0, Batch Number: 8275, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 1s\n",
      "Epoch Number: 0, Batch Number: 8300, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 5s\n",
      "Epoch Number: 0, Batch Number: 8325, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 9s\n",
      "Epoch Number: 0, Batch Number: 8350, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 13s\n",
      "Epoch Number: 0, Batch Number: 8375, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 18s\n",
      "Epoch Number: 0, Batch Number: 8400, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 22s\n",
      "Epoch Number: 0, Batch Number: 8425, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 26s\n",
      "Epoch Number: 0, Batch Number: 8450, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 30s\n",
      "Epoch Number: 0, Batch Number: 8475, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 34s\n",
      "Epoch Number: 0, Batch Number: 8500, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 38s\n",
      "Epoch Number: 0, Batch Number: 8525, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 42s\n",
      "Epoch Number: 0, Batch Number: 8550, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 46s\n",
      "Epoch Number: 0, Batch Number: 8575, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 50s\n",
      "Epoch Number: 0, Batch Number: 8600, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 54s\n",
      "Epoch Number: 0, Batch Number: 8625, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 24m 58s\n",
      "Epoch Number: 0, Batch Number: 8650, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 2s\n",
      "Epoch Number: 0, Batch Number: 8675, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 6s\n",
      "Epoch Number: 0, Batch Number: 8700, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 10s\n",
      "Epoch Number: 0, Batch Number: 8725, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 15s\n",
      "Epoch Number: 0, Batch Number: 8750, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 20s\n",
      "Epoch Number: 0, Batch Number: 8775, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 25s\n",
      "Epoch Number: 0, Batch Number: 8800, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 29s\n",
      "Epoch Number: 0, Batch Number: 8825, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 34s\n",
      "Epoch Number: 0, Batch Number: 8850, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 39s\n",
      "Epoch Number: 0, Batch Number: 8875, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 42s\n",
      "Epoch Number: 0, Batch Number: 8900, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 46s\n",
      "Epoch Number: 0, Batch Number: 8925, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 50s\n",
      "Epoch Number: 0, Batch Number: 8950, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 53s\n",
      "Epoch Number: 0, Batch Number: 8975, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 56s\n",
      "Epoch Number: 0, Batch Number: 9000, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 25m 59s\n",
      "Epoch Number: 0, Batch Number: 9025, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 2s\n",
      "Epoch Number: 0, Batch Number: 9050, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 5s\n",
      "Epoch Number: 0, Batch Number: 9075, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 9s\n",
      "Epoch Number: 0, Batch Number: 9100, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 12s\n",
      "Epoch Number: 0, Batch Number: 9125, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 16s\n",
      "Epoch Number: 0, Batch Number: 9150, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 19s\n",
      "Epoch Number: 0, Batch Number: 9175, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 22s\n",
      "Epoch Number: 0, Batch Number: 9200, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 25s\n",
      "Epoch Number: 0, Batch Number: 9225, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 29s\n",
      "Epoch Number: 0, Batch Number: 9250, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 33s\n",
      "Epoch Number: 0, Batch Number: 9275, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 37s\n",
      "Epoch Number: 0, Batch Number: 9300, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 41s\n",
      "Epoch Number: 0, Batch Number: 9325, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 45s\n",
      "Epoch Number: 0, Batch Number: 9350, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 50s\n",
      "Epoch Number: 0, Batch Number: 9375, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 54s\n",
      "Epoch Number: 0, Batch Number: 9400, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 26m 59s\n",
      "Epoch Number: 0, Batch Number: 9425, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 3s\n",
      "Epoch Number: 0, Batch Number: 9450, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 6s\n",
      "Epoch Number: 0, Batch Number: 9475, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 10s\n",
      "Epoch Number: 0, Batch Number: 9500, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 13s\n",
      "Epoch Number: 0, Batch Number: 9525, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 16s\n",
      "Epoch Number: 0, Batch Number: 9550, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 20s\n",
      "Epoch Number: 0, Batch Number: 9575, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 23s\n",
      "Epoch Number: 0, Batch Number: 9600, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 27s\n",
      "Epoch Number: 0, Batch Number: 9625, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 31s\n",
      "Epoch Number: 0, Batch Number: 9650, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 36s\n",
      "Epoch Number: 0, Batch Number: 9675, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0, Batch Number: 9700, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 43s\n",
      "Epoch Number: 0, Batch Number: 9725, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 46s\n",
      "Epoch Number: 0, Batch Number: 9750, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 49s\n",
      "Epoch Number: 0, Batch Number: 9775, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 52s\n",
      "Epoch Number: 0, Batch Number: 9800, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 56s\n",
      "Epoch Number: 0, Batch Number: 9825, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 27m 59s\n",
      "Epoch Number: 0, Batch Number: 9850, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 28m 2s\n",
      "Epoch Number: 0, Batch Number: 9875, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 28m 5s\n",
      "Epoch Number: 0, Batch Number: 9900, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 28m 8s\n",
      "Epoch Number: 0, Batch Number: 9925, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 28m 11s\n",
      "Epoch Number: 0, Batch Number: 9950, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 28m 13s\n",
      "Epoch Number: 0, Batch Number: 9975, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 28m 16s\n",
      "Epoch Number: 0, Batch Number: 10000, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Time so far is 28m 19s\n",
      "\n",
      "Training complete in 28m 19s\n",
      "Best loss: 0.020078\n"
     ]
    }
   ],
   "source": [
    "best_model, train_plot_losses, validation_plot_losses = training.train_model(ntm, data_loader, copy_task_loss, optimizer, None, num_epochs=1, print_every=25, deep_copy_desired=False, validation_criterion=error_bits_per_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD9CAYAAABHnDf0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBDnczcwS9P/PszIEEqj+vgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff71969c908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0, 10000, 100), train_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEbhJREFUeJzt3W2MHVd9x/Hvf+69jk2SxuvYWLaTsE4VIVlqScKSJgIh\nnglR1UBLpUQVuIXKVQsStJVQIl60fVNBxUOLSgG3pIkqCLQ8RhEtTQMVQlRpNoQGQxLsQACbxN5A\n4gRI8NPpiznXe1nvep/u+u6c+X6kq507M7v3nB3nl7Nn/jMTKSUkSc1XjboBkqThMNAlqRAGuiQV\nwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JheieyQ/buHFjGh8fP5MfKUmNd8899zyWUto0335n\nNNDHx8eZnJw8kx8pSY0XEd9fyH5OuUhSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVIhGBPqX\nHjjIP/z3vlE3Q5JWtUYE+lf3/pgPfmkfPv9UkubWiEDfun4tPztynCefPjbqpkjSqtWIQN+2fh0A\n+5/4+YhbIkmrVyMCfWsO9B898cyIWyJJq9e8gR4RF0bElyPi2xHxrYh4W16/ISLuiIi9+evYSjVy\nOtCfXqmPkKTGW8gI/Rjw5ymlHcCVwFsiYgdwA3BnSukS4M78fkWcf/Ya1nQrA12STmPeQE8pPZJS\n+npefgq4H9gGXAvckne7BXjtijWyCraet5YDBrokzWlRc+gRMQ5cBtwFbE4pPZI3PQpsHmrLZtg2\nts4RuiSdxoIDPSLOAT4NvD2l9OTgtlQXiM9aJB4RuyJiMiImp6amltzQreet86SoJJ3GggI9InrU\nYf6xlNJn8uqDEbElb98CHJrte1NKu1NKEymliU2b5n2C0py2rl/Hwaee4ejxE0v+GZJUsoVUuQTw\nUeD+lNL7BjbdBuzMyzuBzw+/edO2rV9HSvDoYUfpkjSbhYzQXwi8AXhZRHwjv64B3gW8MiL2Aq/I\n71eMpYuSdHrzPiQ6pfRVIObY/PLhNmduW9evBeBHhw10SZpNI64UhekR+oHHDXRJmk1jAn1tr8P5\nZ6/hgJUukjSrxgQ61KN059AlaXYNC/S1BrokzaFhgV6P0H3QhSSdqlGBvm39Oh90IUlzaFSgn6x0\ncdpFkk7RqEDf5sVFkjSnRgX6yatFvbhIkk7RqEDvP+jCKRdJOlWjAr3/oAtvoytJp2pUoEM97XLg\n8Z+PuhmStOo0MtAdoUvSqRoX6BeOPYuDTz3DT39hLbokDWpcoF/+nPWkBF///uOjbookrSqNC/TL\nLhqjUwV3P/yTUTdFklaVxgX6OWd12bHlVwx0SZqhcYEO8ILxDdz7gyc4cswHRktSXyMD/YrtY/zi\n2Am+eeDwqJsiSatGIwN9YnwDAJNOu0jSSY0M9I3nnMXFG892Hl2SBjQy0KGeR7/74cc5ccKHXUgS\nNDnQt2/g8NNH2Xvop6NuiiStCs0N9PExAKddJClrbKBftOFZPPvcswx0ScoaG+gRwQu2b+Du7xno\nkgQNDnSA511wHj86/AyHnz466qZI0sg1OtDX9ToAXjEqSTQ80LuduvnHThjoktTsQK8CgGPHrUWX\npEYH+ppu3fyjxx2hS1KjA71b9adcHKFLUrMDvVNPuThCl6SGB3qv4xy6JPU1OtCnp1wcoUtSswP9\n5JSLI3RJanSg9zpWuUhSX6MD3Tp0SZrW6EB3hC5J0+YN9Ii4KSIORcSegXV/GREHIuIb+XXNyjZz\ndv05dOvQJWlhI/SbgatnWf/+lNKl+fWF4TZrYfpVLo7QJWkBgZ5S+gqwKm86bh26JE1bzhz6WyPi\nvjwlMza0Fi2Cd1uUpGlLDfQPAb8KXAo8Arx3rh0jYldETEbE5NTU1BI/bna9yjp0SepbUqCnlA6m\nlI6nlE4A/whccZp9d6eUJlJKE5s2bVpqO2fVr3I55hy6JC0t0CNiy8Db1wF75tp3JVnlIknTuvPt\nEBG3Ai8BNkbEfuAvgJdExKVAAh4G/mgF2zin6Tp0A12S5g30lNL1s6z+6Aq0ZdGmrxR1ykWSGn2l\naKd/UtQpF0lqdqBHBL1OeGGRJNHwQIf6alGnXCSphEDvhCdFJYkCAr3XqbxSVJIoINC7VXgvF0mi\ngEDvdSqnXCSJAgK92wmnXCSJEgLdKRdJAgoI9HrKxRG6JBUR6N6cS5IKCPSuV4pKElBAoPeqyjl0\nSaKAQLfKRZJqBQR6xRFH6JLU/EDvVeHNuSSJAgK927EOXZKgiECvOOocuiQ1P9B7XikqSUABgd7t\n+IALSYICAr3XCZ8pKkkUEOg+gk6Sas0PdKtcJAkoINDXWOUiSUABge4IXZJqzQ/0qr59bkqGuqR2\na3yg9zoB4HNFJbVe4wO926m74B0XJbVd8wO9coQuSVBAoPf6I3Rr0SW1XOMDvZvn0H2uqKS2a3yg\n96q6Cz5XVFLbNT7QT47QnUOX1HIFBLpVLpIEBQR6zyoXSQIKCPSTI3QDXVLLFRDoeYTulIuklmt8\noK9xhC5JwAICPSJuiohDEbFnYN2GiLgjIvbmr2Mr28y59a8U9cIiSW23kBH6zcDVM9bdANyZUroE\nuDO/H4n+HPoRA11Sy80b6CmlrwA/mbH6WuCWvHwL8Noht2vBetahSxKw9Dn0zSmlR/Lyo8DmIbVn\n0bqVdeiSBEM4KZrqJ0vMOTyOiF0RMRkRk1NTU8v9uFN4P3RJqi010A9GxBaA/PXQXDumlHanlCZS\nShObNm1a4sfNzStFJam21EC/DdiZl3cCnx9OcxbP+6FLUm0hZYu3Av8DPDci9kfEm4F3Aa+MiL3A\nK/L7kehZhy5JAHTn2yGldP0cm14+5LYsyfT90J1ykdRujb9SdPp+6I7QJbVb4wN9+n7ojtAltVs5\nge4j6CS1XOMD3UfQSVKt8YFeVUGnCqtcJLVe4wMd6lp0R+iS2q6IQO91KqtcJLVeEYHe7YR16JJa\nr4xArxyhS1IRgd7rhHXoklqviECvp1wcoUtqtyICvVdVVrlIar0iAr3bsQ5dksoI9KqyykVS6xUR\n6L1OWOUiqfWKCPRuxxG6JJUR6JUjdEkqItDrS/8doUtqt0IC3SoXSSoi0LuO0CWpjEDveaWoJJUR\n6N2q8l4uklqvjEC3Dl2Sygj0nleKSlIZge69XCSpkEC3Dl2SCgn0bmWViySVEeidyikXSa1XRKD3\nOsFRT4pKarkiAr1bVaQEx512kdRiZQR6JwA8MSqp1YoI9DWduhsGuqQ2KyLQ+yN0T4xKarNCAj2P\n0D0xKqnFigj0XuUIXZKKCPT+CN1Al9RmRQR6r1/l4pSLpBYrItC7lSN0SSoj0K1DlyS6y/nmiHgY\neAo4DhxLKU0Mo1GL1Z9y8QZdktpsWYGevTSl9NgQfs6STU+5OEKX1F6FTbk4QpfUXssN9AT8Z0Tc\nExG7ZtshInZFxGRETE5NTS3z42bX65ctWuUiqcWWG+gvSildDrwGeEtEvHjmDiml3SmliZTSxKZN\nm5b5cbPrVp4UlaRlBXpK6UD+egj4LHDFMBq1WL2TN+dyykVSey050CPi7Ig4t78MvArYM6yGLUbP\nK0UlaVlVLpuBz0ZE/+d8PKX0H0Np1SKdvNuic+iSWmzJgZ5S+i7wvCG2Zcl6lVMuklRU2aJ16JLa\nrKhAP+qVopJarIhA73mlqCSVEeg+gk6SCgn0no+gk6QyAr3rI+gkqYxA71RWuUhSEYEeEfQ6wRFH\n6JJarIhAh/qe6I7QJbVZOYHeCZ9YJKnVign0Xqfy9rmSWq2gQA+rXCS1WjGB3q0q69AltVoxge4I\nXVLbFRPo3U7l/dAltVo5gV6F90OX1GrFBHqvYx26pHYrJtCtQ5fUdsUEeq+yDl1SuxUT6F2rXCS1\nXEGB7ghdUrsVE+g9q1wktVwxgV6fFHWELqm9Cgr0yjl0Sa1WTKD3qvBeLpJarZxAd4QuqeWKCfS6\nysVAl9RexQR6z5OiklqumECvnynqCF1SexUT6L1OeGGRpFYrJtC9OZektisn0KuK4ycSKRnqktqp\nmEDvdQLAShdJrVVMoHc7dVecR5fUVuUEelWP0K10kdRWxQR6rz9CtxZdUksVE+jdjiN0Se1WTKD3\nKufQJbXbsgI9Iq6OiAcjYl9E3DCsRi3FxnPXAPC5ew+MshmSNDJLDvSI6AAfBF4D7ACuj4gdw2rY\nYr30uc/mdZdt4713fIfP3rt/VM2QpJFZzgj9CmBfSum7KaUjwCeAa4fTrMWLCN79O7/OVRefzzs+\ndR9fe+ixUTVFkkaiu4zv3Qb8cOD9fuA3ltec5VnTrfjwG57P6z/0Nd588yQXjK0bZXMk6aS//u1f\n4wXjG1b0M5YT6AsSEbuAXQAXXXTRSn8c563rcfObruD9d3yHnx85tuKfJ0kLsa7XWfHPWE6gHwAu\nHHh/QV73S1JKu4HdABMTE2ekpnDb+nW853efdyY+SpJWjeXMod8NXBIR2yNiDXAdcNtwmiVJWqwl\nj9BTSsci4q3AF4EOcFNK6VtDa5kkaVGWNYeeUvoC8IUhtUWStAzFXCkqSW1noEtSIQx0SSqEgS5J\nhTDQJakQcSYfqhwRU8D3l/jtG4G23aDFPreDfW6H5fT5OSmlTfPtdEYDfTkiYjKlNDHqdpxJ9rkd\n7HM7nIk+O+UiSYUw0CWpEE0K9N2jbsAI2Od2sM/tsOJ9bswcuiTp9Jo0QpcknUYjAn01PYx6OSLi\nwoj4ckR8OyK+FRFvy+s3RMQdEbE3fx3L6yMiPpD7fV9EXD7ws3bm/fdGxM5R9WmhIqITEfdGxO35\n/faIuCv37ZP5FsxExFn5/b68fXzgZ9yY1z8YEa8eTU8WJiLWR8SnIuKBiLg/Iq4q/ThHxJ/mf9d7\nIuLWiFhb2nGOiJsi4lBE7BlYN7TjGhHPj4hv5u/5QETEohqYUlrVL+pb8z4EXAysAf4P2DHqdi2x\nL1uAy/PyucB3qB+w/TfADXn9DcC78/I1wL8DAVwJ3JXXbwC+m7+O5eWxUfdvnr7/GfBx4Pb8/l+B\n6/Lyh4E/zst/Anw4L18HfDIv78jH/ixge/430Rl1v07T31uAP8zLa4D1JR9n6kdSfg9YN3B8f7+0\n4wy8GLgc2DOwbmjHFfjfvG/k733Noto36l/QAn6BVwFfHHh/I3DjqNs1pL59Hngl8CCwJa/bAjyY\nlz8CXD+w/4N5+/XARwbW/9J+q+1F/TSrO4GXAbfnf6yPAd2Zx5j6/vpX5eVu3i9mHvfB/VbbCzgv\nh1vMWF/scWb6GcMb8nG7HXh1iccZGJ8R6EM5rnnbAwPrf2m/hbyaMOUy28Oot42oLUOT/8S8DLgL\n2JxSeiRvehTYnJfn6nvTfid/C7wDOJHfnw88kVLqP/R1sP0n+5a3H877N6nP24Ep4J/zNNM/RcTZ\nFHycU0oHgPcAPwAeoT5u91D2ce4b1nHdlpdnrl+wJgR6cSLiHODTwNtTSk8Obkv1/5qLKT2KiN8E\nDqWU7hl1W86gLvWf5R9KKV0G/Iz6T/GTCjzOY8C11P8z2wqcDVw90kaNwKiPaxMCfUEPo26KiOhR\nh/nHUkqfyasPRsSWvH0LcCivn6vvTfqdvBD4rYh4GPgE9bTL3wHrI6L/xKzB9p/sW95+HvBjmtXn\n/cD+lNJd+f2nqAO+5OP8CuB7KaWplNJR4DPUx77k49w3rON6IC/PXL9gTQj0Yh5Gnc9YfxS4P6X0\nvoFNtwH9M907qefW++vfmM+WXwkczn/afRF4VUSM5ZHRq/K6VSeldGNK6YKU0jj1sftSSun3gC8D\nr8+7zexz/3fx+rx/yuuvy9UR24FLqE8grToppUeBH0bEc/OqlwPfpuDjTD3VcmVEPCv/O+/3udjj\nPGAoxzVvezIirsy/wzcO/KyFGfUJhgWehLiGuiLkIeCdo27PMvrxIuo/x+4DvpFf11DPHd4J7AX+\nC9iQ9w/gg7nf3wQmBn7Wm4B9+fUHo+7bAvv/EqarXC6m/g91H/BvwFl5/dr8fl/efvHA978z/y4e\nZJFn/0fQ10uByXysP0ddzVD0cQb+CngA2AP8C3WlSlHHGbiV+hzBUeq/xN48zOMKTOTf30PA3zPj\nxPp8L68UlaRCNGHKRZK0AAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmF+H8y01aglgQk\nCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff7196a1b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0, 10000, 100), validation_plot_losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
