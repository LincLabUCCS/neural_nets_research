{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A lot of inspiration from https://github.com/loudinthecloud/pytorch-ntm. Hyperparameters were chosen based\n",
    "# upon his experiments.\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data as d\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _split_cols(mat, lengths):\n",
    "    \"\"\"Split a 2D matrix to variable length columns.\"\"\"\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results += [mat[:, s:e]]\n",
    "    return results\n",
    "\n",
    "class NTM_Head(nn.Module):\n",
    "    def __init__(self, memory, controller_output_size):\n",
    "        super(NTM_Head, self).__init__()\n",
    "        \n",
    "        self.memory = memory\n",
    "        self.controller_output_size = controller_output_size\n",
    "        self.N, self.M = memory.size()\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def address_memory(self, key_vec, β, g, s, γ):\n",
    "        result = F.cosine_similarity(key_vec.unsqueeze(0).expand(self.N, -1), self.memory, dim = 0)\n",
    "        result = β * result\n",
    "        result = result.exp() / result.sum()\n",
    "        result = g * result + (1 - g) * self.prev_address_vector\n",
    "        result = F.conv1d(torch.cat((result[:, -1], result, result[:, 1]), 1).view(-1, 1, self.N), \n",
    "                          s.view(1, 1, -1))\n",
    "        result = result ** γ\n",
    "        result = result / result.sum()\n",
    "        return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NTM_Read_Head(NTM_Head):\n",
    "    def __init__(self, memory, controller_output_size):\n",
    "        super(NTM_Read_Head).__init__(memory, controller_output_size)\n",
    "        \n",
    "        self.read_parameters_lengths = [self.N, 1, 1, 3, 1]\n",
    "        self.fc_read_parameters = nn.Linear(controller_output_size, sum(read_parameters_lengths))\n",
    "        \n",
    "        initialize_parameters()\n",
    "        initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_read_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_read_parameters.bias, std=0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.initial_address_vector = nn.Parameter(torch.zeros(self.N))\n",
    "        self.initial_read = nn.Parameter(torch.zeros(self.M))\n",
    "        \n",
    "        self.prev_address_vector = Variable(self.initial_address_vector)\n",
    "        self.prev_read = Variable(self.initial_read)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        read_parameters = self.fc_read_parameters(x)\n",
    "        \n",
    "        key_vec, β, g, s, γ = _split_cols(read_parameters, self.read_parameters_length)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "                                               \n",
    "        address_vec = address_memory(key_vec, β, g, s, γ)\n",
    "        new_read = self.M.transpose() * address_vec\n",
    "        self.prev_read = new_read\n",
    "        return new_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NTM_Write_Head(NTM_Head):\n",
    "    def __init__(self, memory, controller_output_size):\n",
    "        super(NTM_Write_Head).__init__(memory, controller_output_size)\n",
    "        \n",
    "        self.write_parameters_lengths = [self.N, 1, 1, 3, 1, self.M, self.M]\n",
    "        self.fc_write_parameters = nn.Linear(controller_output_size, sum(write_parameters_size))\n",
    "        \n",
    "        reset_parameters()\n",
    "        initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_write_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_write_parameters.bias, std=0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.initial_address_vector = nn.Parameter(torch.zeros(self.N))       \n",
    "        self.prev_address_vector = Variable(self.initial_address_vector)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        write_parameters = self.fc_write_parameters(x)\n",
    "                                       \n",
    "        key_vec, β, g, s, γ, erase_vec, add_vec = _split_cols(write_parameters, self.write_parameters_size)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "        erase_vec = F.sigmoid(erase_vec)\n",
    "                                               \n",
    "        address_vec = address_memory(key_vec, β, g, s, γ)\n",
    "        self.M *= 1 - torch.bmm(address_vec.unsqueeze(2), erase_vec.unsquueze(1))\n",
    "        self.M += torch.bmm(address_vec.unsqueeze(2), add_vec.unsquueze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, controller, controller_output_size, output_size, \n",
    "                 address_count, address_dimension, heads):\n",
    "        super(NTM, self).__init__()\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = nn.Parameter(torch.zeros(address_count, address_dimension))\n",
    "        \n",
    "        # Construct the heads.\n",
    "        self.heads = nn.ModuleList()\n",
    "        \n",
    "        for head_id in heads:\n",
    "            if head_id = 0:\n",
    "                self.heads.append(NTM_Read_Head(memory, controller_output_size))\n",
    "            else:\n",
    "                self.heads.append(NTM_Write_Head(memory, controller_output_size))\n",
    "        \n",
    "        # Initialize controller\n",
    "        self.controller = controller\n",
    "        self.outputGate = nn.Linear(controller_output_size, output_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        initialize_state()\n",
    "        reset_parameters()\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.prev_reads = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head.prev_read)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdev = 1 / (np.sqrt(N + M))\n",
    "        nn.init.uniform(self.memory, -stdev, stdev)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        controller_output = controller(torch.cat(self.prev_reads.append(x), dim=1))\n",
    "        self.prev_reads = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head(controller_output))\n",
    "            else:\n",
    "                head(controller_output)\n",
    "        \n",
    "        return self.softmax(self.outputGate(controller_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CopyTaskDataset(d.Dataset):\n",
    "    def __init__(self, batch_size, lower, upper, seq_size):\n",
    "        self.input_list = []\n",
    "        for _ in range(batch_size):\n",
    "            self.input_list.append(self.generate_batch(batch_size, lower, upper, seq_size))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batch(self, batch_size, lower, upper, seq_size):\n",
    "        seq_length = random.randint(lower, upper)\n",
    "        batch = torch.from_numpy(\n",
    "            numpy.random.binomial(1, 0.5, (seq_length, batch_size, seq_size)))\n",
    "        end_marker = torch.zeros(seq_length - 1, batch_size, 1)\n",
    "        end_row = torch.ones(1, batch_size, 1)\n",
    "        end_marker = torch.cat((end_marker, end_row), 0)\n",
    "        batch = torch.cat((batch.float(), end_marker), 2)\n",
    "        print(batch)\n",
    "        return batch \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.input_list[i//self.batch_size][:, i % self.batch_size, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainAll(num_batches, batch_size, lower_seq_length=3, \n",
    "             upper_seq_length=10, seq_size=8):\n",
    "    ntm = NTM(1,1,1,1,1)   \n",
    "    dataset = CopyTaskDataset(batch_size, lower_seq_length, upper_seq_length, seq_size)\n",
    "    \n",
    "    data_loader = d.DataLoader(dataset, batch_size = batch_size)\n",
    "    for _ in range(num_batches):\n",
    "        for batch in data_loader:\n",
    "            batch = batch.squeeze()\n",
    "            # Pass in one element of the sequence per time step\n",
    "            for time_step in range(len(batch)):\n",
    "                #ntm(batch[time_step], None)\n",
    "                pass\n",
    "                \n",
    "            # Now, don't pass in any elements.\n",
    "            for time_step in range(len(batch)):\n",
    "                ntm(None, batch[time_step][:,:-1])\n",
    "    print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncapsulatedLSTM(nn.Module):\n",
    "    def __init__(self, batch_size, *args, **kwargs):\n",
    "        super(EncapsulatedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(*args, **kwargs)\n",
    "        hidden_size = args[1]\n",
    "        num_layers = args[2]\n",
    "        \n",
    "        initial_hidden_state = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n",
    "        initial_hidden_state.expand(num_layers, batch_size, hidden_size)\n",
    "        initial_cell_state = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n",
    "        initial_cell_state.expand(num_layers, batch_size, hidden_size)\n",
    "        self.state_tuple = (initial_hidden_state, initial_cell_state)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output, self.state_tuple = self.lstm(input, self.state_tuple)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
