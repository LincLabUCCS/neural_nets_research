{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ericweiner/Documents/neural_nets_research\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets_library import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of inspiration from https://github.com/loudinthecloud/pytorch-ntm. Hyperparameters were chosen based\n",
    "# upon his experiments.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seed(seed=None):\n",
    "    \"\"\"Seed the RNGs for predicatability/reproduction purposes.\"\"\"\n",
    "    if seed is None:\n",
    "        seed = int(get_ms() // 1000)\n",
    "\n",
    "    print(\"Using seed=%d\", seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Memory(nn.Module):\n",
    "    def __init__(self, address_count, address_dimension, batch_size):\n",
    "        super(NTM_Memory, self).__init__()\n",
    "        self.initial_memory = nn.Parameter(torch.zeros(1, address_count, address_dimension))\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        _, N, M = self.initial_memory.size()\n",
    "        stdev = 1 / np.sqrt(N + M)\n",
    "        nn.init.uniform(self.initial_memory, -stdev, stdev)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.memory = self.initial_memory.repeat(self.batch_size, 1, 1)\n",
    "    \n",
    "    def address_memory(self, key_vec, prev_address_vec, β, g, s, γ):\n",
    "        EPSILON = 1e-16\n",
    "        result = F.cosine_similarity((key_vec+EPSILON).unsqueeze(1).expand_as(self.memory), \n",
    "                                     self.memory+EPSILON, dim = 2)\n",
    "        result = F.softmax(β * result, dim=1)\n",
    "        result = g * result + (1 - g) * prev_address_vec\n",
    "        result = torch.cat((result[:, 1:], result[:, :1]), 1) * s[:, 0:1] + result * s[:, 1:2] + \\\n",
    "                 torch.cat((result[:, -1:], result[:, :-1]), 1) * s[:, 2:3]\n",
    "\n",
    "#         result = result ** γ\n",
    "#         result = result / (result.sum(1, keepdim=True) + EPSILON)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def read_memory(self, address_vec):\n",
    "        return torch.bmm(self.memory.transpose(1,2), address_vec.unsqueeze(2)).squeeze(2)\n",
    "    \n",
    "    def update_memory(self, address_vec, erase_vec, add_vec):\n",
    "        self.memory = self.memory * (1 - torch.bmm(address_vec.unsqueeze(2), erase_vec.unsqueeze(1)))\n",
    "        self.memory += torch.bmm(address_vec.unsqueeze(2), add_vec.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_cols(mat, lengths):\n",
    "    \"\"\"Split a 2D matrix to variable length columns.\"\"\"\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results += [mat[:, s:e]]\n",
    "    return results\n",
    "\n",
    "class NTM_Head(nn.Module):\n",
    "    def __init__(self, address_count, address_dimension, \n",
    "                 controller_output_size):\n",
    "        super(NTM_Head, self).__init__()\n",
    "        \n",
    "        self.controller_output_size = controller_output_size\n",
    "        self.N = address_count\n",
    "        self.M = address_dimension\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Read_Head(NTM_Head):\n",
    "    def __init__(self, address_count, address_dimension, controller_output_size, batch_size):\n",
    "        super(NTM_Read_Head, self).__init__(address_count, address_dimension, controller_output_size)\n",
    "        \n",
    "        self.read_parameters_lengths = [self.M, 1, 1, 3, 1]\n",
    "        self.fc_read_parameters = nn.Linear(controller_output_size, sum(self.read_parameters_lengths))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_read_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_read_parameters.bias, std=0.01)\n",
    "        \n",
    "        self.initial_address_vec = nn.Parameter(torch.zeros(self.N))\n",
    "        self.initial_read = nn.Parameter(torch.randn(1, self.M) * 0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.prev_address_vec = self.initial_address_vec.clone()\n",
    "        self.prev_read = self.initial_read.repeat(self.batch_size, 1)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        read_parameters = self.fc_read_parameters(x)\n",
    "        \n",
    "        key_vec, β, g, s, γ = _split_cols(read_parameters, self.read_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "        \n",
    "        self.prev_address_vec = memory.address_memory(key_vec, self.prev_address_vec, β, g, s, γ)\n",
    "        new_read = memory.read_memory(self.prev_address_vec)        \n",
    "        self.prev_read = new_read\n",
    "        return new_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Write_Head(NTM_Head):\n",
    "    def __init__(self, address_count, address_dimension, controller_output_size):\n",
    "        super(NTM_Write_Head, self).__init__(address_count, address_dimension, controller_output_size)\n",
    "        \n",
    "        self.write_parameters_lengths = [self.M, 1, 1, 3, 1, self.M, self.M]\n",
    "        self.fc_write_parameters = nn.Linear(controller_output_size, sum(self.write_parameters_lengths))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_write_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_write_parameters.bias, std=0.01)\n",
    "        \n",
    "        self.initial_address_vec = nn.Parameter(torch.zeros(self.N))\n",
    "    \n",
    "    def initialize_state(self):       \n",
    "        self.prev_address_vec = self.initial_address_vec.clone()\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return False\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        write_parameters = self.fc_write_parameters(x)\n",
    "                                       \n",
    "        key_vec, β, g, s, γ, erase_vec, add_vec = _split_cols(write_parameters, self.write_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "        erase_vec = F.sigmoid(erase_vec)\n",
    "                                               \n",
    "        self.prev_address_vec = memory.address_memory(key_vec, self.prev_address_vec, β, g, s, γ)\n",
    "        memory.update_memory(self.prev_address_vec, erase_vec, add_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, batch_size, controller, controller_output_size, \n",
    "                 output_size, address_count, address_dimension, heads):\n",
    "        super(NTM, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize controller\n",
    "        self.controller = controller\n",
    "        \n",
    "        # Create output gate. No activation function is used with it because\n",
    "        # I used BCEWithLogitsLoss which deals with the sigmoid in a more\n",
    "        # numerically stable manner.\n",
    "        self.outputGate = nn.Linear(controller_output_size, output_size)\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = NTM_Memory(address_count, address_dimension, batch_size)\n",
    "\n",
    "        # Construct the heads.\n",
    "        self.heads = nn.ModuleList()\n",
    "        \n",
    "        for head_id in heads:\n",
    "            if head_id == 0:\n",
    "                self.heads.append(NTM_Read_Head(address_count, address_dimension, \n",
    "                                                controller_output_size, batch_size))\n",
    "            else:\n",
    "                self.heads.append(NTM_Write_Head(address_count, address_dimension,\n",
    "                                                 controller_output_size))\n",
    "        self.initialize_state()\n",
    "        \n",
    "    def initialize_state(self):\n",
    "        self.prev_reads = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            head.initialize_state()\n",
    "            \n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head.prev_read)\n",
    "        \n",
    "        self.memory.initialize_state()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.outputGate.weight)\n",
    "        nn.init.normal(self.outputGate.bias, std=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.initialize_state()\n",
    "        outputs = []\n",
    "        \n",
    "        for current_observation in x.transpose(0,1):\n",
    "            self.prev_reads.append(current_observation)\n",
    "            controller_input = torch.cat(self.prev_reads, 1)\n",
    "            controller_output = self.controller(controller_input)\n",
    "            self.prev_reads = []\n",
    "\n",
    "            for head in self.heads:                \n",
    "                if head.is_read_head():\n",
    "                    self.prev_reads.append(head(controller_output, self.memory))\n",
    "                else:\n",
    "                    head(controller_output, self.memory)\n",
    "                    \n",
    "            current_output = self.outputGate(controller_output)\n",
    "            outputs.append(current_output)\n",
    "        \n",
    "        return torch.stack(outputs).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyTaskDataset(data.Dataset):\n",
    "    def __init__(self, num_batches, batch_size, lower, upper, seq_size):\n",
    "        self.input_list = []\n",
    "        self.label_list = []\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            data, label = self.generate_batch(batch_size, lower, upper, seq_size)\n",
    "            self.input_list.append(data)\n",
    "            self.label_list.append(label)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batch(self, batch_size, lower, upper, seq_size):\n",
    "        seq_length = random.randint(lower, upper)\n",
    "        label = torch.from_numpy(\n",
    "                np.random.binomial(1, 0.5, (seq_length, batch_size, seq_size))).float()\n",
    "        end_marker = torch.zeros(seq_length, batch_size, 1)\n",
    "        seq = torch.cat((label, end_marker), 2)\n",
    "        delimiter_column = torch.zeros(1, batch_size, seq_size+1)\n",
    "        delimiter_column[0, :, seq_size] = 1\n",
    "        seq = torch.cat((seq, delimiter_column), 0)\n",
    "        output_time = torch.zeros(seq_length, batch_size, seq_size+1)\n",
    "        seq = torch.cat((seq, output_time), 0)\n",
    "        return seq, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)*self.batch_size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        batch_index = i//self.batch_size\n",
    "        index_in_batch = i % self.batch_size\n",
    "        return self.input_list[batch_index][:, index_in_batch, :], self.label_list[batch_index][:, index_in_batch, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionTaskDataset(data.Dataset):\n",
    "    def __init__(self, num_batches, batch_size, lower, upper, seq_size):\n",
    "        self.input_list = []\n",
    "        self.label_list = []\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            data, label = self.generate_batch(batch_size, lower, upper, 0)\n",
    "            self.input_list.append(data)\n",
    "            self.label_list.append(label)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batch(self, batch_size, min_int, max_int, think_time):\n",
    "        seq_length = think_time + 2\n",
    "        inputs = [random.randint(min_int, max_int - 1) for _ in range(batch_size)]\n",
    "        outputs = [(inputs[i] + 2) % max_int for i in range(len(inputs))]\n",
    "        input_indices = torch.LongTensor(inputs).unsqueeze(1)\n",
    "        output_indices = torch.LongTensor(outputs).unsqueeze(1)\n",
    "        input_one_hot = torch.zeros(batch_size,max_int)\n",
    "        input_one_hot.scatter_(1, input_indices, 1)\n",
    "        input_one_hot = input_one_hot.unsqueeze(0)\n",
    "        output_one_hot = torch.zeros(batch_size,max_int)\n",
    "        output_one_hot.scatter_(1, output_indices, 1)\n",
    "        output_one_hot = output_one_hot.unsqueeze(0)\n",
    "        label = output_one_hot\n",
    "        seq = torch.cat((input_one_hot, torch.zeros(seq_length - 1, batch_size, max_int)), 0)\n",
    "        return seq, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)*self.batch_size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        batch_index = i//self.batch_size\n",
    "        index_in_batch = i % self.batch_size\n",
    "        return self.input_list[batch_index][:, index_in_batch, :], self.label_list[batch_index][:, index_in_batch, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncapsulatedLSTM(nn.Module):\n",
    "    def __init__(self, batch_size, all_hiddens, *args, **kwargs):\n",
    "        super(EncapsulatedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(*args, **kwargs)\n",
    "        self.all_hiddens = all_hiddens\n",
    "        self.batch_size = batch_size\n",
    "                \n",
    "        self.num_inputs = args[0]\n",
    "        self.hidden_size = args[1]\n",
    "        self.num_layers = args[2]\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "          \n",
    "    def initialize_state(self):\n",
    "        self.state_tuple = (self.initial_hidden_state.repeat(1, self.batch_size, 1), \n",
    "                            self.initial_cell_state.repeat(1, self.batch_size, 1))\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.initial_hidden_state = nn.Parameter(torch.randn(self.num_layers, 1, self.hidden_size) * 0.05)\n",
    "        self.initial_cell_state = nn.Parameter(torch.randn(self.num_layers, 1, self.hidden_size) * 0.05)\n",
    "        \n",
    "        for p in self.lstm.parameters():\n",
    "            if p.dim() == 1:\n",
    "                nn.init.constant(p, 0)\n",
    "            else:\n",
    "                stdev = 5 / (np.sqrt(self.num_inputs +  self.hidden_size))\n",
    "                nn.init.uniform(p, -stdev, stdev)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.initialize_state()\n",
    "        output, self.state_tuple = self.lstm(input.unsqueeze(0), self.state_tuple)\n",
    "        \n",
    "        if self.all_hiddens:\n",
    "            return self.state_tuple[0].transpose(0,1).view(self.batch_size, -1)\n",
    "        else:\n",
    "            return output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition_task_loss(output, label):\n",
    "    return F.binary_cross_entropy_with_logits(output[:, -1:,:], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_task_loss(output, label):\n",
    "    _, seq_length, _ = label.size()\n",
    "    return F.binary_cross_entropy_with_logits(output[:, -seq_length:, :], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_bits_per_sequence(output, label):\n",
    "    batch_size, seq_length, _ = label.size()\n",
    "    binarized_output = output[:, -seq_length:, :].sign()/2 + 0.5\n",
    "    \n",
    "    # The cost is the number of error bits per sequence\n",
    "    return torch.sum(torch.abs(binarized_output - label))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_clipped_optimizer(optimizer_type):\n",
    "#     class ClippedOptimizer(optimizer_type):\n",
    "#         def step(self, closure=None):\n",
    "#             for group in self.param_groups:\n",
    "#                 for p in group['params']:\n",
    "#                     if p.grad is not None:\n",
    "#                         p.grad = p.grad.data.clamp(-10,10)\n",
    "            \n",
    "#             super().step(closure)\n",
    "    \n",
    "#     return ClippedOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClippedRMSprop = construct_clipped_optimizer(optim.RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hidden_size = 100\n",
    "num_layers = 3\n",
    "seq_size = 4\n",
    "address_size = 20\n",
    "#Initialized for addition task\n",
    "controller = EncapsulatedLSTM(batch_size, False, # all hiddens\n",
    "                              10 + address_size, hidden_size, \n",
    "                              num_layers)\n",
    "\n",
    "#Uncomment for copy task\n",
    "#controller = EncapsulatedLSTM(batch_size, False, # all hiddens\n",
    "#                              10 + address_size, hidden_size, \n",
    "#                              num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_count = 128\n",
    "controller_output_size = hidden_size\n",
    "#Initialized for Addition task\n",
    "ntm = NTM(batch_size, controller, controller_output_size, \n",
    "          10, address_count, address_size, [0, 1])\n",
    "\n",
    "#Uncomment for copy task\n",
    "#ntm = NTM(batch_size, controller, controller_output_size, \n",
    "#          seq_size, address_count, address_size, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ntm = ntm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_seq_length = 3\n",
    "upper_seq_length = 10\n",
    "num_batches = 2000\n",
    "\n",
    "dataset = CopyTaskDataset(num_batches, batch_size, lower_seq_length, upper_seq_length, seq_size)\n",
    "data_loader = data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_int = 0\n",
    "\n",
    "dataset = AdditionTaskDataset(num_batches, batch_size, min_int, max_int, think_time)\n",
    "data_loader = data.DataLoader(dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "    0     0     1     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 2x10]\n",
      ", \n",
      "    0     0     0     0     1     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x10]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(ntm.parameters(), momentum=0.9,\n",
    "                          alpha=0.95, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "Epoch Number: 0, Batch Number: 25, Loss: 0.0000\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 0, Batch Number: 50, Loss: 0.0000\n",
      "Time so far is 0m 3s\n",
      "Epoch Number: 0, Batch Number: 75, Loss: 0.0000\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 0, Batch Number: 100, Loss: 0.0000\n",
      "Time so far is 0m 5s\n",
      "Epoch Number: 0, Batch Number: 125, Loss: 0.0000\n",
      "Time so far is 0m 7s\n",
      "Epoch Number: 0, Batch Number: 150, Loss: 0.0000\n",
      "Time so far is 0m 8s\n",
      "Epoch Number: 0, Batch Number: 175, Loss: 0.0000\n",
      "Time so far is 0m 10s\n",
      "Epoch Number: 0, Batch Number: 200, Loss: 0.0000\n",
      "Time so far is 0m 11s\n",
      "Epoch Number: 0, Batch Number: 225, Loss: 0.0000\n",
      "Time so far is 0m 12s\n",
      "Epoch Number: 0, Batch Number: 250, Loss: 0.0000\n",
      "Time so far is 0m 14s\n",
      "Epoch Number: 0, Batch Number: 275, Loss: 0.0000\n",
      "Time so far is 0m 15s\n",
      "Epoch Number: 0, Batch Number: 300, Loss: 0.0000\n",
      "Time so far is 0m 16s\n",
      "Epoch Number: 0, Batch Number: 325, Loss: 0.0000\n",
      "Time so far is 0m 18s\n",
      "Epoch Number: 0, Batch Number: 350, Loss: 0.0000\n",
      "Time so far is 0m 19s\n",
      "Epoch Number: 0, Batch Number: 375, Loss: 0.0000\n",
      "Time so far is 0m 20s\n",
      "Epoch Number: 0, Batch Number: 400, Loss: 0.0000\n",
      "Time so far is 0m 22s\n",
      "Epoch Number: 0, Batch Number: 425, Loss: 0.0000\n",
      "Time so far is 0m 23s\n",
      "Epoch Number: 0, Batch Number: 450, Loss: 0.0000\n",
      "Time so far is 0m 25s\n",
      "Epoch Number: 0, Batch Number: 475, Loss: 0.0000\n",
      "Time so far is 0m 26s\n",
      "Epoch Number: 0, Batch Number: 500, Loss: 0.0000\n",
      "Time so far is 0m 27s\n",
      "Epoch Number: 0, Batch Number: 525, Loss: 0.0000\n",
      "Time so far is 0m 29s\n",
      "Epoch Number: 0, Batch Number: 550, Loss: 0.0000\n",
      "Time so far is 0m 30s\n",
      "Epoch Number: 0, Batch Number: 575, Loss: 0.0000\n",
      "Time so far is 0m 31s\n",
      "Epoch Number: 0, Batch Number: 600, Loss: 0.0000\n",
      "Time so far is 0m 33s\n",
      "Epoch Number: 0, Batch Number: 625, Loss: 0.0000\n",
      "Time so far is 0m 34s\n",
      "Epoch Number: 0, Batch Number: 650, Loss: 0.0000\n",
      "Time so far is 0m 35s\n",
      "Epoch Number: 0, Batch Number: 675, Loss: 0.0000\n",
      "Time so far is 0m 37s\n",
      "Epoch Number: 0, Batch Number: 700, Loss: 0.0000\n",
      "Time so far is 0m 38s\n",
      "Epoch Number: 0, Batch Number: 725, Loss: 0.0000\n",
      "Time so far is 0m 39s\n",
      "Epoch Number: 0, Batch Number: 750, Loss: 0.0000\n",
      "Time so far is 0m 41s\n",
      "Epoch Number: 0, Batch Number: 775, Loss: 0.0000\n",
      "Time so far is 0m 42s\n",
      "Epoch Number: 0, Batch Number: 800, Loss: 0.0000\n",
      "Time so far is 0m 44s\n",
      "Epoch Number: 0, Batch Number: 825, Loss: 0.0000\n",
      "Time so far is 0m 45s\n",
      "Epoch Number: 0, Batch Number: 850, Loss: 0.0000\n",
      "Time so far is 0m 46s\n",
      "Epoch Number: 0, Batch Number: 875, Loss: 0.0000\n",
      "Time so far is 0m 48s\n",
      "Epoch Number: 0, Batch Number: 900, Loss: 0.0000\n",
      "Time so far is 0m 49s\n",
      "Epoch Number: 0, Batch Number: 925, Loss: 0.0000\n",
      "Time so far is 0m 50s\n",
      "Epoch Number: 0, Batch Number: 950, Loss: 0.0000\n",
      "Time so far is 0m 52s\n",
      "Epoch Number: 0, Batch Number: 975, Loss: 0.0000\n",
      "Time so far is 0m 53s\n",
      "Epoch Number: 0, Batch Number: 1000, Loss: 0.0000\n",
      "Time so far is 0m 54s\n",
      "Epoch Number: 0, Batch Number: 1025, Loss: 0.0000\n",
      "Time so far is 0m 56s\n",
      "Epoch Number: 0, Batch Number: 1050, Loss: 0.0000\n",
      "Time so far is 0m 57s\n",
      "Epoch Number: 0, Batch Number: 1075, Loss: 0.0000\n",
      "Time so far is 0m 59s\n",
      "Epoch Number: 0, Batch Number: 1100, Loss: 0.0000\n",
      "Time so far is 0m 60s\n",
      "Epoch Number: 0, Batch Number: 1125, Loss: 0.0000\n",
      "Time so far is 1m 2s\n",
      "Epoch Number: 0, Batch Number: 1150, Loss: 0.0000\n",
      "Time so far is 1m 3s\n",
      "Epoch Number: 0, Batch Number: 1175, Loss: 0.0000\n",
      "Time so far is 1m 5s\n",
      "Epoch Number: 0, Batch Number: 1200, Loss: 0.0000\n",
      "Time so far is 1m 6s\n",
      "Epoch Number: 0, Batch Number: 1225, Loss: 0.0000\n",
      "Time so far is 1m 8s\n",
      "Epoch Number: 0, Batch Number: 1250, Loss: 0.0000\n",
      "Time so far is 1m 10s\n",
      "Epoch Number: 0, Batch Number: 1275, Loss: 0.0000\n",
      "Time so far is 1m 11s\n",
      "Epoch Number: 0, Batch Number: 1300, Loss: 0.0000\n",
      "Time so far is 1m 13s\n",
      "Epoch Number: 0, Batch Number: 1325, Loss: 0.0000\n",
      "Time so far is 1m 14s\n",
      "Epoch Number: 0, Batch Number: 1350, Loss: 0.0000\n",
      "Time so far is 1m 16s\n",
      "Epoch Number: 0, Batch Number: 1375, Loss: 0.0000\n",
      "Time so far is 1m 17s\n",
      "Epoch Number: 0, Batch Number: 1400, Loss: 0.0000\n",
      "Time so far is 1m 19s\n",
      "Epoch Number: 0, Batch Number: 1425, Loss: 0.0000\n",
      "Time so far is 1m 20s\n",
      "Epoch Number: 0, Batch Number: 1450, Loss: 0.0000\n",
      "Time so far is 1m 22s\n",
      "Epoch Number: 0, Batch Number: 1475, Loss: 0.0000\n",
      "Time so far is 1m 23s\n",
      "Epoch Number: 0, Batch Number: 1500, Loss: 0.0000\n",
      "Time so far is 1m 25s\n",
      "Epoch Number: 0, Batch Number: 1525, Loss: 0.0000\n",
      "Time so far is 1m 26s\n",
      "Epoch Number: 0, Batch Number: 1550, Loss: 0.0000\n",
      "Time so far is 1m 27s\n",
      "Epoch Number: 0, Batch Number: 1575, Loss: 0.0000\n",
      "Time so far is 1m 29s\n",
      "Epoch Number: 0, Batch Number: 1600, Loss: 0.0000\n",
      "Time so far is 1m 30s\n",
      "Epoch Number: 0, Batch Number: 1625, Loss: 0.0000\n",
      "Time so far is 1m 32s\n",
      "Epoch Number: 0, Batch Number: 1650, Loss: 0.0000\n",
      "Time so far is 1m 33s\n",
      "Epoch Number: 0, Batch Number: 1675, Loss: 0.0000\n",
      "Time so far is 1m 35s\n",
      "Epoch Number: 0, Batch Number: 1700, Loss: 0.0000\n",
      "Time so far is 1m 36s\n",
      "Epoch Number: 0, Batch Number: 1725, Loss: 0.0000\n",
      "Time so far is 1m 37s\n",
      "Epoch Number: 0, Batch Number: 1750, Loss: 0.0000\n",
      "Time so far is 1m 39s\n",
      "Epoch Number: 0, Batch Number: 1775, Loss: 0.0000\n",
      "Time so far is 1m 40s\n",
      "Epoch Number: 0, Batch Number: 1800, Loss: 0.0000\n",
      "Time so far is 1m 42s\n",
      "Epoch Number: 0, Batch Number: 1825, Loss: 0.0000\n",
      "Time so far is 1m 43s\n",
      "Epoch Number: 0, Batch Number: 1850, Loss: 0.0000\n",
      "Time so far is 1m 45s\n",
      "Epoch Number: 0, Batch Number: 1875, Loss: 0.0000\n",
      "Time so far is 1m 46s\n",
      "Epoch Number: 0, Batch Number: 1900, Loss: 0.0000\n",
      "Time so far is 1m 48s\n",
      "Epoch Number: 0, Batch Number: 1925, Loss: 0.0000\n",
      "Time so far is 1m 49s\n",
      "Epoch Number: 0, Batch Number: 1950, Loss: 0.0000\n",
      "Time so far is 1m 51s\n",
      "Epoch Number: 0, Batch Number: 1975, Loss: 0.0000\n",
      "Time so far is 1m 52s\n",
      "Epoch Number: 0, Batch Number: 2000, Loss: 0.0000\n",
      "Time so far is 1m 54s\n",
      "\n",
      "Training complete in 1m 54s\n",
      "Best loss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "best_model, train_plot_losses, validation_plot_losses = training.train_model(ntm, data_loader, addition_task_loss, optimizer, None, num_epochs=1, print_every=25, deep_copy_desired=False, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1142b7cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0, 2000, 20), train_plot_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 10000, 100), validation_plot_losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
