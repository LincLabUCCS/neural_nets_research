{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "n_input = 8 # Copy task (pg 10, https://arxiv.org/pdf/1410.5401.pdf)\n",
    "n_hidden = 50 # Kinda random...\n",
    "n_output = 8 # Copy task\n",
    "\n",
    "class NTM(nn.Module):\n",
    "    def __init__(self, mem_length, mem_width, read_heads, write_heads, key_strength):\n",
    "        super(NTM, self).__init__()\n",
    "        \n",
    "        # Save useful params\n",
    "        self.read_heads = read_heads\n",
    "        self.write_heads = write_heads\n",
    "        self.key_strength = key_strength      \n",
    "        self.M = mem_length\n",
    "        self.N = mem_width\n",
    "        \n",
    "        # Initialize controller\n",
    "        self.lstm = myLSTM(n_input, [n_hidden, n_hidden], n_output, 2)\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = Variable(torch.zeros(mem_length, mem_width))\n",
    "        # Initialize read_weights\n",
    "        self.read_weights = Variable(torch.zeros(mem_length))\n",
    "        \n",
    "        # Functions we'll need later\n",
    "        self.cosine_similarity = nn.CosineSimilarity()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "    # Equation 2\n",
    "    def read(self, weight_vec):\n",
    "        # weight_vec is N; memory is NxM\n",
    "        return torch.matmul(self.weight_vec * self.memory)\n",
    "        \n",
    "    def write(self, weight_vec, erase_vec, add_vec):\n",
    "        erase_matrix = erase(weight_vec, erase_vec)\n",
    "        self.add(weight_vec, add_vec, erase_matrix)\n",
    "    \n",
    "    # Equation 3\n",
    "    def erase(self, weight_vec, erase_vec):\n",
    "        # weight_vec is size N; erase_vec is M\n",
    "        erase_weighting = torch.matmul(weight_vec, erase_vec.unsqueeze(0))\n",
    "        # erase_wighting is NxM; memory is NxM\n",
    "        erase_weighting = torch.ones(self.N, self.M) - erase_weighting\n",
    "        return self.memory * erase_weighting\n",
    "    \n",
    "    # Equation 4\n",
    "    def add(self, weight_vec, add_vec, erase_matrix):\n",
    "        # weight_vec is size N; add_vec is M\n",
    "        add_weighting = torch.matmul(weight_vec, add_vec.unsqueeze(0))\n",
    "        # add_wighting is NxM; erase_matrix is NxM\n",
    "        return self.erase_matrix + add_weighting\n",
    "      \n",
    "    def read_head(self, key_vecs, interpolation_gates, gammas, shift_weights):  \n",
    "        read_vecs = torch.Tensor(self.read_heads)\n",
    "        for i in range(self.read_heads):\n",
    "            content_weights = self.content_focus(key_vecs[i], self.key_strength)\n",
    "            combined_weights = self.location_focus(interpolation_gates[i], \n",
    "                                                   gammas[i], \n",
    "                                                   self.read_weights[i], \n",
    "                                                   self.content_weights[i], \n",
    "                                                   shift_weights[i])\n",
    "            self.read_weights[i] = combined_weights\n",
    "            read_vecs[i] = self.read(self.read_weights[i])\n",
    "        return read_vecs\n",
    "    \n",
    "    def write_head(self, key_vecs, interpolation_gates, gammas, shift_weights, erase_vecs, add_vecs):\n",
    "        for i in range(self.write_heads):\n",
    "            content_weights = self.content_focus(key_vecs[i], self.key_strength)\n",
    "            combined_weights = self.location_focus(interpolation_gates[i], \n",
    "                                                   gammas[i], \n",
    "                                                   self.write_weights[i], \n",
    "                                                   self.content_weights[i], \n",
    "                                                   shift_weights[i])\n",
    "            self.write_weights[i] = combined_weights\n",
    "            self.write(self.read_weights[i], erase_vec[i], add_vec[i])\n",
    "    \n",
    "    # Equations 5, 6\n",
    "    def content_focus(self, key_vec, key_strength):\n",
    "        # TODO - check that this operation is actually doing what we want!\n",
    "        # key_vec.unsqueeze(0) is 1xM; memory is NxM\n",
    "        sim_vec = self.cosine_similarity(key_vec.unsqueeze(0), self.memory)\n",
    "        # sim_vec is a matrix of length N\n",
    "        sim_vec = key_strength * sim_vec\n",
    "        return self.softmax(sim_vec)\n",
    "     \n",
    "    def location_focus(self, g, gamma, old_weight, content_weight, shift_weights):\n",
    "        # Equation 7\n",
    "        gated_weight = g * content_weight + (1- g) * self.old_weight\n",
    "        \n",
    "        # Equation 8\n",
    "        w_bar = torch.zeros(self.M)\n",
    "        for i in range(self.M):\n",
    "            for j in range(self.N):\n",
    "                w_bar[i] += gated_weight[j] * shift_weights[i-j]\n",
    "        \n",
    "        # Equation 9\n",
    "        weights_to_power = torch.pow(x, gamma)\n",
    "        weights_power_sum = torch.sum(weights_to_power)\n",
    "        return torch.mul(weights_to_power, 1/weights_power_sum)\n",
    "    \n",
    "    def parse_lstm_output(output):\n",
    "        pass\n",
    "        # TODO - get needed read/write head info from the controller. We need:\n",
    "        \n",
    "        #key_vecs (rh x M), interpolation_gates (rh), gammas (rh), shift_weights (rh x M)\n",
    "                                 \n",
    "        #key_vecs (wh x M), interpolation_gates (wh), gammas (wh), shift_weights (wh x M), erase_vecs (wh x M), add_vecs(wh x M),\n",
    "        \n",
    "        # rh=read_heads; wh=write_heads\n",
    "    \n",
    "    def backProp():\n",
    "        pass\n",
    "        \n",
    "    def forward(self, input, label):\n",
    "        \n",
    "        # Concatenate inputs\n",
    "        read_vec = torch.cat(self.read_vecs)\n",
    "        combined_input = torch.cat(input, read_vec)\n",
    "        \n",
    "        # Call controller, get output\n",
    "        output = lstm(combined_input)\n",
    "        \n",
    "        # Parse output\n",
    "        self.parse_lstm_output(output)\n",
    "\n",
    "        # Write\n",
    "        self.write_head()\n",
    "        self.read_head() # We should throw params in here.\n",
    "        \n",
    "        \n",
    "# Create train function down here\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class MyLSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyLSTMLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.forgetGate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.incorporatePositionGate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.incorporateValueGate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.hiddenValueGate = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward_step(self, input, hidden, cell_state):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        \n",
    "        f = self.sigmoid(self.forgetGate(combined))\n",
    "        i = self.sigmoid(self.incorporatePositionGate(combined))\n",
    "        C_new = self.tanh(self.incorporateValueGate(combined))\n",
    "        \n",
    "        cell_state = f * cell_state + i * C_new\n",
    "        \n",
    "        hidden = self.relu(self.hiddenValueGate(cell_state))\n",
    "        \n",
    "        return hidden, cell_state\n",
    "    \n",
    "    def forward(self, input):\n",
    "        hidden, cell_state = self.initAll()\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(input.size()[0]):\n",
    "            hidden, cell_state = self.forward_step(input[i], hidden, cell_state)\n",
    "            outputs.append(hidden)\n",
    "        \n",
    "        return torch.stack(outputs)\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size)).cuda()\n",
    "    \n",
    "    def initCellState(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size)).cuda()\n",
    "    \n",
    "    def initAll(self):\n",
    "        return self.initHidden(), self.initCellState()\n",
    "\n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, layers):\n",
    "        super(myLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        input_sizes = [input_size] + hidden_sizes[:-1] \n",
    "        \n",
    "        for input_size, hidden_size in zip(input_sizes, hidden_sizes):\n",
    "            self.lstm_layers.append(MyLSTMLayer(input_size, hidden_size))\n",
    "        \n",
    "        self.outputGate = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input):\n",
    "        hiddens = input\n",
    "        \n",
    "        for i, lstm_layer in enumerate(self.lstm_layers):\n",
    "            hiddens = lstm_layer(hiddens)\n",
    "        \n",
    "        return self.softmax(self.outputGate(hiddens[-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
