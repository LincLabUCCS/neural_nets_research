{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Transfering a model from PyTorch to Caffe2 and Mobile using ONNX\n",
    "================================================================\n",
    "\n",
    "In this tutorial, we describe how to use ONNX to convert a model defined\n",
    "in PyTorch into the ONNX format and then load it into Caffe2. Once in\n",
    "Caffe2, we can run the model to double-check it was exported correctly,\n",
    "and we then show how to use Caffe2 features such as mobile exporter for\n",
    "executing the model on mobile devices.\n",
    "\n",
    "For this tutorial, you will need to install `onnx <https://github.com/onnx/onnx>`__,\n",
    "`onnx-caffe2 <https://github.com/onnx/onnx-caffe2>`__ and `Caffe2 <https://caffe2.ai/>`__.\n",
    "You can get binary builds of onnx and onnx-caffe2 with\n",
    "``conda install -c ezyang onnx onnx-caffe2``.\n",
    "\n",
    "``NOTE``: This tutorial needs PyTorch master branch which can be installed by following\n",
    "the instructions `here <https://github.com/pytorch/pytorch#from-source>`__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super-resolution is a way of increasing the resolution of images, videos\n",
    "and is widely used in image processing or video editing. For this\n",
    "tutorial, we will first use a small super-resolution model with a dummy\n",
    "input.\n",
    "\n",
    "First, let's create a SuperResolution model in PyTorch. `This\n",
    "model <https://github.com/pytorch/examples/blob/master/super_resolution/model.py>`__\n",
    "comes directly from PyTorch's examples without modification:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super Resolution model definition in PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, upscale_factor, inplace=False):\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.orthogonal(self.conv1.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal(self.conv2.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal(self.conv3.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal(self.conv4.weight)\n",
    "\n",
    "# Create the super-resolution model by using the above model definition.\n",
    "torch_model = SuperResolutionNet(upscale_factor=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinarily, you would now train this model; however, for this tutorial,\n",
    "we will instead download some pre-trained weights. Note that this model\n",
    "was not trained fully for good accuracy and is used here for\n",
    "demonstration purposes only.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperResolutionNet(\n",
       "  (relu): ReLU()\n",
       "  (conv1): Conv2d (1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d (64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d (32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pixel_shuffle): PixelShuffle(upscale_factor=3)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model weights\n",
    "model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\n",
    "batch_size = 1    # just a random number\n",
    "\n",
    "# Initialize model with the pretrained weights\n",
    "map_location = lambda storage, loc: storage\n",
    "if torch.cuda.is_available():\n",
    "    map_location = None\n",
    "torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n",
    "\n",
    "# set the train mode to false since we will only run the forward pass.\n",
    "torch_model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting a model in PyTorch works via tracing. To export a model, you\n",
    "call the ``torch.onnx._export()`` function. This will execute the model,\n",
    "recording a trace of what operators are used to compute the outputs.\n",
    "Because ``_export`` runs the model, we need provide an input tensor\n",
    "``x``. The values in this tensor are not important; it can be an image\n",
    "or a random tensor as long as it is the right size.\n",
    "\n",
    "To learn more details about PyTorch's export interface, check out the\n",
    "`torch.onnx documentation <http://pytorch.org/docs/master/onnx.html>`__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to the model\n",
    "x = Variable(torch.randn(batch_size, 1, 224, 224), requires_grad=True)\n",
    "\n",
    "# Export the model\n",
    "torch_out = torch.onnx._export(torch_model,             # model being run\n",
    "                               x,                       # model input (or a tuple for multiple inputs)\n",
    "                               \"super_resolution.onnx\", # where to save the model (can be a file or file-like object)\n",
    "                               export_params=True)      # store the trained parameter weights inside the model file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``torch_out`` is the output after executing the model. Normally you can\n",
    "ignore this output, but here we will use it to verify that the model we\n",
    "exported computes the same values when run in Caffe2.\n",
    "\n",
    "Now let's take the ONNX representation and use it in Caffe2. This part\n",
    "can normally be done in a separate process or on another machine, but we\n",
    "will continue in the same process so that we can verify that Caffe2 and\n",
    "PyTorch are computing the same value for the network:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "222\n",
      "333\n",
      "444\n",
      "Exported model has been executed on Caffe2 backend, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx_caffe2.backend\n",
    "\n",
    "print(\"111\")\n",
    "\n",
    "# Load the ONNX ModelProto object. model is a standard Python protobuf object\n",
    "model = onnx.load(\"super_resolution.onnx\")\n",
    "\n",
    "print(\"222\")\n",
    "\n",
    "# prepare the caffe2 backend for executing the model this converts the ONNX model into a\n",
    "# Caffe2 NetDef that can execute it. Other ONNX backends, like one for CNTK will be\n",
    "# availiable soon.\n",
    "prepared_backend = onnx_caffe2.backend.prepare(model)\n",
    "\n",
    "print(\"333\")\n",
    "\n",
    "# run the model in Caffe2\n",
    "\n",
    "# Construct a map from input names to Tensor data.\n",
    "# The graph of the model itself contains inputs for all weight parameters, after the input image.\n",
    "# Since the weights are already embedded, we just need to pass the input image.\n",
    "# Set the first input.\n",
    "W = {model.graph.input[0].name: x.data.numpy()}\n",
    "\n",
    "print(\"444\")\n",
    "\n",
    "# Run the Caffe2 net:\n",
    "c2_out = prepared_backend.run(W)[0]\n",
    "\n",
    "# Verify the numerical correctness upto 3 decimal places\n",
    "np.testing.assert_almost_equal(torch_out.data.cpu().numpy(), c2_out, decimal=3)\n",
    "\n",
    "print(\"Exported model has been executed on Caffe2 backend, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see that the output of PyTorch and Caffe2 runs match\n",
    "numerically up to 3 decimal places. As a side-note, if they do not match\n",
    "then there is an issue that the operators in Caffe2 and PyTorch are\n",
    "implemented differently and please contact us in that case.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfering SRResNet using ONNX\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same process as above, we also transferred an interesting new\n",
    "model \"SRResNet\" for super-resolution presented in `this\n",
    "paper <https://arxiv.org/pdf/1609.04802.pdf>`__ (thanks to the authors\n",
    "at Twitter for providing us code and pretrained parameters for the\n",
    "purpose of this tutorial). The model definition and a pre-trained model\n",
    "can be found\n",
    "`here <https://gist.github.com/prigoyal/b245776903efbac00ee89699e001c9bd>`__.\n",
    "Below is what SRResNet model input, output looks like. |SRResNet|\n",
    "\n",
    ".. |SRResNet| image:: /_static/img/SRResNet.png\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model on mobile devices\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have exported a model from PyTorch and shown how to load it\n",
    "and run it in Caffe2. Now that the model is loaded in Caffe2, we can\n",
    "convert it into a format suitable for `running on mobile\n",
    "devices <https://caffe2.ai/docs/mobile-integration.html>`__.\n",
    "\n",
    "We will use Caffe2's\n",
    "`mobile\\_exporter <https://github.com/caffe2/caffe2/blob/master/caffe2/python/predictor/mobile_exporter.py>`__\n",
    "to generate the two model protobufs that can run on mobile. The first is\n",
    "used to initialize the network with the correct weights, and the second\n",
    "actual runs executes the model. We will continue to use the small\n",
    "super-resolution model for the rest of this tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# extract the workspace and the model proto from the internal representation\n",
    "c2_workspace = prepared_backend.workspace\n",
    "c2_model = prepared_backend.predict_net\n",
    "\n",
    "# Now import the caffe2 mobile exporter\n",
    "from caffe2.python.predictor import mobile_exporter\n",
    "\n",
    "# call the Export to get the predict_net, init_net. These nets are needed for running things on mobile\n",
    "init_net, predict_net = mobile_exporter.Export(c2_workspace, c2_model, c2_model.external_input)\n",
    "\n",
    "# Let's also save the init_net and predict_net to a file that we will later use for running them on mobile\n",
    "with open('init_net.pb', \"wb\") as fopen:\n",
    "    fopen.write(init_net.SerializeToString())\n",
    "with open('predict_net.pb', \"wb\") as fopen:\n",
    "    fopen.write(predict_net.SerializeToString())\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``init_net`` has the model parameters and the model input embedded in it\n",
    "and ``predict_net`` will be used to guide the ``init_net`` execution at\n",
    "run-time. In this tutorial, we will use the ``init_net`` and\n",
    "``predict_net`` generated above and run them in both normal Caffe2\n",
    "backend and mobile and verify that the output high-resolution cat image\n",
    "produced in both runs is the same.\n",
    "\n",
    "For this tutorial, we will use a famous cat image used widely which\n",
    "looks like below\n",
    "\n",
    ".. figure:: /_static/img/cat_224x224.jpg\n",
    "   :alt: cat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "from caffe2.proto import caffe2_pb2\n",
    "from caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "from skimage import io, transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the image, pre-process it using standard skimage\n",
    "python library. Note that this preprocessing is the standard practice of\n",
    "processing data for training/testing neural networks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    }
   ],
   "source": [
    "# load the image\n",
    "img_in = io.imread(\"cat.jpg\")\n",
    "\n",
    "# resize the image to dimensions 224x224\n",
    "img = transform.resize(img_in, [224, 224])\n",
    "\n",
    "# save this resized image to be used as input to the model\n",
    "io.imsave(\"resized_cat.jpg\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as a next step, let's take the resized cat image and run the\n",
    "super-resolution model in Caffe2 backend and save the output image. The\n",
    "image processing steps below have been adopted from PyTorch\n",
    "implementation of super-resolution model\n",
    "`here <https://github.com/pytorch/examples/blob/master/super_resolution/super_resolve.py>`__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch-jit-export = core.Net('torch-jit-export')\n",
      "torch-jit-export.Conv(['1', '2'], ['11'], pads=[2, 2, 2, 2], dilations=[1, 1], group=1, strides=[1, 1], kernels=[5, 5])\n",
      "torch-jit-export.Add(['11', '3'], ['12'], broadcast=1, axis=1)\n",
      "torch-jit-export.Relu(['12'], ['13'])\n",
      "torch-jit-export.Conv(['13', '4'], ['15'], pads=[1, 1, 1, 1], dilations=[1, 1], group=1, strides=[1, 1], kernels=[3, 3])\n",
      "torch-jit-export.Add(['15', '5'], ['16'], broadcast=1, axis=1)\n",
      "torch-jit-export.Relu(['16'], ['17'])\n",
      "torch-jit-export.Conv(['17', '6'], ['19'], pads=[1, 1, 1, 1], dilations=[1, 1], group=1, strides=[1, 1], kernels=[3, 3])\n",
      "torch-jit-export.Add(['19', '7'], ['20'], broadcast=1, axis=1)\n",
      "torch-jit-export.Relu(['20'], ['21'])\n",
      "torch-jit-export.Conv(['21', '8'], ['23'], pads=[1, 1, 1, 1], dilations=[1, 1], group=1, strides=[1, 1], kernels=[3, 3])\n",
      "torch-jit-export.Add(['23', '9'], ['24'], broadcast=1, axis=1)\n",
      "torch-jit-export.Reshape(['24'], ['25', 'OC2_DUMMY_0'], shape=[1, 1, 3, 3, 224, 224])\n",
      "torch-jit-export.Transpose(['25'], ['26'], axes=[0, 1, 4, 2, 5, 3])\n",
      "torch-jit-export.Reshape(['26'], ['27', 'OC2_DUMMY_1'], shape=[1, 1, 672, 672])\n"
     ]
    }
   ],
   "source": [
    "# load the resized image and convert it to Ybr format\n",
    "img = Image.open(\"resized_cat.jpg\")\n",
    "img_ycbcr = img.convert('YCbCr')\n",
    "img_y, img_cb, img_cr = img_ycbcr.split()\n",
    "\n",
    "# Let's run the mobile nets that we generated above so that caffe2 workspace is properly initialized\n",
    "workspace.RunNetOnce(init_net)\n",
    "workspace.RunNetOnce(predict_net)\n",
    "\n",
    "# Caffe2 has a nice net_printer to be able to inspect what the net looks like and identify\n",
    "# what our input and output blob names are.\n",
    "print(net_printer.to_string(predict_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we can see that input is named \"9\" and output is\n",
    "named \"27\"(it is a little bit weird that we will have numbers as blob\n",
    "names but this is because the tracing JIT produces numbered entries for\n",
    "the models)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked?  True\n",
      "imgy <PIL.Image.Image image mode=L size=224x224 at 0x7F832C5A7DA0>\n",
      "ugh [[[[  55.   50.   49. ...,  135.  142.  146.]\n",
      "   [  52.   49.   49. ...,  136.  141.  145.]\n",
      "   [  51.   47.   47. ...,  133.  137.  140.]\n",
      "   ..., \n",
      "   [ 170.  172.  163. ...,  176.  170.  155.]\n",
      "   [ 163.  164.  159. ...,  204.  194.  172.]\n",
      "   [ 163.  162.  166. ...,  207.  196.  170.]]]]\n",
      "Traceback for operator 10 in network torch-jit-export\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at elementwise_op.h:127] axis >= 0 && axis <= A.ndim() - B.ndim(). Broadcast axis should be in the range of[0, A.ndim() - B.ndim()], but axis = 1 Error from operator: \ninput: \"23\" input: \"9\" output: \"24\" name: \"\" type: \"Add\" arg { name: \"broadcast\" i: 1 } arg { name: \"axis\" i: 1 } device_option { device_type: 0 cuda_gpu_id: 0 }",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-21e1dc35fa5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#print(\"test\", thing.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# run the predict_net to get the model output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunNetOnce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Now let's get the model output blob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programming/caffe2/build/caffe2/python/workspace.py\u001b[0m in \u001b[0;36mRunNetOnce\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWorkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_failed_op_net_position\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mGetNetName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mStringifyProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     )\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programming/caffe2/build/caffe2/python/workspace.py\u001b[0m in \u001b[0;36mCallWithExceptionIntercept\u001b[0;34m(func, op_id_fetcher, net_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCallWithExceptionIntercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_id_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mop_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_id_fetcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at elementwise_op.h:127] axis >= 0 && axis <= A.ndim() - B.ndim(). Broadcast axis should be in the range of[0, A.ndim() - B.ndim()], but axis = 1 Error from operator: \ninput: \"23\" input: \"9\" output: \"24\" name: \"\" type: \"Add\" arg { name: \"broadcast\" i: 1 } arg { name: \"axis\" i: 1 } device_option { device_type: 0 cuda_gpu_id: 0 }"
     ]
    }
   ],
   "source": [
    "# Now, let's also pass in the resized cat image for processing by the model.\n",
    "#9\n",
    "worked = workspace.FeedBlob(\"9\", np.array(img_y)[np.newaxis, np.newaxis, :, :].astype(np.float32))\n",
    "print(\"worked? \", worked)\n",
    "print(\"imgy\", img_y)\n",
    "print(\"ugh\", np.array(img_y)[np.newaxis, np.newaxis, :, :].astype(np.float32))\n",
    "thing = np.array(img_y)[np.newaxis, np.newaxis, :, :].astype(np.float32)\n",
    "#print(\"test\", thing.size())\n",
    "# run the predict_net to get the model output\n",
    "workspace.RunNetOnce(predict_net)\n",
    "\n",
    "# Now let's get the model output blob\n",
    "img_out = workspace.FetchBlob(\"27\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll refer back to the post-processing steps in PyTorch\n",
    "implementation of super-resolution model\n",
    "`here <https://github.com/pytorch/examples/blob/master/super_resolution/super_resolve.py>`__\n",
    "to construct back the final output image and save the image.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_out_y = Image.fromarray(np.uint8((img_out[0, 0]).clip(0, 255)), mode='L')\n",
    "\n",
    "# get the output image follow post-processing step from PyTorch implementation\n",
    "final_img = Image.merge(\n",
    "    \"YCbCr\", [\n",
    "        img_out_y,\n",
    "        img_cb.resize(img_out_y.size, Image.BICUBIC),\n",
    "        img_cr.resize(img_out_y.size, Image.BICUBIC),\n",
    "    ]).convert(\"RGB\")\n",
    "\n",
    "# Save the image, we will compare this with the output image from mobile device\n",
    "final_img.save(\"./_static/img/cat_superres.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have finished running our mobile nets in pure Caffe2 backend and now,\n",
    "let's execute the model on an Android device and get the model output.\n",
    "\n",
    "``NOTE``: for Android development, ``adb`` shell is needed otherwise the\n",
    "following section of tutorial will not run.\n",
    "\n",
    "In our first step of runnig model on mobile, we will push a native speed\n",
    "benchmark binary for mobile device to adb. This binary can execute the\n",
    "model on mobile and also export the model output that we can retrieve\n",
    "later. The binary is available\n",
    "`here <https://github.com/caffe2/caffe2/blob/master/caffe2/binaries/speed_benchmark.cc>`__.\n",
    "In order to build the binary, execute the ``build_android.sh`` script\n",
    "following the instructions\n",
    "`here <https://github.com/caffe2/caffe2/blob/master/scripts/build_android.sh>`__.\n",
    "\n",
    "``NOTE``: You need to have ``ANDROID_NDK`` installed and set your env\n",
    "variable ``ANDROID_NDK=path to ndk root``\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first push a bunch of stuff to adb, specify the path for the binary\n",
    "CAFFE2_MOBILE_BINARY = ('caffe2/binaries/speed_benchmark')\n",
    "\n",
    "# we had saved our init_net and proto_net in steps above, we use them now.\n",
    "# Push the binary and the model protos\n",
    "os.system('adb push ' + CAFFE2_MOBILE_BINARY + ' /data/local/tmp/')\n",
    "os.system('adb push init_net.pb /data/local/tmp')\n",
    "os.system('adb push predict_net.pb /data/local/tmp')\n",
    "\n",
    "# Let's serialize the input image blob to a blob proto and then send it to mobile for execution.\n",
    "with open(\"input.blobproto\", \"wb\") as fid:\n",
    "    fid.write(workspace.SerializeBlob(\"9\"))\n",
    "\n",
    "# push the input image blob to adb\n",
    "os.system('adb push input.blobproto /data/local/tmp/')\n",
    "\n",
    "# Now we run the net on mobile, look at the speed_benchmark --help for what various options mean\n",
    "os.system(\n",
    "    'adb shell /data/local/tmp/speed_benchmark '                     # binary to execute\n",
    "    '--init_net=/data/local/tmp/super_resolution_mobile_init.pb '    # mobile init_net\n",
    "    '--net=/data/local/tmp/super_resolution_mobile_predict.pb '      # mobile predict_net\n",
    "    '--input=9 '                                                     # name of our input image blob\n",
    "    '--input_file=/data/local/tmp/input.blobproto '                  # serialized input image\n",
    "    '--output_folder=/data/local/tmp '                               # destination folder for saving mobile output\n",
    "    '--output=27,9 '                                                 # output blobs we are interested in\n",
    "    '--iter=1 '                                                      # number of net iterations to execute\n",
    "    '--caffe2_log_level=0 '\n",
    ")\n",
    "\n",
    "# get the model output from adb and save to a file\n",
    "os.system('adb pull /data/local/tmp/27 ./output.blobproto')\n",
    "\n",
    "\n",
    "# We can recover the output content and post-process the model using same steps as we followed earlier\n",
    "blob_proto = caffe2_pb2.BlobProto()\n",
    "blob_proto.ParseFromString(open('./output.blobproto').read())\n",
    "img_out = utils.Caffe2TensorToNumpyArray(blob_proto.tensor)\n",
    "img_out_y = Image.fromarray(np.uint8((img_out[0,0]).clip(0, 255)), mode='L')\n",
    "final_img = Image.merge(\n",
    "    \"YCbCr\", [\n",
    "        img_out_y,\n",
    "        img_cb.resize(img_out_y.size, Image.BICUBIC),\n",
    "        img_cr.resize(img_out_y.size, Image.BICUBIC),\n",
    "    ]).convert(\"RGB\")\n",
    "final_img.save(\"./_static/img/cat_superres_mobile.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can compare the image ``cat_superres.jpg`` (model output from\n",
    "pure caffe2 backend execution) and ``cat_superres_mobile.jpg`` (model\n",
    "output from mobile execution) and see that both the images look same. If\n",
    "they don't look same, something went wrong with execution on mobile and\n",
    "in that case, please contact Caffe2 community. You should expect to see\n",
    "the output image to look like following:\n",
    "\n",
    ".. figure:: /_static/img/cat_output1.png\n",
    "   :alt: output\\_cat\n",
    "\n",
    "\n",
    "Using the above steps, you can deploy your models on mobile easily.\n",
    "Also, for more information on caffe2 mobile backend, checkout\n",
    "`caffe2-android-demo <https://caffe2.ai/docs/AI-Camera-demo-android.html>`__.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
