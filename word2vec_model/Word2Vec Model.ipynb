{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import torch.utils.data as data\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from gensim import utils\n",
    "from huffman_tree import HuffmanTree\n",
    "from hierarchical_softmax import HierarchicalSoftmax\n",
    "import hierarchical_softmax\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_directory = os.path.expanduser('~')\n",
    "nn_library_path = home_directory + '/Documents/HarveyMuddWork/Neural_Nets_Research/neural_nets_research/Neural Nets Library'\n",
    "sys.path.append(nn_library_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, huffman_tree):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size, sparse=True)\n",
    "        self.softmax_layer = HierarchicalSoftmax(huffman_tree)\n",
    "    \n",
    "    def forward(self, context, id_list):\n",
    "        context_vector = torch.sum(self.embeddings(context), dim=1).squeeze()\n",
    "        probabilities = self.softmax_layer(context_vector, id_list.squeeze())\n",
    "        \n",
    "        return probabilities\n",
    "       \n",
    "    def lookup(self, word, word_dictionary):\n",
    "        word_id = word_dictionary[word]\n",
    "        start_vec = Variable(torch.LongTensor([word_id]).unsqueeze(0)).cuda()\n",
    "        \n",
    "        return self.embeddings(start_vec)[0]\n",
    "    \n",
    "    def backprop(self, id_list, lr):\n",
    "        self.softmax_layer.backprop(id_list, lr)\n",
    "        \n",
    "        for p in self.embeddings.parameters():\n",
    "            p.data = p.data + (-lr) * p.grad.data\n",
    "            # zero gradients after we make the calculation\n",
    "            p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CBOW_model(model, dset_loader, init_lr = 0.001, num_epochs=20, \n",
    "                     print_every=200, plot_every=100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_loss = float('inf')\n",
    "    model.train(True)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_batch_number = 0\n",
    "    all_losses = []\n",
    "    \n",
    "    lr_scheduler = hierarchical_softmax.lr_scheduler\n",
    "    criterion = hierarchical_softmax.nll_cost\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        epoch_running_loss = 0.0\n",
    "        current_batch = 0\n",
    "        \n",
    "        lr = lr_scheduler(epoch, init_lr=init_lr)\n",
    "        \n",
    "        # Iterate over data.\n",
    "        for context, id_list in dset_loader:\n",
    "            current_batch += 1\n",
    "            total_batch_number += 1\n",
    "\n",
    "            # wrap them in Variable\n",
    "            context, id_list = Variable(context.cuda()), \\\n",
    "                             Variable(id_list.cuda())\n",
    "            \n",
    "            # forward\n",
    "            outputs = model(context, id_list)\n",
    "            loss = criterion(outputs)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            model.backprop(id_list, lr)\n",
    "\n",
    "            # statistics\n",
    "            epoch_running_loss += loss.data[0]\n",
    "            running_loss += loss.data[0]\n",
    "\n",
    "            if total_batch_number % print_every == 0:\n",
    "                curr_loss = epoch_running_loss / (current_batch * dset_loader.batch_size)\n",
    "                time_elapsed = time.time() - since\n",
    "\n",
    "                print('Epoch Number: {}, Batch Number: {}, Loss: {:.4f}'.format(\n",
    "                    epoch, current_batch, curr_loss))\n",
    "                print('Time so far is {:.0f}m {:.0f}s'.format(\n",
    "                    time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "            if total_batch_number % plot_every == 0:\n",
    "                all_losses.append(running_loss/ (total_batch_number * dset_loader.batch_size))\n",
    "\n",
    "\n",
    "        # deep copy the model\n",
    "        if epoch_running_loss < best_loss:\n",
    "            best_loss = epoch_running_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    model.train(False)\n",
    "\n",
    "    return best_model, all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies(ls):\n",
    "    frequencies = defaultdict(int)\n",
    "    \n",
    "    for word in ls:\n",
    "        frequencies[word] += 1\n",
    "    \n",
    "    return frequencies\n",
    "\n",
    "def filter_corpus(text, minimum_frequency=10):\n",
    "    words = list(map(lambda word: word.lower(), filter(lambda word: re.match('[A-Za-z]', word) is not None, text)))\n",
    "    frequencies = get_frequencies(words)\n",
    "    \n",
    "    return list(filter(lambda word: frequencies[word] >= minimum_frequency, words))\n",
    "\n",
    "def make_word_dictionary(text):\n",
    "    word_dictionary = {}\n",
    "    word_index = 0\n",
    "    \n",
    "    for word in text:\n",
    "        if word not in word_dictionary:\n",
    "            word_dictionary[word] = word_index\n",
    "            word_index += 1\n",
    "    \n",
    "    return word_dictionary\n",
    "\n",
    "def make_huffman_tree(text, hidden_size):\n",
    "    frequencies = list(get_frequencies(text).items())\n",
    "    \n",
    "    return HuffmanTree(frequencies, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(data.Dataset):\n",
    "    def __init__(self, text, context_size, word_dictionary):\n",
    "        self.word_dictionary = word_dictionary\n",
    "        self.context_size = context_size\n",
    "        self.word_indices = torch.LongTensor(list(map(lambda word: word_dictionary[word], text)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.word_indices) - 2*self.context_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        prior_words = self.word_indices[index:index+context_size]\n",
    "        later_words = self.word_indices[index+context_size+1:index+2*context_size+1]\n",
    "        \n",
    "        return torch.cat((prior_words, later_words)), self.word_indices[index+context_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "penn_treebank_text = nltk.corpus.treebank.words()\n",
    "training_corpus = filter_corpus(penn_treebank_text)\n",
    "word_dictionary = make_word_dictionary(training_corpus)\n",
    "context_size = 4\n",
    "vocab_size = len(word_dictionary)\n",
    "\n",
    "dataset = CBOWDataset(training_corpus, context_size, word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = []\n",
    "id_to_word_dict = {}\n",
    "\n",
    "for word, id_value in word_dictionary.items():\n",
    "    id_to_word_dict[id_value] = word\n",
    "\n",
    "for i in range(len(id_to_word_dict)):\n",
    "    id_to_word.append(id_to_word_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 200\n",
    "huffman_tree = make_huffman_tree(map(lambda word: word_dictionary[word], training_corpus), hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = CBOW(vocab_size, hidden_size, huffman_tree).cuda()\n",
    "data_loader = data.DataLoader(dataset, batch_size = 1, shuffle = True, num_workers = 1, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "Epoch Number: 0, Batch Number: 200, Loss: 6.2986\n",
      "Time so far is 0m 1s\n",
      "Epoch Number: 0, Batch Number: 400, Loss: 5.1644\n",
      "Time so far is 0m 2s\n",
      "Epoch Number: 0, Batch Number: 600, Loss: 4.8028\n",
      "Time so far is 0m 3s\n",
      "Epoch Number: 0, Batch Number: 800, Loss: 4.4436\n",
      "Time so far is 0m 4s\n",
      "Epoch Number: 0, Batch Number: 1000, Loss: 3.9928\n",
      "Time so far is 0m 5s\n",
      "Epoch Number: 0, Batch Number: 1200, Loss: 3.7571\n",
      "Time so far is 0m 6s\n",
      "Epoch Number: 0, Batch Number: 1400, Loss: 3.5892\n",
      "Time so far is 0m 7s\n",
      "Epoch Number: 0, Batch Number: 1600, Loss: 3.4076\n",
      "Time so far is 0m 8s\n",
      "Epoch Number: 0, Batch Number: 1800, Loss: 3.2540\n",
      "Time so far is 0m 9s\n",
      "Epoch Number: 0, Batch Number: 2000, Loss: 3.1315\n",
      "Time so far is 0m 10s\n",
      "Epoch Number: 0, Batch Number: 2200, Loss: 3.0154\n",
      "Time so far is 0m 11s\n",
      "Epoch Number: 0, Batch Number: 2400, Loss: 2.9029\n",
      "Time so far is 0m 12s\n",
      "Epoch Number: 0, Batch Number: 2600, Loss: 2.8311\n",
      "Time so far is 0m 13s\n",
      "Epoch Number: 0, Batch Number: 2800, Loss: 2.7552\n",
      "Time so far is 0m 14s\n",
      "Epoch Number: 0, Batch Number: 3000, Loss: 2.6647\n",
      "Time so far is 0m 15s\n",
      "Epoch Number: 0, Batch Number: 3200, Loss: 2.5842\n",
      "Time so far is 0m 16s\n",
      "Epoch Number: 0, Batch Number: 3400, Loss: 2.5317\n",
      "Time so far is 0m 17s\n",
      "Epoch Number: 0, Batch Number: 3600, Loss: 2.4703\n",
      "Time so far is 0m 18s\n",
      "Epoch Number: 0, Batch Number: 3800, Loss: 2.4142\n",
      "Time so far is 0m 19s\n",
      "Epoch Number: 0, Batch Number: 4000, Loss: 2.3620\n",
      "Time so far is 0m 20s\n",
      "Epoch Number: 0, Batch Number: 4200, Loss: 2.3212\n",
      "Time so far is 0m 21s\n",
      "Epoch Number: 0, Batch Number: 4400, Loss: 2.2739\n",
      "Time so far is 0m 22s\n",
      "Epoch Number: 0, Batch Number: 4600, Loss: 2.2322\n",
      "Time so far is 0m 23s\n",
      "Epoch Number: 0, Batch Number: 4800, Loss: 2.1845\n",
      "Time so far is 0m 24s\n",
      "Epoch Number: 0, Batch Number: 5000, Loss: 2.1470\n",
      "Time so far is 0m 25s\n",
      "Epoch Number: 0, Batch Number: 5200, Loss: 2.1082\n",
      "Time so far is 0m 26s\n",
      "Epoch Number: 0, Batch Number: 5400, Loss: 2.0738\n",
      "Time so far is 0m 27s\n",
      "Epoch Number: 0, Batch Number: 5600, Loss: 2.0274\n",
      "Time so far is 0m 28s\n",
      "Epoch Number: 0, Batch Number: 5800, Loss: 1.9876\n",
      "Time so far is 0m 29s\n",
      "Epoch Number: 0, Batch Number: 6000, Loss: 1.9585\n",
      "Time so far is 0m 30s\n",
      "Epoch Number: 0, Batch Number: 6200, Loss: 1.9302\n",
      "Time so far is 0m 31s\n",
      "Epoch Number: 0, Batch Number: 6400, Loss: 1.9029\n",
      "Time so far is 0m 32s\n",
      "Epoch Number: 0, Batch Number: 6600, Loss: 1.8772\n",
      "Time so far is 0m 33s\n",
      "Epoch Number: 0, Batch Number: 6800, Loss: 1.8470\n",
      "Time so far is 0m 34s\n",
      "Epoch Number: 0, Batch Number: 7000, Loss: 1.8186\n",
      "Time so far is 0m 35s\n",
      "Epoch Number: 0, Batch Number: 7200, Loss: 1.7938\n",
      "Time so far is 0m 36s\n",
      "Epoch Number: 0, Batch Number: 7400, Loss: 1.7705\n",
      "Time so far is 0m 37s\n",
      "Epoch Number: 0, Batch Number: 7600, Loss: 1.7485\n",
      "Time so far is 0m 38s\n",
      "Epoch Number: 0, Batch Number: 7800, Loss: 1.7256\n",
      "Time so far is 0m 39s\n",
      "Epoch Number: 0, Batch Number: 8000, Loss: 1.7005\n",
      "Time so far is 0m 40s\n",
      "Epoch Number: 0, Batch Number: 8200, Loss: 1.6767\n",
      "Time so far is 0m 41s\n",
      "Epoch Number: 0, Batch Number: 8400, Loss: 1.6588\n",
      "Time so far is 0m 42s\n",
      "Epoch Number: 0, Batch Number: 8600, Loss: 1.6403\n",
      "Time so far is 0m 43s\n",
      "Epoch Number: 0, Batch Number: 8800, Loss: 1.6231\n",
      "Time so far is 0m 44s\n",
      "Epoch Number: 0, Batch Number: 9000, Loss: 1.6059\n",
      "Time so far is 0m 45s\n",
      "Epoch Number: 0, Batch Number: 9200, Loss: 1.5913\n",
      "Time so far is 0m 46s\n",
      "Epoch Number: 0, Batch Number: 9400, Loss: 1.5747\n",
      "Time so far is 0m 47s\n",
      "Epoch Number: 0, Batch Number: 9600, Loss: 1.5573\n",
      "Time so far is 0m 48s\n",
      "Epoch Number: 0, Batch Number: 9800, Loss: 1.5424\n",
      "Time so far is 0m 49s\n",
      "Epoch Number: 0, Batch Number: 10000, Loss: 1.5308\n",
      "Time so far is 0m 50s\n",
      "Epoch Number: 0, Batch Number: 10200, Loss: 1.5151\n",
      "Time so far is 0m 51s\n",
      "Epoch Number: 0, Batch Number: 10400, Loss: 1.4979\n",
      "Time so far is 0m 52s\n",
      "Epoch Number: 0, Batch Number: 10600, Loss: 1.4889\n",
      "Time so far is 0m 53s\n",
      "Epoch Number: 0, Batch Number: 10800, Loss: 1.4721\n",
      "Time so far is 0m 54s\n",
      "Epoch Number: 0, Batch Number: 11000, Loss: 1.4602\n",
      "Time so far is 0m 55s\n",
      "Epoch Number: 0, Batch Number: 11200, Loss: 1.4511\n",
      "Time so far is 0m 56s\n",
      "Epoch Number: 0, Batch Number: 11400, Loss: 1.4411\n",
      "Time so far is 0m 56s\n",
      "Epoch Number: 0, Batch Number: 11600, Loss: 1.4266\n",
      "Time so far is 0m 58s\n",
      "Epoch Number: 0, Batch Number: 11800, Loss: 1.4160\n",
      "Time so far is 0m 59s\n",
      "Epoch Number: 0, Batch Number: 12000, Loss: 1.4037\n",
      "Time so far is 0m 59s\n",
      "Epoch Number: 0, Batch Number: 12200, Loss: 1.3911\n",
      "Time so far is 1m 1s\n",
      "Epoch Number: 0, Batch Number: 12400, Loss: 1.3809\n",
      "Time so far is 1m 2s\n",
      "Epoch Number: 0, Batch Number: 12600, Loss: 1.3667\n",
      "Time so far is 1m 3s\n",
      "Epoch Number: 0, Batch Number: 12800, Loss: 1.3538\n",
      "Time so far is 1m 4s\n",
      "Epoch Number: 0, Batch Number: 13000, Loss: 1.3417\n",
      "Time so far is 1m 5s\n",
      "Epoch Number: 0, Batch Number: 13200, Loss: 1.3332\n",
      "Time so far is 1m 6s\n",
      "Epoch Number: 0, Batch Number: 13400, Loss: 1.3251\n",
      "Time so far is 1m 7s\n",
      "Epoch Number: 0, Batch Number: 13600, Loss: 1.3137\n",
      "Time so far is 1m 8s\n",
      "Epoch Number: 0, Batch Number: 13800, Loss: 1.3028\n",
      "Time so far is 1m 9s\n",
      "Epoch Number: 0, Batch Number: 14000, Loss: 1.2948\n",
      "Time so far is 1m 10s\n",
      "Epoch Number: 0, Batch Number: 14200, Loss: 1.2832\n",
      "Time so far is 1m 11s\n",
      "Epoch Number: 0, Batch Number: 14400, Loss: 1.2740\n",
      "Time so far is 1m 12s\n",
      "Epoch Number: 0, Batch Number: 14600, Loss: 1.2643\n",
      "Time so far is 1m 13s\n",
      "Epoch Number: 0, Batch Number: 14800, Loss: 1.2541\n",
      "Time so far is 1m 14s\n",
      "Epoch Number: 0, Batch Number: 15000, Loss: 1.2444\n",
      "Time so far is 1m 15s\n",
      "Epoch Number: 0, Batch Number: 15200, Loss: 1.2373\n",
      "Time so far is 1m 16s\n",
      "Epoch Number: 0, Batch Number: 15400, Loss: 1.2281\n",
      "Time so far is 1m 17s\n",
      "Epoch Number: 0, Batch Number: 15600, Loss: 1.2190\n",
      "Time so far is 1m 18s\n",
      "Epoch Number: 0, Batch Number: 15800, Loss: 1.2114\n",
      "Time so far is 1m 19s\n",
      "Epoch Number: 0, Batch Number: 16000, Loss: 1.2044\n",
      "Time so far is 1m 20s\n",
      "Epoch Number: 0, Batch Number: 16200, Loss: 1.1957\n",
      "Time so far is 1m 21s\n",
      "Epoch Number: 0, Batch Number: 16400, Loss: 1.1890\n",
      "Time so far is 1m 22s\n",
      "Epoch Number: 0, Batch Number: 16600, Loss: 1.1813\n",
      "Time so far is 1m 23s\n",
      "Epoch Number: 0, Batch Number: 16800, Loss: 1.1739\n",
      "Time so far is 1m 24s\n",
      "Epoch Number: 0, Batch Number: 17000, Loss: 1.1674\n",
      "Time so far is 1m 25s\n",
      "Epoch Number: 0, Batch Number: 17200, Loss: 1.1594\n",
      "Time so far is 1m 26s\n",
      "Epoch Number: 0, Batch Number: 17400, Loss: 1.1520\n",
      "Time so far is 1m 27s\n",
      "Epoch Number: 0, Batch Number: 17600, Loss: 1.1462\n",
      "Time so far is 1m 28s\n",
      "Epoch Number: 0, Batch Number: 17800, Loss: 1.1381\n",
      "Time so far is 1m 29s\n",
      "Epoch Number: 0, Batch Number: 18000, Loss: 1.1311\n",
      "Time so far is 1m 30s\n",
      "Epoch Number: 0, Batch Number: 18200, Loss: 1.1250\n",
      "Time so far is 1m 31s\n",
      "Epoch Number: 0, Batch Number: 18400, Loss: 1.1174\n",
      "Time so far is 1m 32s\n",
      "Epoch Number: 0, Batch Number: 18600, Loss: 1.1100\n",
      "Time so far is 1m 33s\n",
      "Epoch Number: 0, Batch Number: 18800, Loss: 1.1029\n",
      "Time so far is 1m 34s\n",
      "Epoch Number: 0, Batch Number: 19000, Loss: 1.0964\n",
      "Time so far is 1m 35s\n",
      "Epoch Number: 0, Batch Number: 19200, Loss: 1.0924\n",
      "Time so far is 1m 36s\n",
      "Epoch Number: 0, Batch Number: 19400, Loss: 1.0883\n",
      "Time so far is 1m 37s\n",
      "Epoch Number: 0, Batch Number: 19600, Loss: 1.0805\n",
      "Time so far is 1m 38s\n",
      "Epoch Number: 0, Batch Number: 19800, Loss: 1.0741\n",
      "Time so far is 1m 39s\n",
      "Epoch Number: 0, Batch Number: 20000, Loss: 1.0692\n",
      "Time so far is 1m 40s\n",
      "Epoch Number: 0, Batch Number: 20200, Loss: 1.0622\n",
      "Time so far is 1m 41s\n",
      "Epoch Number: 0, Batch Number: 20400, Loss: 1.0570\n",
      "Time so far is 1m 42s\n",
      "Epoch Number: 0, Batch Number: 20600, Loss: 1.0520\n",
      "Time so far is 1m 43s\n",
      "Epoch Number: 0, Batch Number: 20800, Loss: 1.0459\n",
      "Time so far is 1m 44s\n",
      "Epoch Number: 0, Batch Number: 21000, Loss: 1.0401\n",
      "Time so far is 1m 45s\n",
      "Epoch Number: 0, Batch Number: 21200, Loss: 1.0347\n",
      "Time so far is 1m 46s\n",
      "Epoch Number: 0, Batch Number: 21400, Loss: 1.0294\n",
      "Time so far is 1m 47s\n",
      "Epoch Number: 0, Batch Number: 21600, Loss: 1.0242\n",
      "Time so far is 1m 48s\n",
      "Epoch Number: 0, Batch Number: 21800, Loss: 1.0190\n",
      "Time so far is 1m 49s\n",
      "Epoch Number: 0, Batch Number: 22000, Loss: 1.0138\n",
      "Time so far is 1m 50s\n",
      "Epoch Number: 0, Batch Number: 22200, Loss: 1.0100\n",
      "Time so far is 1m 51s\n",
      "Epoch Number: 0, Batch Number: 22400, Loss: 1.0050\n",
      "Time so far is 1m 52s\n",
      "Epoch Number: 0, Batch Number: 22600, Loss: 0.9990\n",
      "Time so far is 1m 53s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0, Batch Number: 22800, Loss: 0.9932\n",
      "Time so far is 1m 54s\n",
      "Epoch Number: 0, Batch Number: 23000, Loss: 0.9893\n",
      "Time so far is 1m 55s\n",
      "Epoch Number: 0, Batch Number: 23200, Loss: 0.9845\n",
      "Time so far is 1m 56s\n",
      "Epoch Number: 0, Batch Number: 23400, Loss: 0.9804\n",
      "Time so far is 1m 57s\n",
      "Epoch Number: 0, Batch Number: 23600, Loss: 0.9759\n",
      "Time so far is 1m 58s\n",
      "Epoch Number: 0, Batch Number: 23800, Loss: 0.9705\n",
      "Time so far is 1m 59s\n",
      "Epoch Number: 0, Batch Number: 24000, Loss: 0.9651\n",
      "Time so far is 1m 60s\n",
      "Epoch Number: 0, Batch Number: 24200, Loss: 0.9599\n",
      "Time so far is 2m 1s\n",
      "Epoch Number: 0, Batch Number: 24400, Loss: 0.9554\n",
      "Time so far is 2m 2s\n",
      "Epoch Number: 0, Batch Number: 24600, Loss: 0.9501\n",
      "Time so far is 2m 3s\n",
      "Epoch Number: 0, Batch Number: 24800, Loss: 0.9471\n",
      "Time so far is 2m 4s\n",
      "Epoch Number: 0, Batch Number: 25000, Loss: 0.9432\n",
      "Time so far is 2m 5s\n",
      "Epoch Number: 0, Batch Number: 25200, Loss: 0.9392\n",
      "Time so far is 2m 6s\n",
      "Epoch Number: 0, Batch Number: 25400, Loss: 0.9345\n",
      "Time so far is 2m 7s\n",
      "Epoch Number: 0, Batch Number: 25600, Loss: 0.9302\n",
      "Time so far is 2m 7s\n",
      "Epoch Number: 0, Batch Number: 25800, Loss: 0.9259\n",
      "Time so far is 2m 8s\n",
      "Epoch Number: 0, Batch Number: 26000, Loss: 0.9214\n",
      "Time so far is 2m 9s\n",
      "Epoch Number: 0, Batch Number: 26200, Loss: 0.9168\n",
      "Time so far is 2m 10s\n",
      "Epoch Number: 0, Batch Number: 26400, Loss: 0.9120\n",
      "Time so far is 2m 11s\n",
      "Epoch Number: 0, Batch Number: 26600, Loss: 0.9081\n",
      "Time so far is 2m 13s\n",
      "Epoch Number: 0, Batch Number: 26800, Loss: 0.9045\n",
      "Time so far is 2m 13s\n",
      "Epoch Number: 0, Batch Number: 27000, Loss: 0.9005\n",
      "Time so far is 2m 14s\n",
      "Epoch Number: 0, Batch Number: 27200, Loss: 0.8965\n",
      "Time so far is 2m 16s\n",
      "Epoch Number: 0, Batch Number: 27400, Loss: 0.8925\n",
      "Time so far is 2m 16s\n",
      "Epoch Number: 0, Batch Number: 27600, Loss: 0.8888\n",
      "Time so far is 2m 17s\n",
      "Epoch Number: 0, Batch Number: 27800, Loss: 0.8853\n",
      "Time so far is 2m 18s\n",
      "Epoch Number: 0, Batch Number: 28000, Loss: 0.8818\n",
      "Time so far is 2m 19s\n",
      "Epoch Number: 0, Batch Number: 28200, Loss: 0.8783\n",
      "Time so far is 2m 20s\n",
      "Epoch Number: 0, Batch Number: 28400, Loss: 0.8746\n",
      "Time so far is 2m 21s\n",
      "Epoch Number: 0, Batch Number: 28600, Loss: 0.8705\n",
      "Time so far is 2m 22s\n",
      "Epoch Number: 0, Batch Number: 28800, Loss: 0.8668\n",
      "Time so far is 2m 23s\n",
      "Epoch Number: 0, Batch Number: 29000, Loss: 0.8632\n",
      "Time so far is 2m 24s\n",
      "Epoch Number: 0, Batch Number: 29200, Loss: 0.8604\n",
      "Time so far is 2m 25s\n",
      "Epoch Number: 0, Batch Number: 29400, Loss: 0.8568\n",
      "Time so far is 2m 26s\n",
      "Epoch Number: 0, Batch Number: 29600, Loss: 0.8533\n",
      "Time so far is 2m 27s\n",
      "Epoch Number: 0, Batch Number: 29800, Loss: 0.8498\n",
      "Time so far is 2m 28s\n",
      "Epoch Number: 0, Batch Number: 30000, Loss: 0.8472\n",
      "Time so far is 2m 29s\n",
      "Epoch Number: 0, Batch Number: 30200, Loss: 0.8434\n",
      "Time so far is 2m 30s\n",
      "Epoch Number: 0, Batch Number: 30400, Loss: 0.8405\n",
      "Time so far is 2m 31s\n",
      "Epoch Number: 0, Batch Number: 30600, Loss: 0.8365\n",
      "Time so far is 2m 32s\n",
      "Epoch Number: 0, Batch Number: 30800, Loss: 0.8331\n",
      "Time so far is 2m 33s\n",
      "Epoch Number: 0, Batch Number: 31000, Loss: 0.8296\n",
      "Time so far is 2m 34s\n",
      "Epoch Number: 0, Batch Number: 31200, Loss: 0.8264\n",
      "Time so far is 2m 35s\n",
      "Epoch Number: 0, Batch Number: 31400, Loss: 0.8235\n",
      "Time so far is 2m 36s\n",
      "Epoch Number: 0, Batch Number: 31600, Loss: 0.8200\n",
      "Time so far is 2m 37s\n",
      "Epoch Number: 0, Batch Number: 31800, Loss: 0.8173\n",
      "Time so far is 2m 38s\n",
      "Epoch Number: 0, Batch Number: 32000, Loss: 0.8139\n",
      "Time so far is 2m 39s\n",
      "Epoch Number: 0, Batch Number: 32200, Loss: 0.8118\n",
      "Time so far is 2m 40s\n",
      "Epoch Number: 0, Batch Number: 32400, Loss: 0.8092\n",
      "Time so far is 2m 41s\n",
      "Epoch Number: 0, Batch Number: 32600, Loss: 0.8054\n",
      "Time so far is 2m 42s\n",
      "Epoch Number: 0, Batch Number: 32800, Loss: 0.8027\n",
      "Time so far is 2m 43s\n",
      "Epoch Number: 0, Batch Number: 33000, Loss: 0.7998\n",
      "Time so far is 2m 44s\n",
      "Epoch Number: 0, Batch Number: 33200, Loss: 0.7970\n",
      "Time so far is 2m 45s\n",
      "Epoch Number: 0, Batch Number: 33400, Loss: 0.7941\n",
      "Time so far is 2m 46s\n",
      "Epoch Number: 0, Batch Number: 33600, Loss: 0.7912\n",
      "Time so far is 2m 47s\n",
      "Epoch Number: 0, Batch Number: 33800, Loss: 0.7880\n",
      "Time so far is 2m 48s\n",
      "Epoch Number: 0, Batch Number: 34000, Loss: 0.7854\n",
      "Time so far is 2m 49s\n",
      "Epoch Number: 0, Batch Number: 34200, Loss: 0.7833\n",
      "Time so far is 2m 50s\n",
      "Epoch Number: 0, Batch Number: 34400, Loss: 0.7804\n",
      "Time so far is 2m 51s\n",
      "Epoch Number: 0, Batch Number: 34600, Loss: 0.7777\n",
      "Time so far is 2m 52s\n",
      "Epoch Number: 0, Batch Number: 34800, Loss: 0.7748\n",
      "Time so far is 2m 53s\n",
      "Epoch Number: 0, Batch Number: 35000, Loss: 0.7722\n",
      "Time so far is 2m 54s\n",
      "Epoch Number: 0, Batch Number: 35200, Loss: 0.7697\n",
      "Time so far is 2m 55s\n",
      "Epoch Number: 0, Batch Number: 35400, Loss: 0.7668\n",
      "Time so far is 2m 56s\n",
      "Epoch Number: 0, Batch Number: 35600, Loss: 0.7645\n",
      "Time so far is 2m 57s\n",
      "Epoch Number: 0, Batch Number: 35800, Loss: 0.7613\n",
      "Time so far is 2m 58s\n",
      "Epoch Number: 0, Batch Number: 36000, Loss: 0.7587\n",
      "Time so far is 2m 59s\n",
      "Epoch Number: 0, Batch Number: 36200, Loss: 0.7562\n",
      "Time so far is 3m 0s\n",
      "Epoch Number: 0, Batch Number: 36400, Loss: 0.7538\n",
      "Time so far is 3m 1s\n",
      "Epoch Number: 0, Batch Number: 36600, Loss: 0.7510\n",
      "Time so far is 3m 2s\n",
      "Epoch Number: 0, Batch Number: 36800, Loss: 0.7482\n",
      "Time so far is 3m 3s\n",
      "Epoch Number: 0, Batch Number: 37000, Loss: 0.7460\n",
      "Time so far is 3m 4s\n",
      "Epoch Number: 0, Batch Number: 37200, Loss: 0.7444\n",
      "Time so far is 3m 5s\n",
      "Epoch Number: 0, Batch Number: 37400, Loss: 0.7419\n",
      "Time so far is 3m 6s\n",
      "Epoch Number: 0, Batch Number: 37600, Loss: 0.7392\n",
      "Time so far is 3m 7s\n",
      "Epoch Number: 0, Batch Number: 37800, Loss: 0.7361\n",
      "Time so far is 3m 8s\n",
      "Epoch Number: 0, Batch Number: 38000, Loss: 0.7337\n",
      "Time so far is 3m 9s\n",
      "Epoch Number: 0, Batch Number: 38200, Loss: 0.7317\n",
      "Time so far is 3m 10s\n",
      "Epoch Number: 0, Batch Number: 38400, Loss: 0.7297\n",
      "Time so far is 3m 11s\n",
      "Epoch Number: 0, Batch Number: 38600, Loss: 0.7277\n",
      "Time so far is 3m 12s\n",
      "Epoch Number: 0, Batch Number: 38800, Loss: 0.7256\n",
      "Time so far is 3m 13s\n",
      "Epoch Number: 0, Batch Number: 39000, Loss: 0.7234\n",
      "Time so far is 3m 14s\n",
      "Epoch Number: 0, Batch Number: 39200, Loss: 0.7216\n",
      "Time so far is 3m 15s\n",
      "Epoch Number: 0, Batch Number: 39400, Loss: 0.7194\n",
      "Time so far is 3m 16s\n",
      "Epoch Number: 0, Batch Number: 39600, Loss: 0.7174\n",
      "Time so far is 3m 17s\n",
      "Epoch Number: 0, Batch Number: 39800, Loss: 0.7154\n",
      "Time so far is 3m 18s\n",
      "Epoch Number: 0, Batch Number: 40000, Loss: 0.7130\n",
      "Time so far is 3m 19s\n",
      "Epoch Number: 0, Batch Number: 40200, Loss: 0.7106\n",
      "Time so far is 3m 20s\n",
      "Epoch Number: 0, Batch Number: 40400, Loss: 0.7079\n",
      "Time so far is 3m 21s\n",
      "Epoch Number: 0, Batch Number: 40600, Loss: 0.7056\n",
      "Time so far is 3m 22s\n",
      "Epoch Number: 0, Batch Number: 40800, Loss: 0.7033\n",
      "Time so far is 3m 23s\n",
      "Epoch Number: 0, Batch Number: 41000, Loss: 0.7010\n",
      "Time so far is 3m 24s\n",
      "Epoch Number: 0, Batch Number: 41200, Loss: 0.6987\n",
      "Time so far is 3m 25s\n",
      "Epoch Number: 0, Batch Number: 41400, Loss: 0.6965\n",
      "Time so far is 3m 26s\n",
      "Epoch Number: 0, Batch Number: 41600, Loss: 0.6943\n",
      "Time so far is 3m 27s\n",
      "Epoch Number: 0, Batch Number: 41800, Loss: 0.6924\n",
      "Time so far is 3m 27s\n",
      "Epoch Number: 0, Batch Number: 42000, Loss: 0.6899\n",
      "Time so far is 3m 28s\n",
      "Epoch Number: 0, Batch Number: 42200, Loss: 0.6877\n",
      "Time so far is 3m 29s\n",
      "Epoch Number: 0, Batch Number: 42400, Loss: 0.6855\n",
      "Time so far is 3m 30s\n",
      "Epoch Number: 0, Batch Number: 42600, Loss: 0.6829\n",
      "Time so far is 3m 31s\n",
      "Epoch Number: 0, Batch Number: 42800, Loss: 0.6808\n",
      "Time so far is 3m 32s\n",
      "Epoch Number: 0, Batch Number: 43000, Loss: 0.6785\n",
      "Time so far is 3m 33s\n",
      "Epoch Number: 0, Batch Number: 43200, Loss: 0.6769\n",
      "Time so far is 3m 34s\n",
      "Epoch Number: 0, Batch Number: 43400, Loss: 0.6750\n",
      "Time so far is 3m 35s\n",
      "Epoch Number: 0, Batch Number: 43600, Loss: 0.6728\n",
      "Time so far is 3m 36s\n",
      "Epoch Number: 0, Batch Number: 43800, Loss: 0.6710\n",
      "Time so far is 3m 37s\n",
      "Epoch Number: 0, Batch Number: 44000, Loss: 0.6691\n",
      "Time so far is 3m 38s\n",
      "Epoch Number: 0, Batch Number: 44200, Loss: 0.6673\n",
      "Time so far is 3m 39s\n",
      "Epoch Number: 0, Batch Number: 44400, Loss: 0.6651\n",
      "Time so far is 3m 40s\n",
      "Epoch Number: 0, Batch Number: 44600, Loss: 0.6637\n",
      "Time so far is 3m 41s\n",
      "Epoch Number: 0, Batch Number: 44800, Loss: 0.6617\n",
      "Time so far is 3m 42s\n",
      "Epoch Number: 0, Batch Number: 45000, Loss: 0.6599\n",
      "Time so far is 3m 43s\n",
      "Epoch Number: 0, Batch Number: 45200, Loss: 0.6577\n",
      "Time so far is 3m 44s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0, Batch Number: 45400, Loss: 0.6557\n",
      "Time so far is 3m 45s\n",
      "Epoch Number: 0, Batch Number: 45600, Loss: 0.6537\n",
      "Time so far is 3m 46s\n",
      "Epoch Number: 0, Batch Number: 45800, Loss: 0.6519\n",
      "Time so far is 3m 47s\n",
      "Epoch Number: 0, Batch Number: 46000, Loss: 0.6504\n",
      "Time so far is 3m 48s\n",
      "Epoch Number: 0, Batch Number: 46200, Loss: 0.6489\n",
      "Time so far is 3m 49s\n",
      "Epoch Number: 0, Batch Number: 46400, Loss: 0.6474\n",
      "Time so far is 3m 50s\n",
      "Epoch Number: 0, Batch Number: 46600, Loss: 0.6457\n",
      "Time so far is 3m 51s\n",
      "Epoch Number: 0, Batch Number: 46800, Loss: 0.6440\n",
      "Time so far is 3m 52s\n",
      "Epoch Number: 0, Batch Number: 47000, Loss: 0.6422\n",
      "Time so far is 3m 53s\n",
      "Epoch Number: 0, Batch Number: 47200, Loss: 0.6402\n",
      "Time so far is 3m 54s\n",
      "Epoch Number: 0, Batch Number: 47400, Loss: 0.6385\n",
      "Time so far is 3m 55s\n",
      "Epoch Number: 0, Batch Number: 47600, Loss: 0.6371\n",
      "Time so far is 3m 56s\n",
      "Epoch Number: 0, Batch Number: 47800, Loss: 0.6353\n",
      "Time so far is 3m 57s\n",
      "Epoch Number: 0, Batch Number: 48000, Loss: 0.6341\n",
      "Time so far is 3m 58s\n",
      "Epoch Number: 0, Batch Number: 48200, Loss: 0.6324\n",
      "Time so far is 3m 59s\n",
      "Epoch Number: 0, Batch Number: 48400, Loss: 0.6304\n",
      "Time so far is 3m 60s\n",
      "Epoch Number: 0, Batch Number: 48600, Loss: 0.6287\n",
      "Time so far is 4m 1s\n",
      "Epoch Number: 0, Batch Number: 48800, Loss: 0.6270\n",
      "Time so far is 4m 2s\n",
      "Epoch Number: 0, Batch Number: 49000, Loss: 0.6250\n",
      "Time so far is 4m 3s\n",
      "Epoch Number: 0, Batch Number: 49200, Loss: 0.6238\n",
      "Time so far is 4m 4s\n",
      "Epoch Number: 0, Batch Number: 49400, Loss: 0.6225\n",
      "Time so far is 4m 5s\n",
      "Epoch Number: 0, Batch Number: 49600, Loss: 0.6210\n",
      "Time so far is 4m 6s\n",
      "Epoch Number: 0, Batch Number: 49800, Loss: 0.6198\n",
      "Time so far is 4m 7s\n",
      "Epoch Number: 0, Batch Number: 50000, Loss: 0.6182\n",
      "Time so far is 4m 8s\n",
      "Epoch Number: 0, Batch Number: 50200, Loss: 0.6166\n",
      "Time so far is 4m 9s\n",
      "Epoch Number: 0, Batch Number: 50400, Loss: 0.6149\n",
      "Time so far is 4m 10s\n",
      "Epoch Number: 0, Batch Number: 50600, Loss: 0.6132\n",
      "Time so far is 4m 11s\n",
      "Epoch Number: 0, Batch Number: 50800, Loss: 0.6117\n",
      "Time so far is 4m 12s\n",
      "Epoch Number: 0, Batch Number: 51000, Loss: 0.6101\n",
      "Time so far is 4m 13s\n",
      "Epoch Number: 0, Batch Number: 51200, Loss: 0.6084\n",
      "Time so far is 4m 14s\n",
      "Epoch Number: 0, Batch Number: 51400, Loss: 0.6068\n",
      "Time so far is 4m 15s\n",
      "Epoch Number: 0, Batch Number: 51600, Loss: 0.6052\n",
      "Time so far is 4m 16s\n",
      "Epoch Number: 0, Batch Number: 51800, Loss: 0.6036\n",
      "Time so far is 4m 17s\n",
      "Epoch Number: 0, Batch Number: 52000, Loss: 0.6020\n",
      "Time so far is 4m 18s\n",
      "Epoch Number: 0, Batch Number: 52200, Loss: 0.6006\n",
      "Time so far is 4m 19s\n",
      "Epoch Number: 0, Batch Number: 52400, Loss: 0.5991\n",
      "Time so far is 4m 20s\n",
      "Epoch Number: 0, Batch Number: 52600, Loss: 0.5974\n",
      "Time so far is 4m 21s\n",
      "Epoch Number: 0, Batch Number: 52800, Loss: 0.5959\n",
      "Time so far is 4m 22s\n",
      "Epoch Number: 0, Batch Number: 53000, Loss: 0.5944\n",
      "Time so far is 4m 23s\n",
      "Epoch Number: 0, Batch Number: 53200, Loss: 0.5931\n",
      "Time so far is 4m 24s\n",
      "Epoch Number: 0, Batch Number: 53400, Loss: 0.5920\n",
      "Time so far is 4m 25s\n",
      "Epoch Number: 0, Batch Number: 53600, Loss: 0.5909\n",
      "Time so far is 4m 26s\n",
      "Epoch Number: 0, Batch Number: 53800, Loss: 0.5892\n",
      "Time so far is 4m 27s\n",
      "Epoch Number: 0, Batch Number: 54000, Loss: 0.5879\n",
      "Time so far is 4m 28s\n",
      "Epoch Number: 0, Batch Number: 54200, Loss: 0.5863\n",
      "Time so far is 4m 29s\n",
      "Epoch Number: 0, Batch Number: 54400, Loss: 0.5848\n",
      "Time so far is 4m 30s\n",
      "Epoch Number: 0, Batch Number: 54600, Loss: 0.5833\n",
      "Time so far is 4m 31s\n",
      "Epoch Number: 0, Batch Number: 54800, Loss: 0.5821\n",
      "Time so far is 4m 32s\n",
      "Epoch Number: 0, Batch Number: 55000, Loss: 0.5809\n",
      "Time so far is 4m 33s\n",
      "Epoch Number: 0, Batch Number: 55200, Loss: 0.5795\n",
      "Time so far is 4m 34s\n",
      "Epoch Number: 0, Batch Number: 55400, Loss: 0.5780\n",
      "Time so far is 4m 35s\n",
      "Epoch Number: 0, Batch Number: 55600, Loss: 0.5765\n",
      "Time so far is 4m 35s\n",
      "Epoch Number: 0, Batch Number: 55800, Loss: 0.5751\n",
      "Time so far is 4m 36s\n",
      "Epoch Number: 0, Batch Number: 56000, Loss: 0.5739\n",
      "Time so far is 4m 37s\n",
      "Epoch Number: 0, Batch Number: 56200, Loss: 0.5726\n",
      "Time so far is 4m 39s\n",
      "Epoch Number: 0, Batch Number: 56400, Loss: 0.5711\n",
      "Time so far is 4m 40s\n",
      "Epoch Number: 0, Batch Number: 56600, Loss: 0.5698\n",
      "Time so far is 4m 41s\n",
      "Epoch Number: 0, Batch Number: 56800, Loss: 0.5685\n",
      "Time so far is 4m 42s\n",
      "Epoch Number: 0, Batch Number: 57000, Loss: 0.5670\n",
      "Time so far is 4m 43s\n",
      "Epoch Number: 0, Batch Number: 57200, Loss: 0.5655\n",
      "Time so far is 4m 44s\n",
      "Epoch Number: 0, Batch Number: 57400, Loss: 0.5640\n",
      "Time so far is 4m 45s\n",
      "Epoch Number: 0, Batch Number: 57600, Loss: 0.5628\n",
      "Time so far is 4m 46s\n",
      "Epoch Number: 0, Batch Number: 57800, Loss: 0.5614\n",
      "Time so far is 4m 47s\n",
      "Epoch Number: 0, Batch Number: 58000, Loss: 0.5600\n",
      "Time so far is 4m 48s\n",
      "Epoch Number: 0, Batch Number: 58200, Loss: 0.5590\n",
      "Time so far is 4m 49s\n",
      "Epoch Number: 0, Batch Number: 58400, Loss: 0.5575\n",
      "Time so far is 4m 50s\n",
      "Epoch Number: 0, Batch Number: 58600, Loss: 0.5564\n",
      "Time so far is 4m 51s\n",
      "\n",
      "Training complete in 4m 52s\n",
      "Best loss: 32645.959677\n"
     ]
    }
   ],
   "source": [
    "word2vec_model, losses = train_CBOW_model(word2vec_model, data_loader, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4d6968f5c0>]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4dc406f940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(model, positive=[], negative=[], topn=10):\n",
    "    \"\"\"\n",
    "    Find the top-N most similar words. Positive words contribute positively towards the\n",
    "    similarity, negative words negatively.\n",
    "    `model.word_vectors` must be a matrix of word embeddings (already L2-normalized),\n",
    "    and its format must be either 2d numpy (dense) or scipy.sparse.csr.\n",
    "    \"\"\"\n",
    "    if isinstance(positive, basestring) and not negative:\n",
    "        # allow calls like most_similar('dog'), as a shorthand for most_similar(['dog'])\n",
    "        positive = [positive]\n",
    "\n",
    "    # add weights for each word, if not already present; default to 1.0 for positive and -1.0 for negative words\n",
    "    positive = [\n",
    "        (word, 1.0) if isinstance(word, (basestring, numpy.ndarray)) else word\n",
    "        for word in positive]\n",
    "    negative = [\n",
    "        (word, -1.0) if isinstance(word, (basestring, numpy.ndarray)) else word\n",
    "        for word in negative]\n",
    "\n",
    "    # compute the weighted average of all words\n",
    "    all_words, mean = set(), []\n",
    "    for word, weight in positive + negative:\n",
    "        if isinstance(word, numpy.ndarray):\n",
    "            mean.append(weight * word)\n",
    "        elif word in model.word2id:\n",
    "            word_index = model.word2id[word]\n",
    "            mean.append(weight * model.word_vectors[word_index])\n",
    "            all_words.add(word_index)\n",
    "        else:\n",
    "            raise KeyError(\"word '%s' not in vocabulary\" % word)\n",
    "    if not mean:\n",
    "        raise ValueError(\"cannot compute similarity with no input\")\n",
    "    if scipy.sparse.issparse(model.word_vectors):\n",
    "        mean = scipy.sparse.vstack(mean)\n",
    "    else:\n",
    "        mean = numpy.array(mean)\n",
    "    mean = matutils.unitvec(mean.mean(axis=0)).astype(model.word_vectors.dtype)\n",
    "\n",
    "    dists = model.word_vectors.dot(mean.T).flatten()\n",
    "    if not topn:\n",
    "        return dists\n",
    "    best = numpy.argsort(dists)[::-1][:topn + len(all_words)]\n",
    "\n",
    "    # ignore (don't return) words from the input\n",
    "    result = [(model.id2word[sim], float(dists[sim])) for sim in best if sim not in all_words]\n",
    "\n",
    "    return result[:topn]\n",
    "\n",
    "def log_accuracy(section):\n",
    "    correct, incorrect = section['correct'], section['incorrect']\n",
    "    if correct + incorrect > 0:\n",
    "        print(\"%s: %.1f%% (%i/%i)\" %\n",
    "            (section['section'], 100.0 * correct / (correct + incorrect),\n",
    "            correct, correct + incorrect))\n",
    "\n",
    "def accuracy(model, questions, ok_words, word_dictionary):\n",
    "    \"\"\"\n",
    "    Compute accuracy of the word embeddings.\n",
    "    `questions` is a filename where lines are 4-tuples of words, split into\n",
    "    sections by \": SECTION NAME\" lines.\n",
    "    See https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt for an example.\n",
    "    The accuracy is reported (=printed to log and returned as a list) for each\n",
    "    section separately, plus there's one aggregate summary at the end.\n",
    "    Only evaluate on words in `word2id` (such as 30k most common words), ignoring\n",
    "    any test examples where any of the four words falls outside `word2id`.\n",
    "    This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
    "    \"\"\"\n",
    "\n",
    "    sections, section = [], None\n",
    "    for line_no, line in enumerate(utils.smart_open(questions)):\n",
    "        line = utils.to_unicode(line)\n",
    "        if line.startswith(': '):\n",
    "            # a new section starts => store the old section\n",
    "            if section:\n",
    "                sections.append(section)\n",
    "                log_accuracy(section)\n",
    "            section = {'section': line.lstrip(': ').strip(), 'correct': 0, 'incorrect': 0}\n",
    "        else:\n",
    "            if not section:\n",
    "                raise ValueError(\"missing section header before line #%i in %s\" % (line_no, questions))\n",
    "            try:\n",
    "                a, b, c, expected = [word.lower() for word in line.split()] \n",
    "            except:\n",
    "                print(\"skipping invalid line #%i in %s\" % (line_no, questions))\n",
    "            if a not in ok_words or b not in ok_words or c not in ok_words or expected not in ok_words:\n",
    "                print(\"skipping line #%i with OOV words: %s\" % (line_no, line.strip()))\n",
    "                continue\n",
    "\n",
    "            ignore = set(word_dictionary[v] for v in [a, b, c])  # indexes of words to ignore\n",
    "            predicted = None\n",
    "\n",
    "            # find the most likely prediction, ignoring OOV words and input words\n",
    "            sims = most_similar(model, positive=[b, c], negative=[a], topn=False)\n",
    "            for index in numpy.argsort(sims)[::-1]:\n",
    "                if model.id2word[index] in ok_words and index not in ignore:\n",
    "                    predicted = model.id2word[index]\n",
    "                    if predicted != expected:\n",
    "                        logger.debug(\"%s: expected %s, predicted %s\" % (line.strip(), expected, predicted))\n",
    "                    break\n",
    "\n",
    "            section['correct' if predicted == expected else 'incorrect'] += 1\n",
    "    if section:\n",
    "        # store the last section, too\n",
    "        sections.append(section)\n",
    "        log_accuracy(section)\n",
    "\n",
    "    total = {'section': 'total', 'correct': sum(s['correct'] for s in sections), 'incorrect': sum(s['incorrect'] for s in sections)}\n",
    "    log_accuracy(total)\n",
    "    sections.append(total)\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.module.lookup('man', word_dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
