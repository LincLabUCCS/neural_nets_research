{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import torch.utils.data as data\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_directory = os.path.expanduser('~')\n",
    "nn_library_path = home_directory + '/Documents/Programming/Neural Nets Library'\n",
    "sys.path.append(nn_library_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.EmbeddingBag(vocab_size, hidden_size)\n",
    "        self.softmax_layer = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        context_vector = self.embeddings(x)\n",
    "        hidden_values = self.softmax_layer(context_vector)\n",
    "        \n",
    "        return hidden_values\n",
    "    \n",
    "    def lookup(self, word, word_dictionary):\n",
    "        word_id = word_dictionary[word]\n",
    "        start_vec = Variable(torch.LongTensor([word_id]).unsqueeze(0)).cuda()\n",
    "        \n",
    "        return self.embeddings(start_vec)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies(ls):\n",
    "    frequencies = defaultdict(int)\n",
    "    \n",
    "    for word in ls:\n",
    "        frequencies[word] += 1\n",
    "    \n",
    "    return frequencies\n",
    "\n",
    "def filter_corpus(text, minimum_frequency=10):\n",
    "    words = list(map(lambda word: word.lower(), filter(lambda word: re.match('[A-Za-z]', word) is not None, text)))\n",
    "    frequencies = get_frequencies(words)\n",
    "    \n",
    "    return list(filter(lambda word: frequencies[word] >= minimum_frequency, words))\n",
    "\n",
    "def make_word_dictionary(text):\n",
    "    word_dictionary = {}\n",
    "    word_index = 0\n",
    "    \n",
    "    for word in text:\n",
    "        if word not in word_dictionary:\n",
    "            word_dictionary[word] = word_index\n",
    "            word_index += 1\n",
    "    \n",
    "    return word_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(data.Dataset):\n",
    "    def __init__(self, text, context_size, word_dictionary):\n",
    "        self.word_dictionary = word_dictionary\n",
    "        self.context_size = context_size\n",
    "        self.word_indices = torch.LongTensor(list(map(lambda word: word_dictionary[word], text)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.word_indices) - 2*self.context_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        prior_words = self.word_indices[index:index+context_size]\n",
    "        later_words = self.word_indices[index+context_size+1:index+2*context_size+1]\n",
    "        \n",
    "        return torch.cat((prior_words, later_words)), self.word_indices[index+context_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "penn_treebank_text = nltk.corpus.treebank.words()\n",
    "training_corpus = filter_corpus(penn_treebank_text)\n",
    "word_dictionary = make_word_dictionary(training_corpus)\n",
    "context_size = 4\n",
    "vocab_size = len(word_dictionary)\n",
    "\n",
    "dataset = CBOWDataset(training_corpus, context_size, word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = []\n",
    "id_to_word_dict = {}\n",
    "\n",
    "for word, id_value in word_dictionary.items():\n",
    "    id_to_word_dict[id_value] = word\n",
    "\n",
    "for i in range(len(id_to_word_dict)):\n",
    "    id_to_word.append(id_to_word_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = nn.DataParallel(CBOW(vocab_size, 200).cuda())\n",
    "data_loader = data.DataLoader(dataset, batch_size = 256, shuffle = True, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(word2vec_model.parameters(), lr = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "LR is set to 0.001\n",
      "Epoch Number: 0, Batch Number: 200, Loss: 0.0255\n",
      "Time so far is 0m 1s\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "Epoch Number: 1, Batch Number: 176, Loss: 0.0255\n",
      "Time so far is 0m 2s\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "Epoch Number: 2, Batch Number: 152, Loss: 0.0255\n",
      "Time so far is 0m 3s\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "Epoch Number: 3, Batch Number: 128, Loss: 0.0255\n",
      "Time so far is 0m 4s\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "Epoch Number: 4, Batch Number: 104, Loss: 0.0255\n",
      "Time so far is 0m 5s\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "Epoch Number: 5, Batch Number: 80, Loss: 0.0255\n",
      "Time so far is 0m 6s\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "Epoch Number: 6, Batch Number: 56, Loss: 0.0255\n",
      "Time so far is 0m 7s\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "LR is set to 0.0001\n",
      "Epoch Number: 7, Batch Number: 32, Loss: 0.0255\n",
      "Time so far is 0m 7s\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "Epoch Number: 8, Batch Number: 8, Loss: 0.0254\n",
      "Time so far is 0m 8s\n",
      "Epoch Number: 8, Batch Number: 208, Loss: 0.0255\n",
      "Time so far is 0m 9s\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "Epoch Number: 9, Batch Number: 184, Loss: 0.0255\n",
      "Time so far is 0m 10s\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "Epoch Number: 10, Batch Number: 160, Loss: 0.0255\n",
      "Time so far is 0m 11s\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "Epoch Number: 11, Batch Number: 136, Loss: 0.0255\n",
      "Time so far is 0m 12s\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "Epoch Number: 12, Batch Number: 112, Loss: 0.0255\n",
      "Time so far is 0m 12s\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "Epoch Number: 13, Batch Number: 88, Loss: 0.0256\n",
      "Time so far is 0m 13s\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "Epoch Number: 14, Batch Number: 64, Loss: 0.0255\n",
      "Time so far is 0m 14s\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "Epoch Number: 15, Batch Number: 40, Loss: 0.0255\n",
      "Time so far is 0m 15s\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "Epoch Number: 16, Batch Number: 16, Loss: 0.0256\n",
      "Time so far is 0m 16s\n",
      "Epoch Number: 16, Batch Number: 216, Loss: 0.0255\n",
      "Time so far is 0m 17s\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "Epoch Number: 17, Batch Number: 192, Loss: 0.0255\n",
      "Time so far is 0m 17s\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "Epoch Number: 18, Batch Number: 168, Loss: 0.0255\n",
      "Time so far is 0m 18s\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "Epoch Number: 19, Batch Number: 144, Loss: 0.0255\n",
      "Time so far is 0m 19s\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "Epoch Number: 20, Batch Number: 120, Loss: 0.0255\n",
      "Time so far is 0m 20s\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "Epoch Number: 21, Batch Number: 96, Loss: 0.0255\n",
      "Time so far is 0m 21s\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "Epoch Number: 22, Batch Number: 72, Loss: 0.0256\n",
      "Time so far is 0m 22s\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "Epoch Number: 23, Batch Number: 48, Loss: 0.0255\n",
      "Time so far is 0m 23s\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "Epoch Number: 24, Batch Number: 24, Loss: 0.0255\n",
      "Time so far is 0m 24s\n",
      "Epoch Number: 24, Batch Number: 224, Loss: 0.0255\n",
      "Time so far is 0m 24s\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "Epoch Number: 25, Batch Number: 200, Loss: 0.0255\n",
      "Time so far is 0m 25s\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "Epoch Number: 26, Batch Number: 176, Loss: 0.0255\n",
      "Time so far is 0m 26s\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "Epoch Number: 27, Batch Number: 152, Loss: 0.0255\n",
      "Time so far is 0m 27s\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-07\n",
      "Epoch Number: 28, Batch Number: 128, Loss: 0.0255\n",
      "Time so far is 0m 28s\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "Epoch Number: 29, Batch Number: 104, Loss: 0.0255\n",
      "Time so far is 0m 28s\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "Epoch Number: 30, Batch Number: 80, Loss: 0.0255\n",
      "Time so far is 0m 29s\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "Epoch Number: 31, Batch Number: 56, Loss: 0.0255\n",
      "Time so far is 0m 30s\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "Epoch Number: 32, Batch Number: 32, Loss: 0.0255\n",
      "Time so far is 0m 31s\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "Epoch Number: 33, Batch Number: 8, Loss: 0.0254\n",
      "Time so far is 0m 31s\n",
      "Epoch Number: 33, Batch Number: 208, Loss: 0.0255\n",
      "Time so far is 0m 32s\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "Epoch Number: 34, Batch Number: 184, Loss: 0.0255\n",
      "Time so far is 0m 33s\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "LR is set to 1.0000000000000004e-08\n",
      "Epoch Number: 35, Batch Number: 160, Loss: 0.0255\n",
      "Time so far is 0m 34s\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "Epoch Number: 36, Batch Number: 136, Loss: 0.0255\n",
      "Time so far is 0m 35s\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "Epoch Number: 37, Batch Number: 112, Loss: 0.0256\n",
      "Time so far is 0m 35s\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "Epoch Number: 38, Batch Number: 88, Loss: 0.0255\n",
      "Time so far is 0m 36s\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "Epoch Number: 39, Batch Number: 64, Loss: 0.0256\n",
      "Time so far is 0m 37s\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "Epoch Number: 40, Batch Number: 40, Loss: 0.0256\n",
      "Time so far is 0m 38s\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "Epoch Number: 41, Batch Number: 16, Loss: 0.0255\n",
      "Time so far is 0m 39s\n",
      "Epoch Number: 41, Batch Number: 216, Loss: 0.0255\n",
      "Time so far is 0m 39s\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "LR is set to 1.0000000000000005e-09\n",
      "Epoch Number: 42, Batch Number: 192, Loss: 0.0255\n",
      "Time so far is 0m 40s\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "Epoch Number: 43, Batch Number: 168, Loss: 0.0255\n",
      "Time so far is 0m 41s\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "Epoch Number: 44, Batch Number: 144, Loss: 0.0255\n",
      "Time so far is 0m 42s\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "Epoch Number: 45, Batch Number: 120, Loss: 0.0255\n",
      "Time so far is 0m 42s\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "Epoch Number: 46, Batch Number: 96, Loss: 0.0255\n",
      "Time so far is 0m 43s\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "Epoch Number: 47, Batch Number: 72, Loss: 0.0256\n",
      "Time so far is 0m 44s\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "Epoch Number: 48, Batch Number: 48, Loss: 0.0255\n",
      "Time so far is 0m 45s\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "LR is set to 1.0000000000000004e-10\n",
      "Epoch Number: 49, Batch Number: 24, Loss: 0.0256\n",
      "Time so far is 0m 46s\n",
      "Epoch Number: 49, Batch Number: 224, Loss: 0.0255\n",
      "Time so far is 0m 46s\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "Epoch Number: 50, Batch Number: 200, Loss: 0.0255\n",
      "Time so far is 0m 47s\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "Epoch Number: 51, Batch Number: 176, Loss: 0.0255\n",
      "Time so far is 0m 48s\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "Epoch Number: 52, Batch Number: 152, Loss: 0.0255\n",
      "Time so far is 0m 48s\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "Epoch Number: 53, Batch Number: 128, Loss: 0.0255\n",
      "Time so far is 0m 49s\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "Epoch Number: 54, Batch Number: 104, Loss: 0.0256\n",
      "Time so far is 0m 50s\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "Epoch Number: 55, Batch Number: 80, Loss: 0.0255\n",
      "Time so far is 0m 51s\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "LR is set to 1.0000000000000006e-11\n",
      "Epoch Number: 56, Batch Number: 56, Loss: 0.0255\n",
      "Time so far is 0m 52s\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "Epoch Number: 57, Batch Number: 32, Loss: 0.0256\n",
      "Time so far is 0m 52s\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "Epoch Number: 58, Batch Number: 8, Loss: 0.0254\n",
      "Time so far is 0m 53s\n",
      "Epoch Number: 58, Batch Number: 208, Loss: 0.0255\n",
      "Time so far is 0m 54s\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "Epoch Number: 59, Batch Number: 184, Loss: 0.0255\n",
      "Time so far is 0m 55s\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "Epoch Number: 60, Batch Number: 160, Loss: 0.0255\n",
      "Time so far is 0m 55s\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "Epoch Number: 61, Batch Number: 136, Loss: 0.0255\n",
      "Time so far is 0m 56s\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "Epoch Number: 62, Batch Number: 112, Loss: 0.0256\n",
      "Time so far is 0m 57s\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "LR is set to 1.0000000000000006e-12\n",
      "Epoch Number: 63, Batch Number: 88, Loss: 0.0256\n",
      "Time so far is 0m 58s\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "Epoch Number: 64, Batch Number: 64, Loss: 0.0255\n",
      "Time so far is 0m 58s\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "Epoch Number: 65, Batch Number: 40, Loss: 0.0255\n",
      "Time so far is 0m 59s\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "Epoch Number: 66, Batch Number: 16, Loss: 0.0256\n",
      "Time so far is 1m 0s\n",
      "Epoch Number: 66, Batch Number: 216, Loss: 0.0255\n",
      "Time so far is 1m 1s\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "Epoch Number: 67, Batch Number: 192, Loss: 0.0255\n",
      "Time so far is 1m 1s\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "Epoch Number: 68, Batch Number: 168, Loss: 0.0255\n",
      "Time so far is 1m 2s\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "Epoch Number: 69, Batch Number: 144, Loss: 0.0255\n",
      "Time so far is 1m 3s\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "LR is set to 1.0000000000000005e-13\n",
      "Epoch Number: 70, Batch Number: 120, Loss: 0.0256\n",
      "Time so far is 1m 4s\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "Epoch Number: 71, Batch Number: 96, Loss: 0.0255\n",
      "Time so far is 1m 5s\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "Epoch Number: 72, Batch Number: 72, Loss: 0.0255\n",
      "Time so far is 1m 5s\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "Epoch Number: 73, Batch Number: 48, Loss: 0.0256\n",
      "Time so far is 1m 6s\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "Epoch Number: 74, Batch Number: 24, Loss: 0.0256\n",
      "Time so far is 1m 7s\n",
      "Epoch Number: 74, Batch Number: 224, Loss: 0.0255\n",
      "Time so far is 1m 8s\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 75, Batch Number: 200, Loss: 0.0255\n",
      "Time so far is 1m 8s\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "Epoch Number: 76, Batch Number: 176, Loss: 0.0255\n",
      "Time so far is 1m 9s\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "LR is set to 1.0000000000000006e-14\n",
      "Epoch Number: 77, Batch Number: 152, Loss: 0.0256\n",
      "Time so far is 1m 10s\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "Epoch Number: 78, Batch Number: 128, Loss: 0.0255\n",
      "Time so far is 1m 11s\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "Epoch Number: 79, Batch Number: 104, Loss: 0.0255\n",
      "Time so far is 1m 11s\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "Epoch Number: 80, Batch Number: 80, Loss: 0.0255\n",
      "Time so far is 1m 12s\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "Epoch Number: 81, Batch Number: 56, Loss: 0.0255\n",
      "Time so far is 1m 13s\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "Epoch Number: 82, Batch Number: 32, Loss: 0.0255\n",
      "Time so far is 1m 14s\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "Epoch Number: 83, Batch Number: 8, Loss: 0.0255\n",
      "Time so far is 1m 15s\n",
      "Epoch Number: 83, Batch Number: 208, Loss: 0.0255\n",
      "Time so far is 1m 15s\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "LR is set to 1.0000000000000007e-15\n",
      "Epoch Number: 84, Batch Number: 184, Loss: 0.0255\n",
      "Time so far is 1m 16s\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "Epoch Number: 85, Batch Number: 160, Loss: 0.0255\n",
      "Time so far is 1m 17s\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "Epoch Number: 86, Batch Number: 136, Loss: 0.0255\n",
      "Time so far is 1m 17s\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "Epoch Number: 87, Batch Number: 112, Loss: 0.0255\n",
      "Time so far is 1m 18s\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "Epoch Number: 88, Batch Number: 88, Loss: 0.0255\n",
      "Time so far is 1m 19s\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "Epoch Number: 89, Batch Number: 64, Loss: 0.0255\n",
      "Time so far is 1m 20s\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "Epoch Number: 90, Batch Number: 40, Loss: 0.0255\n",
      "Time so far is 1m 21s\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "LR is set to 1.0000000000000007e-16\n",
      "Epoch Number: 91, Batch Number: 16, Loss: 0.0255\n",
      "Time so far is 1m 21s\n",
      "Epoch Number: 91, Batch Number: 216, Loss: 0.0255\n",
      "Time so far is 1m 22s\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "Epoch Number: 92, Batch Number: 192, Loss: 0.0255\n",
      "Time so far is 1m 23s\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "Epoch Number: 93, Batch Number: 168, Loss: 0.0255\n",
      "Time so far is 1m 24s\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "Epoch Number: 94, Batch Number: 144, Loss: 0.0255\n",
      "Time so far is 1m 24s\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "Epoch Number: 95, Batch Number: 120, Loss: 0.0255\n",
      "Time so far is 1m 25s\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "Epoch Number: 96, Batch Number: 96, Loss: 0.0255\n",
      "Time so far is 1m 26s\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "Epoch Number: 97, Batch Number: 72, Loss: 0.0255\n",
      "Time so far is 1m 27s\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "LR is set to 1.0000000000000008e-17\n",
      "Epoch Number: 98, Batch Number: 48, Loss: 0.0255\n",
      "Time so far is 1m 28s\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "Epoch Number: 99, Batch Number: 24, Loss: 0.0255\n",
      "Time so far is 1m 28s\n",
      "Epoch Number: 99, Batch Number: 224, Loss: 0.0255\n",
      "Time so far is 1m 29s\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "Epoch Number: 100, Batch Number: 200, Loss: 0.0255\n",
      "Time so far is 1m 30s\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "Epoch Number: 101, Batch Number: 176, Loss: 0.0255\n",
      "Time so far is 1m 31s\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "Epoch Number: 102, Batch Number: 152, Loss: 0.0255\n",
      "Time so far is 1m 31s\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "Epoch Number: 103, Batch Number: 128, Loss: 0.0255\n",
      "Time so far is 1m 32s\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "Epoch Number: 104, Batch Number: 104, Loss: 0.0255\n",
      "Time so far is 1m 33s\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "LR is set to 1.0000000000000008e-18\n",
      "Epoch Number: 105, Batch Number: 80, Loss: 0.0255\n",
      "Time so far is 1m 34s\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "Epoch Number: 106, Batch Number: 56, Loss: 0.0256\n",
      "Time so far is 1m 34s\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "Epoch Number: 107, Batch Number: 32, Loss: 0.0256\n",
      "Time so far is 1m 35s\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "Epoch Number: 108, Batch Number: 8, Loss: 0.0255\n",
      "Time so far is 1m 36s\n",
      "Epoch Number: 108, Batch Number: 208, Loss: 0.0255\n",
      "Time so far is 1m 37s\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "Epoch Number: 109, Batch Number: 184, Loss: 0.0255\n",
      "Time so far is 1m 37s\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "Epoch Number: 110, Batch Number: 160, Loss: 0.0255\n",
      "Time so far is 1m 38s\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "Epoch Number: 111, Batch Number: 136, Loss: 0.0255\n",
      "Time so far is 1m 39s\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "LR is set to 1.0000000000000008e-19\n",
      "Epoch Number: 112, Batch Number: 112, Loss: 0.0256\n",
      "Time so far is 1m 40s\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "Epoch Number: 113, Batch Number: 88, Loss: 0.0256\n",
      "Time so far is 1m 40s\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "Epoch Number: 114, Batch Number: 64, Loss: 0.0256\n",
      "Time so far is 1m 41s\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "Epoch Number: 115, Batch Number: 40, Loss: 0.0256\n",
      "Time so far is 1m 42s\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "Epoch Number: 116, Batch Number: 16, Loss: 0.0256\n",
      "Time so far is 1m 43s\n",
      "Epoch Number: 116, Batch Number: 216, Loss: 0.0255\n",
      "Time so far is 1m 43s\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "Epoch Number: 117, Batch Number: 192, Loss: 0.0255\n",
      "Time so far is 1m 44s\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "Epoch Number: 118, Batch Number: 168, Loss: 0.0255\n",
      "Time so far is 1m 45s\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "LR is set to 1.000000000000001e-20\n",
      "Epoch Number: 119, Batch Number: 144, Loss: 0.0255\n",
      "Time so far is 1m 46s\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "Epoch Number: 120, Batch Number: 120, Loss: 0.0255\n",
      "Time so far is 1m 46s\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "Epoch Number: 121, Batch Number: 96, Loss: 0.0256\n",
      "Time so far is 1m 47s\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "Epoch Number: 122, Batch Number: 72, Loss: 0.0255\n",
      "Time so far is 1m 48s\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "Epoch Number: 123, Batch Number: 48, Loss: 0.0255\n",
      "Time so far is 1m 49s\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "Epoch Number: 124, Batch Number: 24, Loss: 0.0255\n",
      "Time so far is 1m 49s\n",
      "Epoch Number: 124, Batch Number: 224, Loss: 0.0255\n",
      "Time so far is 1m 50s\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "Epoch Number: 125, Batch Number: 200, Loss: 0.0255\n",
      "Time so far is 1m 51s\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "LR is set to 1.000000000000001e-21\n",
      "Epoch Number: 126, Batch Number: 176, Loss: 0.0255\n",
      "Time so far is 1m 52s\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "Epoch Number: 127, Batch Number: 152, Loss: 0.0256\n",
      "Time so far is 1m 52s\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "Epoch Number: 128, Batch Number: 128, Loss: 0.0255\n",
      "Time so far is 1m 53s\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "Epoch Number: 129, Batch Number: 104, Loss: 0.0256\n",
      "Time so far is 1m 54s\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "Epoch Number: 130, Batch Number: 80, Loss: 0.0255\n",
      "Time so far is 1m 55s\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "Epoch Number: 131, Batch Number: 56, Loss: 0.0255\n",
      "Time so far is 1m 55s\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "Epoch Number: 132, Batch Number: 32, Loss: 0.0256\n",
      "Time so far is 1m 56s\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "LR is set to 1.0000000000000011e-22\n",
      "Epoch Number: 133, Batch Number: 8, Loss: 0.0254\n",
      "Time so far is 1m 57s\n",
      "Epoch Number: 133, Batch Number: 208, Loss: 0.0255\n",
      "Time so far is 1m 57s\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "Epoch Number: 134, Batch Number: 184, Loss: 0.0255\n",
      "Time so far is 1m 58s\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "Epoch Number: 135, Batch Number: 160, Loss: 0.0255\n",
      "Time so far is 1m 59s\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "Epoch Number: 136, Batch Number: 136, Loss: 0.0255\n",
      "Time so far is 1m 60s\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "Epoch Number: 137, Batch Number: 112, Loss: 0.0255\n",
      "Time so far is 2m 1s\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "Epoch Number: 138, Batch Number: 88, Loss: 0.0256\n",
      "Time so far is 2m 2s\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "Epoch Number: 139, Batch Number: 64, Loss: 0.0255\n",
      "Time so far is 2m 2s\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "LR is set to 1.0000000000000011e-23\n",
      "Epoch Number: 140, Batch Number: 40, Loss: 0.0255\n",
      "Time so far is 2m 3s\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "Epoch Number: 141, Batch Number: 16, Loss: 0.0255\n",
      "Time so far is 2m 4s\n",
      "Epoch Number: 141, Batch Number: 216, Loss: 0.0255\n",
      "Time so far is 2m 4s\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "Epoch Number: 142, Batch Number: 192, Loss: 0.0255\n",
      "Time so far is 2m 5s\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "Epoch Number: 143, Batch Number: 168, Loss: 0.0255\n",
      "Time so far is 2m 6s\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "Epoch Number: 144, Batch Number: 144, Loss: 0.0255\n",
      "Time so far is 2m 7s\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "Epoch Number: 145, Batch Number: 120, Loss: 0.0255\n",
      "Time so far is 2m 8s\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "Epoch Number: 146, Batch Number: 96, Loss: 0.0256\n",
      "Time so far is 2m 8s\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "LR is set to 1.0000000000000012e-24\n",
      "Epoch Number: 147, Batch Number: 72, Loss: 0.0255\n",
      "Time so far is 2m 9s\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "Epoch Number: 148, Batch Number: 48, Loss: 0.0255\n",
      "Time so far is 2m 10s\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 149, Batch Number: 24, Loss: 0.0255\n",
      "Time so far is 2m 11s\n",
      "Epoch Number: 149, Batch Number: 224, Loss: 0.0255\n",
      "Time so far is 2m 11s\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "Epoch Number: 150, Batch Number: 200, Loss: 0.0255\n",
      "Time so far is 2m 12s\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "Epoch Number: 151, Batch Number: 176, Loss: 0.0255\n",
      "Time so far is 2m 13s\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "Epoch Number: 152, Batch Number: 152, Loss: 0.0255\n",
      "Time so far is 2m 14s\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "Epoch Number: 153, Batch Number: 128, Loss: 0.0255\n",
      "Time so far is 2m 14s\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "LR is set to 1.0000000000000013e-25\n",
      "Epoch Number: 154, Batch Number: 104, Loss: 0.0255\n",
      "Time so far is 2m 15s\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "Epoch Number: 155, Batch Number: 80, Loss: 0.0255\n",
      "Time so far is 2m 16s\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "Epoch Number: 156, Batch Number: 56, Loss: 0.0255\n",
      "Time so far is 2m 17s\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "Epoch Number: 157, Batch Number: 32, Loss: 0.0256\n",
      "Time so far is 2m 18s\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "Epoch Number: 158, Batch Number: 8, Loss: 0.0255\n",
      "Time so far is 2m 18s\n",
      "Epoch Number: 158, Batch Number: 208, Loss: 0.0255\n",
      "Time so far is 2m 19s\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "Epoch Number: 159, Batch Number: 184, Loss: 0.0255\n",
      "Time so far is 2m 20s\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "Epoch Number: 160, Batch Number: 160, Loss: 0.0255\n",
      "Time so far is 2m 21s\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "LR is set to 1.0000000000000013e-26\n",
      "Epoch Number: 161, Batch Number: 136, Loss: 0.0255\n",
      "Time so far is 2m 21s\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "Epoch Number: 162, Batch Number: 112, Loss: 0.0255\n",
      "Time so far is 2m 22s\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "Epoch Number: 163, Batch Number: 88, Loss: 0.0255\n",
      "Time so far is 2m 23s\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "Epoch Number: 164, Batch Number: 64, Loss: 0.0255\n",
      "Time so far is 2m 24s\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "Epoch Number: 165, Batch Number: 40, Loss: 0.0256\n",
      "Time so far is 2m 25s\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "Epoch Number: 166, Batch Number: 16, Loss: 0.0255\n",
      "Time so far is 2m 26s\n",
      "Epoch Number: 166, Batch Number: 216, Loss: 0.0255\n",
      "Time so far is 2m 26s\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "Epoch Number: 167, Batch Number: 192, Loss: 0.0255\n",
      "Time so far is 2m 27s\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "LR is set to 1.0000000000000015e-27\n",
      "Epoch Number: 168, Batch Number: 168, Loss: 0.0255\n",
      "Time so far is 2m 28s\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "Epoch Number: 169, Batch Number: 144, Loss: 0.0255\n",
      "Time so far is 2m 29s\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "Epoch Number: 170, Batch Number: 120, Loss: 0.0256\n",
      "Time so far is 2m 29s\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "Epoch Number: 171, Batch Number: 96, Loss: 0.0256\n",
      "Time so far is 2m 30s\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "Epoch Number: 172, Batch Number: 72, Loss: 0.0256\n",
      "Time so far is 2m 31s\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "Epoch Number: 173, Batch Number: 48, Loss: 0.0255\n",
      "Time so far is 2m 32s\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "Epoch Number: 174, Batch Number: 24, Loss: 0.0255\n",
      "Time so far is 2m 33s\n",
      "Epoch Number: 174, Batch Number: 224, Loss: 0.0255\n",
      "Time so far is 2m 33s\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "LR is set to 1.0000000000000014e-28\n",
      "Epoch Number: 175, Batch Number: 200, Loss: 0.0255\n",
      "Time so far is 2m 34s\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "Epoch Number: 176, Batch Number: 176, Loss: 0.0255\n",
      "Time so far is 2m 35s\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "Epoch Number: 177, Batch Number: 152, Loss: 0.0255\n",
      "Time so far is 2m 36s\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "Epoch Number: 178, Batch Number: 128, Loss: 0.0255\n",
      "Time so far is 2m 37s\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "Epoch Number: 179, Batch Number: 104, Loss: 0.0255\n",
      "Time so far is 2m 37s\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "Epoch Number: 180, Batch Number: 80, Loss: 0.0255\n",
      "Time so far is 2m 38s\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "Epoch Number: 181, Batch Number: 56, Loss: 0.0255\n",
      "Time so far is 2m 39s\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "LR is set to 1.0000000000000015e-29\n",
      "Epoch Number: 182, Batch Number: 32, Loss: 0.0255\n",
      "Time so far is 2m 40s\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "Epoch Number: 183, Batch Number: 8, Loss: 0.0255\n",
      "Time so far is 2m 41s\n",
      "Epoch Number: 183, Batch Number: 208, Loss: 0.0255\n",
      "Time so far is 2m 41s\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "Epoch Number: 184, Batch Number: 184, Loss: 0.0255\n",
      "Time so far is 2m 42s\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "Epoch Number: 185, Batch Number: 160, Loss: 0.0255\n",
      "Time so far is 2m 43s\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "Epoch Number: 186, Batch Number: 136, Loss: 0.0255\n",
      "Time so far is 2m 44s\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "Epoch Number: 187, Batch Number: 112, Loss: 0.0255\n",
      "Time so far is 2m 44s\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "Epoch Number: 188, Batch Number: 88, Loss: 0.0255\n",
      "Time so far is 2m 45s\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "LR is set to 1.0000000000000015e-30\n",
      "Epoch Number: 189, Batch Number: 64, Loss: 0.0255\n",
      "Time so far is 2m 46s\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "Epoch Number: 190, Batch Number: 40, Loss: 0.0255\n",
      "Time so far is 2m 47s\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "Epoch Number: 191, Batch Number: 16, Loss: 0.0254\n",
      "Time so far is 2m 48s\n",
      "Epoch Number: 191, Batch Number: 216, Loss: 0.0255\n",
      "Time so far is 2m 48s\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "Epoch Number: 192, Batch Number: 192, Loss: 0.0255\n",
      "Time so far is 2m 49s\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "Epoch Number: 193, Batch Number: 168, Loss: 0.0255\n",
      "Time so far is 2m 50s\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "Epoch Number: 194, Batch Number: 144, Loss: 0.0255\n",
      "Time so far is 2m 51s\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "Epoch Number: 195, Batch Number: 120, Loss: 0.0255\n",
      "Time so far is 2m 51s\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "LR is set to 1.0000000000000016e-31\n",
      "Epoch Number: 196, Batch Number: 96, Loss: 0.0255\n",
      "Time so far is 2m 52s\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "Epoch Number: 197, Batch Number: 72, Loss: 0.0255\n",
      "Time so far is 2m 53s\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "Epoch Number: 198, Batch Number: 48, Loss: 0.0255\n",
      "Time so far is 2m 54s\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "Epoch Number: 199, Batch Number: 24, Loss: 0.0254\n",
      "Time so far is 2m 54s\n",
      "Epoch Number: 199, Batch Number: 224, Loss: 0.0255\n",
      "Time so far is 2m 55s\n",
      "\n",
      "Training complete in 2m 55s\n",
      "Best loss: 1463.410931\n"
     ]
    }
   ],
   "source": [
    "word2vec_model, losses = training.train_model(word2vec_model, data_loader, criterion, optimizer, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff6f938b470>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD8CAYAAABZ/vJZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff7780282e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(model, positive=[], negative=[], topn=10):\n",
    "    \"\"\"\n",
    "    Find the top-N most similar words. Positive words contribute positively towards the\n",
    "    similarity, negative words negatively.\n",
    "    `model.word_vectors` must be a matrix of word embeddings (already L2-normalized),\n",
    "    and its format must be either 2d numpy (dense) or scipy.sparse.csr.\n",
    "    \"\"\"\n",
    "    if isinstance(positive, basestring) and not negative:\n",
    "        # allow calls like most_similar('dog'), as a shorthand for most_similar(['dog'])\n",
    "        positive = [positive]\n",
    "\n",
    "    # add weights for each word, if not already present; default to 1.0 for positive and -1.0 for negative words\n",
    "    positive = [\n",
    "        (word, 1.0) if isinstance(word, (basestring, numpy.ndarray)) else word\n",
    "        for word in positive]\n",
    "    negative = [\n",
    "        (word, -1.0) if isinstance(word, (basestring, numpy.ndarray)) else word\n",
    "        for word in negative]\n",
    "\n",
    "    # compute the weighted average of all words\n",
    "    all_words, mean = set(), []\n",
    "    for word, weight in positive + negative:\n",
    "        if isinstance(word, numpy.ndarray):\n",
    "            mean.append(weight * word)\n",
    "        elif word in model.word2id:\n",
    "            word_index = model.word2id[word]\n",
    "            mean.append(weight * model.word_vectors[word_index])\n",
    "            all_words.add(word_index)\n",
    "        else:\n",
    "            raise KeyError(\"word '%s' not in vocabulary\" % word)\n",
    "    if not mean:\n",
    "        raise ValueError(\"cannot compute similarity with no input\")\n",
    "    if scipy.sparse.issparse(model.word_vectors):\n",
    "        mean = scipy.sparse.vstack(mean)\n",
    "    else:\n",
    "        mean = numpy.array(mean)\n",
    "    mean = matutils.unitvec(mean.mean(axis=0)).astype(model.word_vectors.dtype)\n",
    "\n",
    "    dists = model.word_vectors.dot(mean.T).flatten()\n",
    "    if not topn:\n",
    "        return dists\n",
    "    best = numpy.argsort(dists)[::-1][:topn + len(all_words)]\n",
    "\n",
    "    # ignore (don't return) words from the input\n",
    "    result = [(model.id2word[sim], float(dists[sim])) for sim in best if sim not in all_words]\n",
    "\n",
    "    return result[:topn]\n",
    "\n",
    "def log_accuracy(section):\n",
    "    correct, incorrect = section['correct'], section['incorrect']\n",
    "    if correct + incorrect > 0:\n",
    "        print(\"%s: %.1f%% (%i/%i)\" %\n",
    "            (section['section'], 100.0 * correct / (correct + incorrect),\n",
    "            correct, correct + incorrect))\n",
    "\n",
    "def accuracy(model, questions, ok_words, word_dictionary):\n",
    "    \"\"\"\n",
    "    Compute accuracy of the word embeddings.\n",
    "    `questions` is a filename where lines are 4-tuples of words, split into\n",
    "    sections by \": SECTION NAME\" lines.\n",
    "    See https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt for an example.\n",
    "    The accuracy is reported (=printed to log and returned as a list) for each\n",
    "    section separately, plus there's one aggregate summary at the end.\n",
    "    Only evaluate on words in `word2id` (such as 30k most common words), ignoring\n",
    "    any test examples where any of the four words falls outside `word2id`.\n",
    "    This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
    "    \"\"\"\n",
    "\n",
    "    sections, section = [], None\n",
    "    for line_no, line in enumerate(utils.smart_open(questions)):\n",
    "        line = utils.to_unicode(line)\n",
    "        if line.startswith(': '):\n",
    "            # a new section starts => store the old section\n",
    "            if section:\n",
    "                sections.append(section)\n",
    "                log_accuracy(section)\n",
    "            section = {'section': line.lstrip(': ').strip(), 'correct': 0, 'incorrect': 0}\n",
    "        else:\n",
    "            if not section:\n",
    "                raise ValueError(\"missing section header before line #%i in %s\" % (line_no, questions))\n",
    "            try:\n",
    "                a, b, c, expected = [word.lower() for word in line.split()] \n",
    "            except:\n",
    "                print(\"skipping invalid line #%i in %s\" % (line_no, questions))\n",
    "            if a not in ok_words or b not in ok_words or c not in ok_words or expected not in ok_words:\n",
    "                print(\"skipping line #%i with OOV words: %s\" % (line_no, line.strip()))\n",
    "                continue\n",
    "\n",
    "            ignore = set(word_dictionary[v] for v in [a, b, c])  # indexes of words to ignore\n",
    "            predicted = None\n",
    "\n",
    "            # find the most likely prediction, ignoring OOV words and input words\n",
    "            sims = most_similar(model, positive=[b, c], negative=[a], topn=False)\n",
    "            for index in numpy.argsort(sims)[::-1]:\n",
    "                if model.id2word[index] in ok_words and index not in ignore:\n",
    "                    predicted = model.id2word[index]\n",
    "                    if predicted != expected:\n",
    "                        logger.debug(\"%s: expected %s, predicted %s\" % (line.strip(), expected, predicted))\n",
    "                    break\n",
    "\n",
    "            section['correct' if predicted == expected else 'incorrect'] += 1\n",
    "    if section:\n",
    "        # store the last section, too\n",
    "        sections.append(section)\n",
    "        log_accuracy(section)\n",
    "\n",
    "    total = {'section': 'total', 'correct': sum(s['correct'] for s in sections), 'incorrect': sum(s['incorrect'] for s in sections)}\n",
    "    log_accuracy(total)\n",
    "    sections.append(total)\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.2996\n",
       "-0.5917\n",
       " 0.1506\n",
       "-0.7426\n",
       "-1.6214\n",
       "-0.4217\n",
       " 0.4004\n",
       " 0.1098\n",
       " 1.1753\n",
       "-0.5553\n",
       "-0.7912\n",
       " 1.5314\n",
       " 0.6268\n",
       " 2.3838\n",
       "-0.9732\n",
       "-0.8033\n",
       "-0.1535\n",
       "-0.9036\n",
       "-0.3500\n",
       " 0.9619\n",
       "-1.5441\n",
       " 0.4123\n",
       " 0.3171\n",
       " 0.2561\n",
       "-0.0216\n",
       " 0.1781\n",
       " 0.0233\n",
       "-0.5583\n",
       "-0.1787\n",
       " 1.8202\n",
       " 0.1236\n",
       "-0.3718\n",
       " 1.3955\n",
       " 2.0521\n",
       "-1.2064\n",
       "-2.5305\n",
       " 0.3084\n",
       "-0.3150\n",
       " 0.3582\n",
       "-1.5001\n",
       "-0.2157\n",
       "-0.3455\n",
       "-0.2168\n",
       "-0.0532\n",
       " 1.8616\n",
       "-0.6592\n",
       " 0.3282\n",
       "-0.4089\n",
       " 0.5479\n",
       " 0.0725\n",
       "-1.6682\n",
       "-2.2214\n",
       "-0.5638\n",
       "-1.5363\n",
       " 1.6185\n",
       " 0.1768\n",
       "-1.2364\n",
       "-1.1346\n",
       "-0.5710\n",
       " 0.4065\n",
       "-1.1067\n",
       "-0.2257\n",
       " 0.5945\n",
       "-2.7637\n",
       " 0.6140\n",
       "-0.6395\n",
       " 0.4668\n",
       "-1.2638\n",
       " 0.4237\n",
       " 0.7419\n",
       "-1.3740\n",
       " 1.3386\n",
       "-0.5660\n",
       " 0.0170\n",
       " 0.1355\n",
       "-1.5914\n",
       "-0.0297\n",
       " 0.9536\n",
       "-1.0453\n",
       "-0.5211\n",
       " 0.8433\n",
       "-0.8491\n",
       " 0.2185\n",
       " 0.7313\n",
       " 1.1205\n",
       " 0.1938\n",
       "-0.1025\n",
       "-0.0094\n",
       "-0.4112\n",
       "-0.1801\n",
       "-1.2364\n",
       "-0.9337\n",
       "-2.3567\n",
       "-1.0634\n",
       " 0.4226\n",
       "-0.4526\n",
       "-2.3171\n",
       " 2.0197\n",
       "-0.5446\n",
       "-0.4111\n",
       "-0.2156\n",
       "-0.0309\n",
       " 0.0095\n",
       " 1.6864\n",
       "-0.4980\n",
       "-1.2551\n",
       " 0.8065\n",
       "-1.2130\n",
       "-0.8944\n",
       " 0.9086\n",
       " 0.5113\n",
       " 0.4413\n",
       " 0.3812\n",
       " 0.4418\n",
       "-0.7326\n",
       " 2.0286\n",
       " 1.3737\n",
       " 1.9850\n",
       "-1.7395\n",
       " 1.0533\n",
       " 0.0176\n",
       " 1.0853\n",
       "-2.1236\n",
       "-1.3823\n",
       " 0.5950\n",
       " 0.2607\n",
       " 2.4890\n",
       "-0.2910\n",
       "-1.8057\n",
       " 0.4739\n",
       " 1.9533\n",
       "-1.0892\n",
       "-1.3047\n",
       "-0.4465\n",
       "-0.8632\n",
       " 1.2329\n",
       " 0.0829\n",
       "-0.5422\n",
       " 0.2013\n",
       " 0.7747\n",
       " 0.0589\n",
       "-0.1522\n",
       "-0.8715\n",
       " 0.6123\n",
       "-0.7277\n",
       "-1.3203\n",
       "-1.2858\n",
       " 0.4421\n",
       " 0.8028\n",
       "-0.3288\n",
       " 1.2796\n",
       " 1.3285\n",
       "-1.8276\n",
       " 0.1657\n",
       " 0.1000\n",
       " 1.2644\n",
       " 0.4039\n",
       "-0.1695\n",
       " 0.0122\n",
       " 0.5731\n",
       " 1.1181\n",
       " 0.6660\n",
       "-1.2469\n",
       " 0.3838\n",
       " 0.3107\n",
       "-0.6676\n",
       "-0.3235\n",
       "-0.8293\n",
       " 1.0198\n",
       "-0.4479\n",
       " 0.4627\n",
       " 0.3528\n",
       " 1.0013\n",
       " 0.2349\n",
       "-1.3278\n",
       " 1.7176\n",
       " 1.2733\n",
       " 2.3980\n",
       "-0.2214\n",
       "-0.0056\n",
       "-1.1421\n",
       " 0.1186\n",
       "-0.2583\n",
       "-0.8808\n",
       "-1.0797\n",
       "-0.1411\n",
       "-1.8307\n",
       " 1.2270\n",
       " 0.0617\n",
       " 0.1429\n",
       " 1.1365\n",
       " 2.2491\n",
       " 1.3221\n",
       "-0.4072\n",
       " 1.1423\n",
       " 0.9320\n",
       "-0.4288\n",
       "-0.2861\n",
       "-0.6653\n",
       " 1.3704\n",
       "[torch.cuda.FloatTensor of size 200 (GPU 0)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.module.lookup('man', word_dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
