{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FILE CONTAINS PREPROCESSING CODE THAT WAS USED ONCE AND IS SEPARATE TO\n",
    "# IMPLY THAT IT DOESN'T NEED TO AGAIN UNLESS IMPORTANT FILES ARE DELETED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import torch.utils.data as data\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from gensim import utils\n",
    "from huffman_tree import HuffmanTree\n",
    "from hierarchical_softmax import HierarchicalSoftmax\n",
    "import hierarchical_softmax\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import json\n",
    "import pickle\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_directory = os.path.expanduser('~')\n",
    "nn_library_path = home_directory + '/Documents/neural_nets_research/Neural Nets Library'\n",
    "sys.path.append(nn_library_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USED TO CREATE FREQUENCY DICTIONARY (ran once)\n",
    "\n",
    "wikipedia_text_path = \"wiki.en.text\"\n",
    "\n",
    "# Read line by line\n",
    "last_line = None\n",
    "frequencies = defaultdict(int)\n",
    "all_letters = string.ascii_letters\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "counter = 1\n",
    "with open(wikipedia_text_path) as f:\n",
    "    for line in f:\n",
    "        for word in line.split():\n",
    "            frequencies[unicodeToAscii(word)] += 1\n",
    "            \n",
    "        if (counter % 10000 == 0):\n",
    "            print(counter)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "filter_file_handle = open(\"wiki.en.frequencies.pkl\", \"wb\")\n",
    "pickle.dump(frequencies, filter_file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading of wikipedia into a dictionary\n",
    "def printOccurrence(dictionary, word):\n",
    "    print(\"The word\", word, \"appears\", dictionary[word], \"times.\")\n",
    "    \n",
    "printOccurrence(frequencies, \"mehdi\")\n",
    "printOccurrence(frequencies, \"ununpentium\")\n",
    "print(len(frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USED TO CREATE WIKIPEDIA SUBFILES (ran once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filtered_subfiles(large_file_path, filter_dictionary, divisor_value):\n",
    "    file_handle = open(large_file_path, \"r\")\n",
    "    file_counter = 0\n",
    "    subfiles_handle_prefix = \"/media/mehdi2277/MyFiles/large_datasets/wikipedia/wiki_s_\"\n",
    "    \n",
    "    line_counter = 0\n",
    "    current_subfile_array = []\n",
    "    with open(large_file_path) as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                if word in filter_dictionary:\n",
    "                    current_subfile_array += [word]\n",
    "\n",
    "            if (line_counter % divisor_value == 0):\n",
    "                subfile_name = subfiles_handle_prefix + str(file_counter) + \".json\"\n",
    "                subfile_handle = open(subfile_name, \"w\")\n",
    "                \n",
    "                # Save serializable array of words corresponding to 1000 articles\n",
    "                json.dump(current_subfile_array, subfile_handle)\n",
    "                print(\"Writing to \", subfile_name)\n",
    "                \n",
    "                # Increment file count and close file\n",
    "                file_counter += 1\n",
    "                subfile_handle.close()\n",
    "                \n",
    "                # Empty array\n",
    "                current_subfile_array = []\n",
    "\n",
    "            line_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 120\n",
    "wikipedia_text_path = \"wiki.en.text\"\n",
    "frequencies = {k: v for k, v in frequencies.items() if v > threshold}\n",
    "\n",
    "# Create subfiles\n",
    "make_filtered_subfiles(wikipedia_text_path, frequencies, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse word to id dictionary\n",
    "id_to_word = []\n",
    "id_to_word_dict = {}\n",
    "\n",
    "for word, id_value in word_dictionary.items():\n",
    "    id_to_word_dict[id_value] = word\n",
    "\n",
    "for i in range(len(id_to_word_dict)):\n",
    "    id_to_word.append(id_to_word_dict[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
