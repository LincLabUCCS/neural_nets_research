{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data as d\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_cols(mat, lengths):\n",
    "    \"\"\"Split a 2D matrix to variable length columns.\"\"\"\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results += [mat[:, s:e]]\n",
    "    return results\n",
    "\n",
    "class NTM_Head(nn.Module):\n",
    "    def __init__(self, memory, controller_output_size,sharp_on):\n",
    "        super(NTM_Head, self).__init__()\n",
    "        \n",
    "        self.memory = memory\n",
    "        self.controller_output_size = controller_output_size\n",
    "        self.N, self.M = memory.size()\n",
    "        self.sharp_on = sharp_on\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def address_memory(self, key_vec, β, g, γ):\n",
    "        result = F.cosine_similarity(key_vec.unsqueeze(0).expand(self.N, -1), self.memory, dim = 0)\n",
    "        result = β * result\n",
    "        result = result.exp() / result.sum()\n",
    "        result = g * result + (1 - g) * self.prev_address_vector\n",
    "        if self.sharp_on:\n",
    "            result = result ** γ\n",
    "            result = result / result.sum()\n",
    "        return resultclass NTM_Read_Head(NTM_Head):\n",
    "    def __init__(self, memory, controller_output_size):\n",
    "        super(NTM_Read_Head).__init__(memory, controller_output_size)\n",
    "        \n",
    "        self.read_parameters_lengths = [self.N, 1, 1, 1]\n",
    "        self.fc_read_parameters = nn.Linear(controller_output_size, sum(read_parameters_lengths))\n",
    "        \n",
    "        initialize_parameters()\n",
    "        initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_read_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_read_parameters.bias, std=0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.initial_address_vector = nn.Parameter(torch.zeros(self.N))\n",
    "        self.initial_read = nn.Parameter(torch.zeros(self.M))\n",
    "        \n",
    "        self.prev_address_vector = Variable(self.initial_address_vector)\n",
    "        self.prev_read = Variable(self.initial_read)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        read_parameters = self.fc_read_parameters(x)\n",
    "        \n",
    "        key_vec, β, g, γ = _split_cols(read_parameters, self.read_parameters_length)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "                                               \n",
    "        address_vec = address_memory(key_vec, β, g, γ)\n",
    "        new_read = self.M.transpose() * address_vec\n",
    "        self.prev_read = new_read\n",
    "        return new_read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Read_Head(NTM_Head):\n",
    "    def __init__(self, memory, controller_output_size, sharp_on):\n",
    "        super(NTM_Read_Head).__init__(memory, controller_output_size, sharp_on)\n",
    "        \n",
    "        self.read_parameters_lengths = [self.N, 1, 1, 1]\n",
    "        self.fc_read_parameters = nn.Linear(controller_output_size, sum(read_parameters_lengths))\n",
    "        \n",
    "        initialize_parameters()\n",
    "        initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_read_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_read_parameters.bias, std=0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.initial_address_vector = nn.Parameter(torch.zeros(self.N))\n",
    "        self.initial_read = nn.Parameter(torch.zeros(self.M))\n",
    "        \n",
    "        self.prev_address_vector = Variable(self.initial_address_vector)\n",
    "        self.prev_read = Variable(self.initial_read)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        read_parameters = self.fc_read_parameters(x)\n",
    "        \n",
    "        key_vec, β, g, γ = _split_cols(read_parameters, self.read_parameters_length)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "                                               \n",
    "        address_vec = address_memory(key_vec, β, g, γ)\n",
    "        new_read = self.M.transpose() * address_vec\n",
    "        self.prev_read = new_read\n",
    "        return new_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM_Write_Head(NTM_Head):\n",
    "    def __init__(self, memory, controller_output_size,sharp_on):\n",
    "        super(NTM_Write_Head).__init__(memory, controller_output_size, sharp_on)\n",
    "        \n",
    "        self.write_parameters_lengths = [self.N, 1, 1, 1, self.M, self.M]\n",
    "        self.fc_write_parameters = nn.Linear(controller_output_size, sum(write_parameters_lengths))\n",
    "        self.sharp_on = sharp_on\n",
    "        \n",
    "        reset_parameters()\n",
    "        initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.fc_write_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_write_parameters.bias, std=0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.initial_address_vector = nn.Parameter(torch.zeros(self.N))       \n",
    "        self.prev_address_vector = Variable(self.initial_address_vector)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        return False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        write_parameters = self.fc_write_parameters(x)\n",
    "                                       \n",
    "        key_vec, β, g, γ, erase_vec, add_vec = _split_cols(write_parameters, self.write_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "#         γ = 1 + F.softplus(γ)\n",
    "        erase_vec = F.sigmoid(erase_vec)\n",
    "                                               \n",
    "        address_vec = address_memory(key_vec, β, g, γ)\n",
    "        self.M *= 1 - torch.bmm(address_vec.unsqueeze(2), erase_vec.unsquueze(1))\n",
    "        self.M += torch.bmm(address_vec.unsqueeze(2), add_vec.unsquueze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncapsulatedLSTM(nn.Module):\n",
    "    def __init__(self, batch_size, *args, **kwargs):\n",
    "        super(EncapsulatedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(*args, **kwargs)\n",
    "        hidden_size = args[1]\n",
    "        num_layers = args[2]\n",
    "        \n",
    "        initial_hidden_state = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n",
    "        initial_hidden_state.expand(num_layers, batch_size, hidden_size)\n",
    "        initial_cell_state = nn.Parameter(torch.randn(num_layers, 1, hidden_size))\n",
    "        initial_cell_state.expand(num_layers, batch_size, hidden_size)\n",
    "        self.state_tuple = (initial_hidden_state, initial_cell_state)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output, self.state_tuple = self.lstm(input, self.state_tuple)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC(nn.Module):\n",
    "    def __init__(self, controller, controller_output_size, output_size, \n",
    "                 address_count, address_dimension, heads, sharp_on=True):\n",
    "        super(DNC, self).__init__()\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = nn.Parameter(torch.zeros(address_count, address_dimension))\n",
    "        \n",
    "        # Construct the heads.\n",
    "        self.heads = nn.ModuleList()\n",
    "        \n",
    "        for head_id in heads:\n",
    "            if head_id = 0:\n",
    "                self.heads.append(NTM_Read_Head(memory, controller_output_size))\n",
    "            else:\n",
    "                self.heads.append(NTM_Write_Head(memory, controller_output_size))\n",
    "        \n",
    "        # Initialize controller\n",
    "        self.controller = controller\n",
    "        self.outputGate = nn.Linear(controller_output_size, output_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sharp_on = sharp_on\n",
    "        \n",
    "        initialize_state()\n",
    "        reset_parameters()\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        self.prev_reads = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head.prev_read)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdev = 1 / (np.sqrt(N + M))\n",
    "        nn.init.uniform(self.memory, -stdev, stdev)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        controller_output = controller(torch.cat(self.prev_reads.append(x), dim=1))\n",
    "        self.prev_reads = []\n",
    "        \n",
    "        for head in self.heads:\n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head(controller_output))\n",
    "            else:\n",
    "                head(controller_output)\n",
    "        return self.softmax(self.outputGate(controller_output))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
