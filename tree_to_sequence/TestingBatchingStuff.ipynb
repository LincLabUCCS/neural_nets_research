{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "\n",
    "from tree_to_sequence.program_datasets import *\n",
    "from tree_to_sequence.translating_trees import map_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Fold(object):\n",
    "\n",
    "    class Node(object):\n",
    "        def __init__(self, op, step, index, *args):\n",
    "            self.op = op\n",
    "            self.step = step\n",
    "            self.index = index\n",
    "            self.args = args\n",
    "            self.split_idx = -1\n",
    "            self.batch = True\n",
    "\n",
    "        def split(self, num):\n",
    "            \"\"\"Split resulting node, if function returns multiple values.\"\"\"\n",
    "            nodes = []\n",
    "            for idx in range(num):\n",
    "                nodes.append(Fold.Node(\n",
    "                    self.op, self.step, self.index, *self.args))\n",
    "                nodes[-1].split_idx = idx\n",
    "            return tuple(nodes)\n",
    "\n",
    "        def nobatch(self):\n",
    "            self.batch = False\n",
    "            return self\n",
    "\n",
    "        def get(self, values):\n",
    "            return values[self.step][self.op].get(self.index, self.split_idx)\n",
    "\n",
    "        def __repr__(self):\n",
    "            return \"[%d:%d]%s\" % (\n",
    "                self.step, self.index, self.op)\n",
    "\n",
    "    class ComputedResult(object):\n",
    "        def __init__(self, batch_size, batched_result):\n",
    "            self.batch_size = batch_size\n",
    "            self.result = batched_result\n",
    "            if isinstance(self.result, tuple):\n",
    "                self.result = list(self.result)\n",
    "\n",
    "        def try_get_batched(self, nodes):\n",
    "            all_are_nodes = all(isinstance(n, Fold.Node) for n in nodes)\n",
    "            num_nodes_is_equal = len(nodes) == self.batch_size\n",
    "            if not all_are_nodes or not num_nodes_is_equal:\n",
    "                return None\n",
    "\n",
    "            valid_node_sequence = all(\n",
    "                nodes[i].index < nodes[i + 1].index  # Indices are ordered\n",
    "                and nodes[i].split_idx == nodes[i + 1].split_idx  # Same split index\n",
    "                and nodes[i].step == nodes[i + 1].step  # Same step\n",
    "                and nodes[i].op == nodes[i + 1].op  # Same op\n",
    "                for i in range(len(nodes) - 1))\n",
    "            if not valid_node_sequence:\n",
    "                return None\n",
    "\n",
    "            if nodes[0].split_idx == -1 and not isinstance(self.result, tuple):\n",
    "                return self.result\n",
    "            elif nodes[0].split_idx >= 0 and not isinstance(self.result[nodes[0].split_idx], tuple):\n",
    "                return self.result[nodes[0].split_idx]\n",
    "            else:\n",
    "                # This result was already chunked.\n",
    "                return None\n",
    "\n",
    "        def get(self, index, split_idx=-1):\n",
    "            if split_idx == -1:\n",
    "                if not isinstance(self.result, tuple):\n",
    "                    self.result = torch.chunk(self.result, self.batch_size)\n",
    "                return self.result[index]\n",
    "            else:\n",
    "                if not isinstance(self.result[split_idx], tuple):\n",
    "                    self.result[split_idx] = torch.chunk(self.result[split_idx], self.batch_size)\n",
    "                return self.result[split_idx][index]\n",
    "\n",
    "    def __init__(self, volatile=False, cuda=False):\n",
    "        self.steps = collections.defaultdict(\n",
    "            lambda: collections.defaultdict(list))\n",
    "        self.cached_nodes = collections.defaultdict(dict)\n",
    "        self.total_nodes = 0\n",
    "        self.volatile = volatile\n",
    "        self._cuda = cuda\n",
    "\n",
    "    def cuda(self):\n",
    "        self._cuda = True\n",
    "        return self\n",
    "\n",
    "    def add(self, op, *args):\n",
    "        \"\"\"Add op to the fold.\"\"\"\n",
    "        self.total_nodes += 1\n",
    "        if not all([isinstance(arg, (\n",
    "                Fold.Node, int, torch._C._TensorBase, Variable)) for arg in args]):\n",
    "            \n",
    "            raise ValueError(\n",
    "                \"All args should be Tensor, Variable, int or Node, got: %s\" % str(args))\n",
    "        if args not in self.cached_nodes[op]:\n",
    "            step = max([0] + [arg.step + 1 for arg in args\n",
    "                              if isinstance(arg, Fold.Node)])\n",
    "            node = Fold.Node(op, step, len(self.steps[step][op]), *args)\n",
    "            self.steps[step][op].append(args)\n",
    "            self.cached_nodes[op][args] = node\n",
    "        return self.cached_nodes[op][args]\n",
    "\n",
    "    def _batch_args(self, arg_lists, values):\n",
    "        res = []\n",
    "        for arg in arg_lists:\n",
    "            r = []\n",
    "            if all(isinstance(arg_item, Fold.Node) for arg_item in arg): # Check if everything's a node\n",
    "                assert all(arg[0].batch == arg_item.batch\n",
    "                           for arg_item in arg[1:])\n",
    "                if arg[0].batch:\n",
    "                    batched_arg = values[arg[0].step][arg[0].op].try_get_batched(arg)\n",
    "                    if batched_arg is not None:\n",
    "                        res.append(batched_arg)\n",
    "                    else:\n",
    "                        res.append(\n",
    "                            torch.cat([arg_item.get(values)\n",
    "                                       for arg_item in arg], 0))\n",
    "                else:\n",
    "                    for arg_item in arg[1:]:\n",
    "                        if arg_item != arg[0]:\n",
    "                            raise ValueError(\"Can not use more then one of nobatch argument, got: %s.\" % str(arg_item))\n",
    "                    res.append(arg[0].get(values))\n",
    "            elif all(isinstance(arg_item, int) for arg_item in arg): # Check if everything's an int\n",
    "                if self._cuda:\n",
    "                    var = Variable(\n",
    "                        torch.cuda.LongTensor(arg), volatile=self.volatile)\n",
    "                else:\n",
    "                    var = Variable(\n",
    "                        torch.LongTensor(arg), volatile=self.volatile)\n",
    "                res.append(var)\n",
    "            else: # Check for a mix of tensors and nodes\n",
    "                for arg_item in arg:\n",
    "                    if isinstance(arg_item, Fold.Node):\n",
    "                        assert arg_item.batch\n",
    "                        r.append(arg_item.get(values))\n",
    "                    elif isinstance(arg_item, (torch._C._TensorBase, Variable)):\n",
    "                        r.append(arg_item)\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            'Not allowed to mix Fold.Node/Tensor with int')\n",
    "                res.append(torch.cat(r, 0))\n",
    "        return res\n",
    "\n",
    "    def apply(self, nn, nodes):\n",
    "        \"\"\"Apply current fold to given neural module.\"\"\"\n",
    "        values = {} # dict of dicts where steps are keys\n",
    "        for step in sorted(self.steps.keys()):\n",
    "            values[step] = {}\n",
    "            for op in self.steps[step]:\n",
    "                func = getattr(nn, op) # get the function to call\n",
    "                try:\n",
    "                    batched_args = self._batch_args(\n",
    "                        zip(*self.steps[step][op]), values) # Make a batch out of the calls with the same step and op\n",
    "                except Exception:\n",
    "                    x = self.steps[step][op][0]\n",
    "                    print(\"Error while executing node %s[%d] with args: %s\" % (\n",
    "                        op, step, self.steps[step][op][0]))\n",
    "                    raise\n",
    "                if batched_args:\n",
    "                    arg_size = batched_args[0].size()[0] # Check the size of each element in the batch\n",
    "                else:\n",
    "                    arg_size = 1\n",
    "                res = func(*batched_args) # Call the func, get the result\n",
    "                values[step][op] = Fold.ComputedResult(arg_size, res) # sstore the result in the values dict\n",
    "        try:\n",
    "            return self._batch_args(nodes, values)\n",
    "        except Exception:\n",
    "            print(\"Retrieving %s\" % nodes)\n",
    "            for lst in nodes:\n",
    "                if isinstance(lst[0], Fold.Node):\n",
    "                    print(', '.join([str(x.get(values).size()) for x in lst]))\n",
    "            raise\n",
    "\n",
    "    def __str__(self):\n",
    "        result = ''\n",
    "        for step in sorted(self.steps.keys()):\n",
    "            result += '%d step:\\n' % step\n",
    "            for op in self.steps[step]:\n",
    "                first_el = ''\n",
    "                for arg in self.steps[step][op][0]:\n",
    "                    if first_el: first_el += ', '\n",
    "                    if isinstance(arg, (torch.tensor._TensorBase, Variable)):\n",
    "                        first_el += str(arg.size())\n",
    "                    else:\n",
    "                        first_el += str(arg)\n",
    "                result += '\\t%s = %d x (%s)\\n' % (\n",
    "                    op, len(self.steps[step][op]), first_el)\n",
    "        return result\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "class Unfold(object):\n",
    "    \"\"\"Replacement of Fold for debugging, where it does computation right away.\"\"\"\n",
    "\n",
    "    class Node(object):\n",
    "\n",
    "        def __init__(self, tensor):\n",
    "            self.tensor = tensor\n",
    "\n",
    "        def __repr__(self):\n",
    "            return str(self.tensor)\n",
    "\n",
    "        def nobatch(self):\n",
    "            return self\n",
    "\n",
    "        def split(self, num):\n",
    "            return [Unfold.Node(self.tensor[i]) for i in range(num)]\n",
    "\n",
    "    def __init__(self, nn, volatile=False, cuda=False):\n",
    "        self.nn = nn\n",
    "        self.volatile = volatile\n",
    "        self._cuda = cuda\n",
    "\n",
    "    def cuda(self):\n",
    "        self._cuda = True\n",
    "        return self\n",
    "\n",
    "    def _arg(self, arg):\n",
    "        if isinstance(arg, Unfold.Node):\n",
    "            return arg.tensor\n",
    "        elif isinstance(arg, int):\n",
    "            if self._cuda:\n",
    "                return Variable(torch.cuda.LongTensor([arg]), volatile=self.volatile)\n",
    "            else:\n",
    "                return Variable(torch.LongTensor([arg]), volatile=self.volatile)\n",
    "        else:\n",
    "            return arg\n",
    "\n",
    "    def add(self, op, *args):\n",
    "        values = []\n",
    "        for arg in args:\n",
    "            values.append(self._arg(arg))\n",
    "        res = getattr(self.nn, op)(*values)\n",
    "        return Unfold.Node(res)\n",
    "\n",
    "    def apply(self, nn, nodes):\n",
    "        if nn != self.nn:\n",
    "            raise ValueError(\"Expected that nn argument passed to constructor and passed to apply would match.\")\n",
    "        result = []\n",
    "        for n in nodes:\n",
    "            result.append(torch.cat([self._arg(a) for a in n]))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TreeLSTM(nn.Module):\n",
    "#     def __init__(self, num_units):\n",
    "#         super(TreeLSTM, self).__init__()\n",
    "#         self.num_units = num_units\n",
    "#         self.left = nn.Linear(num_units, 5 * num_units)\n",
    "#         self.right = nn.Linear(num_units, 5 * num_units)\n",
    "\n",
    "#     def forward(self, left_in, right_in):\n",
    "#         lstm_in = self.left(left_in[0])\n",
    "#         lstm_in += self.right(right_in[0])\n",
    "#         a, i, f1, f2, o = lstm_in.chunk(5, 1)\n",
    "#         c = (a.tanh() * i.sigmoid() + f1.sigmoid() * left_in[1] +\n",
    "#              f2.sigmoid() * right_in[1])\n",
    "#         h = o.sigmoid() * c.tanh()\n",
    "#         return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Node:\n",
    "#     \"\"\"\n",
    "#     Node class\n",
    "#     \"\"\"\n",
    "#     def __init__(self, value):\n",
    "#         self.id = value\n",
    "#         self.left = None\n",
    "#         self.right = None\n",
    "#         self.label = 4 \n",
    "        \n",
    "#     def is_leaf(self):\n",
    "#         return self.left is None and self.right is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SPINN(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_classes, size, n_words):\n",
    "#         super(SPINN, self).__init__()\n",
    "#         self.size = size\n",
    "#         self.tree_lstm = TreeLSTM(size)\n",
    "#         self.embeddings = nn.Embedding(n_words, size)\n",
    "#         self.out = nn.Linear(size, n_classes)\n",
    "\n",
    "#     def leaf(self, word_id):\n",
    "#         return self.embeddings(word_id), (torch.FloatTensor(word_id.size()[0], self.size))\n",
    "\n",
    "#     def children(self, left_h, left_c, right_h, right_c):\n",
    "#         return self.tree_lstm((left_h, left_c), (right_h, right_c))\n",
    "\n",
    "#     def logits(self, encoding):\n",
    "#         return self.out(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_tree_regular(model, tree):\n",
    "#     def encode_node(node):\n",
    "#         if node.is_leaf():\n",
    "#             return model.leaf(torch.LongTensor([node.id]))\n",
    "#         else:\n",
    "#             left_h, left_c = encode_node(node.left)\n",
    "#             right_h, right_c = encode_node(node.right)\n",
    "#             return model.children(left_h, left_c, right_h, right_c)\n",
    "#     encoding, _ = encode_node(tree)\n",
    "#     return model.logits(encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_tree_fold(fold, tree):\n",
    "#     def encode_node(node):\n",
    "#         if node.is_leaf():\n",
    "#             return fold.add('leaf', node.id).split(2)\n",
    "#         else:\n",
    "#             left_h, left_c = encode_node(node.left)\n",
    "#             right_h, right_c = encode_node(node.right)\n",
    "#             return fold.add('children', left_h, left_c, right_h, right_c).split(2)\n",
    "#     encoding, _ = encode_node(tree)\n",
    "#     return fold.add('logits', encoding), \"whatever\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n1 = Node(1)\n",
    "# n2 = Node(2)\n",
    "# n3 = Node(3)\n",
    "# n4 = Node(4)\n",
    "# n5 = Node(5)\n",
    "# n6 = Node(6)\n",
    "\n",
    "# n1.left = n3\n",
    "# n1.right = n5\n",
    "\n",
    "# n2.left = n6\n",
    "# n2.right = n4\n",
    "\n",
    "# #n1 and n2 are separate trees\n",
    "# n_classes = 4\n",
    "# size = 10\n",
    "# n_words = 7\n",
    "\n",
    "# model = SPINN(n_classes, size, n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold = Fold()\n",
    "\n",
    "# all_logits, all_labels = [], []\n",
    "# for tree in [n1, n2]:\n",
    "#     all_logits.append(encode_tree_fold(fold, tree))\n",
    "#     all_labels.append(tree.label)\n",
    "\n",
    "# res = fold.apply(model, [all_logits, all_labels])\n",
    "# loss = criterion(res[0], res[1])\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def criterion(a, b):\n",
    "#     return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_logits, all_labels = [], []\n",
    "# for tree in [n1, n2]:\n",
    "#     all_logits.append(encode_tree_regular(model, tree))\n",
    "#     all_labels.append(tree.label)\n",
    "\n",
    "# loss = criterion(torch.cat(all_logits, 0), torch.LongTensor(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from tree_to_sequence.tree_lstm import TreeLSTM\n",
    "# from tree_to_sequence.translating_trees import map_tree, tree_to_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryTreeLSTM(nn.Module):\n",
    "    '''\n",
    "    BinaryTreeLSTM\n",
    "\n",
    "    Takes in a binary tree where each node has a value and a list of children.\n",
    "    Produces a tree of the same size where the value of each node is now encoded.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Initialize tree cell we'll need later.\n",
    "        \"\"\"\n",
    "        super(BinaryTreeLSTM, self).__init__()\n",
    "\n",
    "        self.tree_lstm = TreeCell(input_size, hidden_size, 2)            \n",
    "        self.register_buffer('zero_buffer', torch.zeros(1, hidden_size))\n",
    "        \n",
    "    def encode_none_node(self):\n",
    "        \"\"\"\n",
    "        :return annotations, hidden, cell\n",
    "        \"\"\"\n",
    "        return self.zero_buffer.unsqueeze(1), self.zero_buffer, self.zero_buffer\n",
    "    \n",
    "    # TODO: Later make this stackable\n",
    "    def encode_node_with_children(self, value, leftA, leftH, leftC, rightA, rightH, rightC):\n",
    "        \"\"\"\n",
    "        :return annotations, hidden, cell\n",
    "        \"\"\"\n",
    "        print(\"INPUTS\")\n",
    "        print(\"value\", value.shape)\n",
    "        print(\"annotations\", leftA.shape, rightA.shape)\n",
    "        print(\"hiddens\", leftH.shape, rightH.shape)\n",
    "        print(\"cell states\", leftC.shape, rightC.shape)\n",
    "        newH, newC = self.tree_lstm(value, [leftH, rightH], [leftC, rightC])\n",
    "        newA = newH.unsqueeze(1)\n",
    "        \n",
    "        newA = torch.cat([newA, leftA.float(), rightA.float()])\n",
    "        return newA, newH, newC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeCell(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Cell which takes in arbitrary numbers of hidden and cell states (one per child).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_children):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM cell.\n",
    "\n",
    "        :param input_size: length of input vector\n",
    "        :param hidden_size: length of hidden vector (and cell state)\n",
    "        :param num_children: number of children = number of hidden/cell states passed in\n",
    "        \"\"\"\n",
    "        super(TreeCell, self).__init__()\n",
    "\n",
    "        # Gates = input, output, memory + one forget gate per child\n",
    "        numGates = 3 + num_children\n",
    "\n",
    "        self.gates_value = nn.ModuleList()\n",
    "        self.gates_children = nn.ModuleList()\n",
    "        for _ in range(numGates):\n",
    "            # One linear layer to handle the value of the node\n",
    "            value_linear = nn.Linear(input_size, hidden_size, bias=True)\n",
    "            children_linear = nn.ModuleList()\n",
    "            # One per child of the node\n",
    "            for _ in range(num_children):\n",
    "                children_linear.append(nn.Linear(hidden_size, hidden_size, bias=False))\n",
    "            self.gates_value.append(value_linear)\n",
    "            self.gates_children.append(children_linear)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input, hidden_states, cell_states):\n",
    "        \"\"\"\n",
    "        Calculate a new hidden state and a new cell state from the LSTM gates\n",
    "\n",
    "        :param hidden_states: A list of num_children hidden states.\n",
    "        :param cell_states: A list of num_children cell states.\n",
    "        :return A tuple containing (new hidden state, new cell state)\n",
    "        \"\"\"\n",
    "\n",
    "        data_sums = []\n",
    "\n",
    "        for i in range(len(self.gates_value)):\n",
    "            data_sum = self.gates_value[i](input)\n",
    "            for j in range(len(hidden_states)):\n",
    "                data_sum += self.gates_children[i][j](hidden_states[j])\n",
    "            data_sums.append(data_sum)\n",
    "\n",
    "        # First gate is the input gate\n",
    "        input_val = self.sigmoid(data_sums[0])\n",
    "        # Next output gate\n",
    "        o = self.sigmoid(data_sums[1])\n",
    "        # Next memory gate\n",
    "        m = self.tanh(data_sums[2])\n",
    "        # All the rest are forget gates\n",
    "        forget_data = 0\n",
    "        for i in range(len(cell_states)):\n",
    "            forget_data += self.sigmoid(data_sums[3 + i]) * cell_states[i]\n",
    "\n",
    "        # Put it all together!\n",
    "        new_state = input_val * m + forget_data\n",
    "        new_hidden = o * self.tanh(new_state)\n",
    "\n",
    "        return new_hidden, new_state\n",
    "\n",
    "    def initialize_forget_bias(self, bias_value):\n",
    "        for i in range(3, len(self.gates_value)):\n",
    "            nn.init.constant_(self.gates_value[i].bias, bias_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(fold, node):\n",
    "    value = node.value\n",
    "    if value is None:\n",
    "        return fold.add('encode_none_node').split(3)\n",
    "    \n",
    "    # List of tuples: (node, cell state)\n",
    "    children = []\n",
    "\n",
    "    for child in node.children:\n",
    "        encoded = forward(fold, child)\n",
    "        children += list(encoded)\n",
    "        \n",
    "    while len(children) < 6:\n",
    "        children += fold.add('encode_none_node').split(3)\n",
    "        \n",
    "    return  fold.add('encode_node_with_children', value, *children).split(3)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsqueeze(node):\n",
    "    if node.value is not None:\n",
    "        node.value = node.value.unsqueeze(0)\n",
    "    for child in node.children:\n",
    "        unsqueeze(child)\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for_lambda_dset = ForLambdaDataset(\"ANC/AdditionalForDatasets/ForWithLevels/Easy-arbitraryForList.json\", binarize_input=True, \n",
    "                                   binarize_output=True, eos_token=True, one_hot=True, \n",
    "                                   long_base_case=False, input_as_seq=False,\n",
    "                                   output_as_seq=False)\n",
    "start = datetime.datetime.now()\n",
    "trees = [unsqueeze(tree[0]) for tree in for_lambda_dset]\n",
    "trees = [tree[0] for tree in for_lambda_dset]\n",
    "tree_lstm = BinaryTreeLSTM(trees[0].value.size()[-1], 3)\n",
    "fold = Fold()\n",
    "result = [forward(fold, tree) for tree in trees]\n",
    "annotations = [x[0] for x in result]\n",
    "hiddens = [x[1] for x in result]\n",
    "print(\"INTERMEDIATE HIDDENS\", hiddens)\n",
    "cell_states = [x[2] for x in result]\n",
    "x = fold.apply(tree_lstm, [annotations, hiddens, cell_states])\n",
    "end = datetime.datetime.now()\n",
    "print(\"done\")\n",
    "print(\"DIFF\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0:00:00.004358\n",
    "        \n",
    "# NEW\n",
    "# 0:00:11.194712\n",
    "#         0:00:10.217078\n",
    "#                 0:00:13.408681\n",
    "#                         0:00:15.319642\n",
    "                                \n",
    "# OLD\n",
    "# 0:00:52.612726\n",
    "#         0:00:50.966683\n",
    "#                 0:00:52.092692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(fold, decoder_hiddens, decoder_cell_states, targetNode, parent_val, child_index, annotations): # Assumes teacher forcing is true\n",
    "    \n",
    "    et = fold.add(\"calc_attention\", decoder_hiddens, annotations)\n",
    "    loss = fold.add(\"calc_loss\", parent_val, child_index, et, targetNode.value)\n",
    "    next_input = targetNode.value\n",
    "\n",
    "    decoder_input = fold.add(\"get_next_decoder_input\", next_input, et)\n",
    "\n",
    "    for i, child in enumerate(targetNode.children):\n",
    "        # Parent node of a node's children is that node\n",
    "        parent = next_input\n",
    "#         new_child_index = fold.add(\"identity\", i).nobatch()\n",
    "        new_child_index = i\n",
    "#         new_lstm = fold.add('get_lstm', new_child_index)#.nobatch()\n",
    "        child_hiddens, child_cell_states = fold.add(\"get_next_child_states\", parent, \n",
    "                                                                new_child_index,\n",
    "                                                                 decoder_input, \n",
    "                                                                 decoder_hiddens, \n",
    "                                                                 decoder_cell_states).split(2)\n",
    "#         child_hiddens, child_cell_states = fold.add(\"get_next_child_states\", parent, new_child_index, \n",
    "#                                                                  decoder_input, \n",
    "#                                                                  decoder_hiddens, \n",
    "#                                                                  decoder_cell_states).split(2)\n",
    "\n",
    "        new_loss = decode(fold, child_hiddens, child_cell_states, child, parent, new_child_index, annotations)\n",
    "        loss = fold.add(\"plus\", loss, new_loss)\n",
    "        \n",
    "    return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeToTreeAttention(nn.Module):\n",
    "    def __init__(self, decoder, hidden_size, embedding_size, nclass,\n",
    "                 root_value=-1, alignment_size=50, align_type=1, max_size=50): #TODO: Add encoder back!\n",
    "        \"\"\"\n",
    "        Translates an encoded representation of one tree into another\n",
    "        \"\"\"\n",
    "        super(TreeToTreeAttention, self).__init__()\n",
    "        \n",
    "        # Save useful values\n",
    "        self.nclass = nclass\n",
    "#         self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.align_type = align_type\n",
    "        self.max_size = max_size\n",
    "        self.root_value = root_value\n",
    "        \n",
    "        # EOS is always the last token\n",
    "        self.EOS_value = nclass\n",
    "        \n",
    "        # Useful functions\n",
    "        self.softmax = nn.Softmax(0)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Set up attention\n",
    "        if align_type == 0:\n",
    "            self.attention_hidden = nn.Linear(hidden_size, alignment_size)\n",
    "            self.attention_context = nn.Linear(hidden_size, alignment_size, bias=False)\n",
    "            self.attention_alignment_vector = nn.Linear(alignment_size, 1)\n",
    "        elif align_type == 1:\n",
    "            self.attention_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "            \n",
    "        self.attention_presoftmax = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.embedding = nn.Embedding(nclass + 1, embedding_size)  \n",
    "    \n",
    "    def calc_attention(self, decoder_hiddens, annotations):\n",
    "        #TODO: Move this part to an outer func\n",
    "        if self.align_type <= 1:\n",
    "            attention_hidden_values = self.attention_hidden(annotations)\n",
    "        else:\n",
    "            attention_hidden_values = annotations\n",
    "        \n",
    "        # Use attention and past hidden state to generate scores\n",
    "        decoder_hidden = decoder_hiddens[-1].unsqueeze(0)\n",
    "        attention_logits = self.attention_logits(attention_hidden_values, decoder_hidden)\n",
    "        attention_probs = self.softmax(attention_logits) # number_of_nodes x 1\n",
    "        \n",
    "        context_vec = (attention_probs * annotations).sum(1).unsqueeze(1) #1 x 1 x hidden_size\n",
    "#         print(\"INPUTS\")\n",
    "#         print(\"ANNOTATIONS\", annotations.shape)\n",
    "#         print(\"HHH\", decoder_hiddens.shape)\n",
    "#         print(\"CCC\", context_vec.shape)\n",
    "        et = self.tanh(self.attention_presoftmax(torch.cat((decoder_hiddens, context_vec), \n",
    "                                                       dim=2))) # 1 x hidden_size\n",
    "        return et\n",
    "    \n",
    "    def calc_loss(self, parent, child_index, et, true_value): # this should be deocder specific\n",
    "        return self.decoder.calculate_loss(parent, child_index, et, true_value)\n",
    "    \n",
    "    def get_next_decoder_input(self, next_input, et):\n",
    "        print(\"ET\", et.shape)\n",
    "        print(\"embedding\", self.embedding(next_input).shape)\n",
    "        print(\"INPUT\", next_input.shape)\n",
    "        return torch.cat((self.embedding(next_input), et), 2)\n",
    "        \n",
    "    def get_next_child_states(self, parent, child_index, input, hidden_state, cell_state): # should be decoder specific\n",
    "        return self.decoder.get_next_child_states(parent, child_index, input, hidden_state, cell_state)\n",
    "    \n",
    "    def plus(self, first, second):\n",
    "        return first + second\n",
    "    \n",
    "    def attention_logits(self, attention_hidden_values, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Calculates the logits over the nodes in the input tree.\n",
    "        \"\"\"\n",
    "        if self.align_type == 0:\n",
    "            return self.attention_alignment_vector(self.tanh(self.attention_context(decoder_hidden) \n",
    "                                                             + attention_hidden_values))\n",
    "        else:\n",
    "            return (decoder_hidden * attention_hidden_values).sum(1).unsqueeze(1) \n",
    "        \n",
    "    def decode(self, fold, decoder_hiddens, decoder_cell_states, targetNode, parent_val, child_index, annotations): # Assumes teacher forcing is true\n",
    "    \n",
    "#         print(\"ORIG\")\n",
    "#         print(\"HH\", decoder_hiddens.shape)\n",
    "#         print(\"CC\", decoder_cell_states.shape)\n",
    "#         print(\"AA\", annotations.shape)\n",
    "        et = fold.add(\"calc_attention\", decoder_hiddens, annotations)\n",
    "        loss = fold.add(\"calc_loss\", parent_val, child_index, et, targetNode.value)\n",
    "        next_input = targetNode.value\n",
    "\n",
    "        decoder_input = fold.add(\"get_next_decoder_input\", next_input, et)\n",
    "\n",
    "        for i, child in enumerate(targetNode.children):\n",
    "            # Parent node of a node's children is that node\n",
    "            parent = next_input\n",
    "    #         new_child_index = fold.add(\"identity\", i).nobatch()\n",
    "            new_child_index = i\n",
    "    #         new_lstm = fold.add('get_lstm', new_child_index)#.nobatch()\n",
    "            child_hiddens, child_cell_states = fold.add(\"get_next_child_states\", parent, \n",
    "                                                                    new_child_index,\n",
    "                                                                     decoder_input, \n",
    "                                                                     decoder_hiddens, \n",
    "                                                                     decoder_cell_states).split(2)\n",
    "    #         child_hiddens, child_cell_states = fold.add(\"get_next_child_states\", parent, new_child_index, \n",
    "    #                                                                  decoder_input, \n",
    "    #                                                                  decoder_hiddens, \n",
    "    #                                                                  decoder_cell_states).split(2)\n",
    "\n",
    "            new_loss = decode(fold, child_hiddens, child_cell_states, child, parent, new_child_index, annotations)\n",
    "            loss = fold.add(\"plus\", loss, new_loss)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeDecoder(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, max_num_children, nclass):\n",
    "        \"\"\"\n",
    "        :param embedding_size: length of the encoded representation of a node\n",
    "        :param hidden_size: hidden state size\n",
    "        :param max_num_children: max. number of children a node can have\n",
    "        :param nclass: number of different tokens which could be in a tree (not counting end\n",
    "                       of tree token)\n",
    "        \"\"\"\n",
    "        super(TreeDecoder, self).__init__()\n",
    "                \n",
    "#         self.loss_func = nn.CrossEntropyLoss() #TODO: make this work!!!\n",
    "        \n",
    "        # Linear layer to calculate log odds. The one is to account for the eos token.\n",
    "        self.output_log_odds = nn.Linear(hidden_size, nclass + 1)        \n",
    "        \n",
    "        # Create a separate lstm for each child index\n",
    "        self.lstm_list = nn.ModuleList()\n",
    "        \n",
    "        self.max_num_children = max_num_children\n",
    "        \n",
    "        for i in range(self.max_num_children):\n",
    "            self.lstm_list.append(nn.LSTMCell(embedding_size + hidden_size, hidden_size))\n",
    "            \n",
    "    def loss_func(self, a, b):\n",
    "        print(\"FIRST\", a.shape)\n",
    "        print(\"SECOND\", b.shape)\n",
    "        return torch.ones(a.size()[0], 1)\n",
    "#         return a.float() - b.float() #TODO: do this correctly!\n",
    "    \n",
    "    def calculate_loss(self, parent, child_index, et, true_value):\n",
    "        \"\"\"\n",
    "        Calculate cross entropy loss from et.\n",
    "        \n",
    "        :param parent: node's parent (dummy; used for compatibility with grammar decoder)\n",
    "        :param child_index: index of generated child (dummy; used for compatibility with grammar \n",
    "                            decoder)\n",
    "        :param et: vector incorporating info from the attention and hidden state of past node\n",
    "        :param true_value: true value of the new node\n",
    "        :returns: cross entropy loss\n",
    "        \"\"\"\n",
    "        log_odds = self.output_log_odds(et)\n",
    "        return self.loss_func(log_odds, true_value)\n",
    "    \n",
    "#     def get_lstm(self, child_index):\n",
    "#         return self.lstm_list[0]\n",
    "#         return self.lstm_list[child_index] #TODO: ACTUALLY DO SOMETHING!\n",
    "    \n",
    "    def get_next_child_states(self, parent, child_index, input, hidden_state, cell_state):\n",
    "        \"\"\"\n",
    "        Generate the hidden and cell states which will be used to generate the current node's \n",
    "        children\n",
    "        \n",
    "        :param parent: node's parent (dummy; used for compatibility with grammar decoder)\n",
    "        :param child_index: index of generated child\n",
    "        :param input: embedded reprentation of the node's parent\n",
    "        :param hidden_state: hidden state generated by an lstm\n",
    "        :param cell_state: cell state generated by an lstm\n",
    "        \"\"\"\n",
    "#         lstm(input, (hidden_state, cell_state))\n",
    "        hiddens = []\n",
    "        cell_states = []\n",
    "        for i in child_index:\n",
    "            hidden, cell = self.lstm_list[i](input[i], (hidden_state[i], cell_state[i]))\n",
    "            hiddens.append(hidden)\n",
    "            cell_states.append(cell)\n",
    "        return torch.cat(hiddens, dim=0).unsqueeze(1), torch.cat(cell_states, dim=0).unsqueeze(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "num_vars = 10\n",
    "num_ints = 11\n",
    "input_as_seq = False\n",
    "output_as_seq = False\n",
    "one_hot = False\n",
    "binarize_input = True\n",
    "binarize_output = False\n",
    "eos_token = True\n",
    "long_base_case = True\n",
    "input_size = num_vars + num_ints + len(for_ops.keys()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarize_output=True\n",
    "num_vars = 10\n",
    "num_ints = 11\n",
    "embedding_size = 256\n",
    "hidden_size = 3\n",
    "num_layers = 1\n",
    "alignment_size = 50\n",
    "align_type = 1\n",
    "encoder_input_size = num_vars + num_ints + len(for_ops)\n",
    "nclass = num_vars + num_ints + len(lambda_ops)\n",
    "plot_every = 100\n",
    "max_num_children = 2 if binarize_output else 4\n",
    "max_size=50\n",
    "\n",
    "decoder = TreeDecoder(embedding_size, hidden_size, max_num_children, nclass=nclass)\n",
    "program_model = TreeToTreeAttention(decoder, hidden_size, embedding_size, nclass=nclass, max_size=max_size,\n",
    "                                    alignment_size=alignment_size, align_type=align_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for_lambda_dset = ForLambdaDataset(\"ANC/AdditionalForDatasets/ForWithLevels/Easy-arbitraryForList.json\", binarize_input=binarize_input, \n",
    "                                   binarize_output=binarize_output, eos_token=eos_token, one_hot=one_hot, \n",
    "                                   long_base_case=long_base_case, input_as_seq=input_as_seq,\n",
    "                                   output_as_seq=output_as_seq)\n",
    "start = datetime.datetime.now()\n",
    "trees = [unsqueeze(tree[0]) for tree in for_lambda_dset]\n",
    "embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "trees = [tree[0] for tree in for_lambda_dset]\n",
    "if not one_hot:\n",
    "    trees = [map_tree(lambda node: embedding(node).squeeze(0), tree) for tree in trees]\n",
    "target_trees = [map_tree(lambda node: node.unsqueeze(0), tree[1]) for tree in for_lambda_dset]\n",
    "print(\"VAL SHAPES\", trees[0].value.shape, target_trees[0].value.shape)\n",
    "fold = Fold()\n",
    "result = [forward(fold, tree) for tree in trees]\n",
    "annotations = [x[0] for x in result]\n",
    "hiddens = [x[1] for x in result]\n",
    "cell_states = [x[2] for x in result]\n",
    "tree_lstm = BinaryTreeLSTM(trees[0].value.size()[-1], 3)\n",
    "x = fold.apply(tree_lstm, [annotations, hiddens, cell_states])\n",
    "print(\"early stuff worked!\")\n",
    "fold2 = Fold()\n",
    "annotations_real = [vec.unsqueeze(0) for vec in x[0]]\n",
    "hiddens_real = [vec.unsqueeze(0).unsqueeze(0) for vec in x[1]]\n",
    "cell_states_real = [vec.unsqueeze(0).unsqueeze(0) for vec in x[2]]\n",
    "\n",
    "losses = [program_model.decode(fold2, hidden, cell_state, tree, -1, 0, annotation) for hidden, cell_state, tree, annotation in zip(hiddens_real, cell_states_real, target_trees, annotations_real)]\n",
    "y = fold2.apply(program_model, [losses])\n",
    "end = datetime.datetime.now()\n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
