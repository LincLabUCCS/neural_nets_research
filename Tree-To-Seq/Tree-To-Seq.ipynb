{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets_library import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "from translating_trees import *\n",
    "from for_prog_dataset import ForDataset\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_lambda_dset = ForDataset('ANC/Easy-arbitraryForList.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TreeCell(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Cell which takes in arbitrary numbers of hidden and cell states (one per child).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_children):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM cell.\n",
    "        \n",
    "        :param input_size: length of input vector\n",
    "        :param hidden_size: length of hidden vector (and cell state)\n",
    "        :param num_children: number of children = number of hidden/cell states passed in\n",
    "        \"\"\"\n",
    "        super(TreeCell, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Gates = input, output, memory + one forget gate per child\n",
    "        numGates = 3 + num_children\n",
    "        \n",
    "        self.gates_value = torch.nn.ModuleList()\n",
    "        self.gates_children = torch.nn.ModuleList()\n",
    "        for _ in range(numGates):\n",
    "            # One linear layer to handle the value of the node\n",
    "            value_linear = nn.Linear(input_size, hidden_size, bias = True)\n",
    "            children_linear = torch.nn.ModuleList()\n",
    "            # One per child of the node\n",
    "            for _ in range(num_children):\n",
    "                children_linear.append(nn.Linear(hidden_size, hidden_size, bias = False))\n",
    "            self.gates_value.append(value_linear)\n",
    "            self.gates_children.append(children_linear)\n",
    "            \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input, hidden_states, cell_states):\n",
    "        \"\"\"\n",
    "        Calculate a new hidden state and a new cell state from the LSTM gates\n",
    "        \n",
    "        :param hidden_states: A list of num_children hidden states.\n",
    "        :param cell_states: A list of num_children cell states.\n",
    "        :return A tuple containing (new hidden state, new cell state)\n",
    "        \"\"\"\n",
    "        \n",
    "        data_sums = []\n",
    "        for i in range(len(self.gates_value)):\n",
    "            data_sum = self.gates_value[i](input)\n",
    "            for j in range(len(hidden_states)):\n",
    "                data_sum += self.gates_children[i][j](hidden_states[j])\n",
    "            data_sums.append(data_sum)\n",
    "            \n",
    "        # First gate is the input gate\n",
    "        i = self.sigmoid(data_sums[0])\n",
    "        # Next output gate\n",
    "        o = self.sigmoid(data_sums[1])\n",
    "        # Next memory gate\n",
    "        m = self.tanh(data_sums[2])\n",
    "        # All the rest are forget gates\n",
    "        forget_data = 0\n",
    "        for i in range(len(cell_states)):\n",
    "            forget_data += data_sums[3 + i] * cell_states[i]\n",
    "        \n",
    "        # Put it all together!\n",
    "        new_state = i * m + forget_data\n",
    "        new_hidden = o * self.tanh(new_state)\n",
    "                \n",
    "        return new_hidden, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TreeLSTM(nn.Module):\n",
    "    '''\n",
    "    TreeLSTM\n",
    "\n",
    "    Takes in a tree where each node has a value and a list of children.\n",
    "    Produces a tree of the same size where the value of each node is now encoded.\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, valid_num_children):\n",
    "        \"\"\"\n",
    "        Initialize tree cells we'll need later.\n",
    "        \"\"\"\n",
    "        super(TreeLSTM, self).__init__()\n",
    "        \n",
    "        self.valid_num_children = [0] + valid_num_children\n",
    "        self.lstm_list = torch.nn.ModuleList()\n",
    "        \n",
    "        for size in self.valid_num_children:\n",
    "            self.lstm_list.append(TreeCell(input_size, hidden_size, size))\n",
    "        \n",
    "    def forward(self, node):\n",
    "        \"\"\"\n",
    "        Creates a tree where each node's value is the encoded version of the original value.\n",
    "        \n",
    "        :param tree: a tree where each node has a value vector and a list of children\n",
    "        :return a tuple - (root of encoded tree, cell state)\n",
    "        \"\"\"\n",
    "        # List of tuples: (node, cell state)\n",
    "        children = []\n",
    "        \n",
    "        # Recursively encode children\n",
    "        for child in node.children:\n",
    "            encoded_child = self.forward(child)\n",
    "            children.append(encoded_child)\n",
    "\n",
    "        # Extract the TreeCell inputs\n",
    "        inputH = [vec[0].value for vec in children]\n",
    "        inputC = [vec[1] for vec in children]\n",
    "        \n",
    "        value = node.value\n",
    "        \n",
    "        found = False\n",
    "        \n",
    "        # Feed the inputs into the TreeCell with the appropriate number of children.        \n",
    "        for i in range(len(self.valid_num_children)):\n",
    "            if self.valid_num_children[i] == len(children):\n",
    "                newH, newC = self.lstm_list[i](value, inputH, inputC)\n",
    "                found = True\n",
    "                break\n",
    "                \n",
    "        if not found:\n",
    "            print(\"WHAAAAAT?\")\n",
    "            raise ValueError(\"Beware.  Something has gone horribly wrong.  You may not have long to live.\")\n",
    "        \n",
    "        \n",
    "        # Set our encoded vector as the root of the new tree\n",
    "        rootNode = Node(newH)\n",
    "        rootNode.children = [vec[0] for vec in children]\n",
    "        return (rootNode, newC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     21
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes in a tree where each node has a value vector and a list of children\n",
    "    Produces a sequence encoding of the tree\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, valid_num_children, attention=True):\n",
    "        \"\"\"\n",
    "        Initialize the TreeLSTMs we'll need later (one per layer)\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.lstm_list = torch.nn.ModuleList()\n",
    "        \n",
    "        # All TreeLSTMs have input of hidden_size except the first.\n",
    "        self.lstm_list.append(TreeLSTM(input_size, hidden_size, valid_num_children))\n",
    "        for i in range(num_layers-1):\n",
    "            self.lstm_list.append(TreeLSTM(hidden_size, hidden_size, valid_num_children))\n",
    "        \n",
    "        self.attention = attention\n",
    "\n",
    "    def forward(self, tree):\n",
    "        \"\"\"\n",
    "        Encodes nodes of a tree in the rows of a matrix.\n",
    "        \n",
    "        :param tree: a tree where each node has a value vector and a list of children\n",
    "        :return a matrix where each row represents the encoded output of a single node and also\n",
    "                the hidden states of the root node.\n",
    "        \n",
    "        \"\"\"\n",
    "        hiddens = []\n",
    "        \n",
    "        for lstm in self.lstm_list:\n",
    "            tree = lstm(tree)[0]\n",
    "            hiddens.append(tree.value)\n",
    "        \n",
    "        hiddens = torch.stack(hiddens)\n",
    "        \n",
    "        if self.attention:\n",
    "            return torch.stack(tree_to_list(tree)), hiddens\n",
    "        else:\n",
    "            return hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     77,
     84,
     114
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Decoder\n",
    "'''\n",
    "class Tree_to_Sequence_Model(nn.Module):\n",
    "    \"\"\"\n",
    "      For the decoder this expects something like an lstm cell or a gru cell and not an lstm/gru.\n",
    "      Batch size is not supported at all. More precisely the encoder expects an input that does not\n",
    "      appear in batches, while the decoder must work with batches, but will only be used with a batch size\n",
    "      of 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, hidden_size, nclass, embedding_size,\n",
    "                 use_lstm=False):\n",
    "        super(Tree_to_Sequence_Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        # nclass + 2 to include end of sequence and trash\n",
    "        self.output_log_odds = nn.Linear(hidden_size, nclass+2)\n",
    "        self.extract_initial_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "\n",
    "        self.register_buffer('SOS_token', torch.LongTensor([[nclass+2]]))\n",
    "        self.EOS_value = nclass + 1\n",
    "\n",
    "        # nclass + 3 to include start of sequence, end of sequence, and trash.\n",
    "        # n + 2 - start of sequence, end of sequence - n + 1, trash - n.\n",
    "        # The first n correspond to the alphabet in order.\n",
    "        self.embedding = nn.Embedding(nclass+3, embedding_size)\n",
    "\n",
    "        self.use_lstm = use_lstm\n",
    "        # nclass is the trash category to avoid penalties after target's EOS token\n",
    "        self.loss_func = nn.CrossEntropyLoss(ignore_index=nclass)\n",
    "\n",
    "        if use_lstm:\n",
    "            self.register_buffer('decoder_initial_cell_state', torch.zeros(hidden_size))\n",
    "\n",
    "    \"\"\"\n",
    "        input: The output of the encoder for the input should correspond to the hidden state of the root. \n",
    "               It should be [num_layers, hidden_size].\n",
    "        target: The target should have dimension, seq_len, and should be a LongTensor.\n",
    "    \"\"\"\n",
    "    def forward_train(self, input, target, teacher_forcing=True):\n",
    "        \n",
    "        # root hidden state\n",
    "        decoder_hiddens = self.encoder(input).unsqueeze(1) # num_layers x 1 x hidden_size\n",
    "        decoder_hiddens = self.tanh(self.extract_initial_hidden(decoder_hiddens))\n",
    "        num_layers, _, _ = decoder_hiddens.size()\n",
    "        \n",
    "        target_length, = target.size()\n",
    "        SOS_token = Variable(self.SOS_token)\n",
    "        decoder_input = self.embedding(SOS_token).squeeze(0) # 1 x embedding_size\n",
    "        loss = 0\n",
    "\n",
    "        if self.use_lstm:\n",
    "            decoder_initial_cell_state = Variable(self.decoder_initial_cell_state)\n",
    "            decoder_cell_states = torch.stack([decoder_initial_cell_state for _ in range(num_layers)]).unsqueeze(1)\n",
    "\n",
    "        for i in range(target_length):\n",
    "            if self.use_lstm:               \n",
    "                decoder_hiddens, decoder_cell_states = self.decoder(decoder_input, (decoder_hiddens, decoder_cell_states)) # num_layers x 1 x hidden_size\n",
    "            else:\n",
    "                decoder_hiddens = self.decoder(decoder_input, decoder_hiddens)\n",
    "\n",
    "            decoder_hidden = decoder_hiddens[-1] # 1 x hidden_size\n",
    "            log_odds = self.output_log_odds(decoder_hidden)\n",
    "            \n",
    "            loss += self.loss_func(log_odds, target[i])\n",
    "\n",
    "            if teacher_forcing:\n",
    "                next_input = target[i].unsqueeze(1)\n",
    "            else:\n",
    "                _, next_input = log_odds.topk(1)\n",
    "\n",
    "            decoder_input = self.embedding(next_input).squeeze(1) # 1 x embedding_size\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \"\"\"\n",
    "        This is just an alias for point_wise_prediction, so that training code that assumes the presence\n",
    "        of a forward_train and forward_prediction works.\n",
    "    \"\"\"\n",
    "    def forward_prediction(self, input, maximum_length=20):\n",
    "        return self.point_wise_prediction(input, maximum_length)\n",
    "    \n",
    "    def point_wise_prediction(self, input, maximum_length=20):\n",
    "        decoder_hiddens = self.encoder(input).unsqueeze(1)\n",
    "        decoder_hiddens = self.tanh(self.extract_initial_hidden(decoder_hiddens))\n",
    "        num_layers, _, _ = decoder_hiddens.size()\n",
    "        SOS_token = Variable(self.SOS_token)\n",
    "\n",
    "        decoder_input = self.embedding(SOS_token).squeeze(0) # 1 x embedding_size\n",
    "        output_so_far = []\n",
    "\n",
    "        if self.use_lstm:\n",
    "            decoder_initial_cell_state = Variable(self.decoder_initial_cell_state)\n",
    "            decoder_cell_states = torch.stack([decoder_initial_cell_state for _ in range(num_layers)]).unsqueeze(1) # num_layers x 1 x hidden_size\n",
    "\n",
    "        for _ in range(maximum_length):\n",
    "            if self.use_lstm:\n",
    "                decoder_hiddens, decoder_cell_states = self.decoder(decoder_input, (decoder_hiddens, decoder_cell_states))\n",
    "            else:\n",
    "                decoder_hiddens = self.decoder(decoder_input, decoder_hiddens)\n",
    "\n",
    "            decoder_hidden = decoder_hiddens[-1]\n",
    "            log_odds = self.output_log_odds(decoder_hidden)\n",
    "\n",
    "            _, next_input = log_odds.topk(1)\n",
    "\n",
    "            if int(next_input) == self.EOS_value:\n",
    "                break\n",
    "\n",
    "            output_so_far.append(int(next_input))\n",
    "            decoder_input = self.embedding(next_input).squeeze(1) # 1 x embedding size\n",
    "\n",
    "        return output_so_far\n",
    "\n",
    "    def beam_search_prediction(self, input, maximum_length=7, beam_width=5):\n",
    "        decoder_hiddens = self.encoder(input).unsqueeze(1)\n",
    "        decoder_hiddens = self.tanh(self.extract_initial_hidden(decoder_hiddens))\n",
    "        num_layers, _, _ = decoder_hiddens.size()\n",
    "\n",
    "        SOS_token = Variable(self.SOS_token)\n",
    "        decoder_input = self.embedding(SOS_token).squeeze(0) # 1 x embedding_size\n",
    "        word_inputs = []\n",
    "\n",
    "        if self.use_lstm:\n",
    "            decoder_initial_cell_state = Variable(self.decoder_initial_cell_state)\n",
    "            decoder_cell_states = torch.stack([decoder_initial_cell_state for _ in range(num_layers)]).unsqueeze(1) # num_layers x 1 x hidden_size\n",
    "\n",
    "        for _ in range(beam_width):\n",
    "            if self.use_lstm:\n",
    "                word_inputs.append((0, [], True, [decoder_input, decoder_hiddens, decoder_cell_states]))\n",
    "            else:\n",
    "                word_inputs.append((0, [], True, [decoder_input, decoder_hiddens]))\n",
    "\n",
    "        for _ in range(maximum_length):\n",
    "            new_word_inputs = []\n",
    "\n",
    "            for i in range(beam_width):\n",
    "                if not word_inputs[i][2]:\n",
    "                    new_word_inputs.append(word_inputs[i])\n",
    "                    continue\n",
    "\n",
    "                if self.use_lstm:\n",
    "                    decoder_input, decoder_hiddens, decoder_cell_states = word_inputs[i][3]\n",
    "                    decoder_hiddens, decoder_cell_states = self.decoder(decoder_input, (decoder_hiddens, decoder_cell_states))\n",
    "                else:\n",
    "                    decoder_input, decoder_hiddens = word_inputs[i][3]\n",
    "                    decoder_hiddens = self.decoder(decoder_input, decoder_hiddens)\n",
    "\n",
    "                decoder_hidden = decoder_hiddens[-1]\n",
    "                log_odds = self.output_log_odds(decoder_hidden).squeeze(0) # nclasses\n",
    "                log_probs = self.log_softmax(log_odds)\n",
    "\n",
    "                log_value, next_input = log_probs.topk(beam_width) # beam_width, beam_width\n",
    "                decoder_input = self.embedding(next_input.unsqueeze(1)) # beam_width x 1 x embedding size\n",
    "\n",
    "                if self.use_lstm:\n",
    "                    new_word_inputs.extend((word_inputs[i][0] + float(log_value[k]), word_inputs[i][1] + [int(next_input[k])],\n",
    "                                           int(next_input[k]) != self.EOS_value, [decoder_input[k], decoder_hiddens, decoder_cell_states])\n",
    "                                           for k in range(beam_width))\n",
    "                else:\n",
    "                    new_word_inputs.extend((word_inputs[i][0] + float(log_value[k]), word_inputs[i][1] + [int(next_input[k])],\n",
    "                                           int(next_input[k]) != self.EOS_value, [decoder_input[k], decoder_hiddens])\n",
    "                                           for k in range(beam_width))\n",
    "                    \n",
    "        word_inputs = sorted(new_word_inputs, key=lambda word_input: word_input[0])[-beam_width:]\n",
    "        return word_inputs[-1][1][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "class Tree_to_Sequence_Attention_Model(Tree_to_Sequence_Model):\n",
    "    def __init__(self, encoder, decoder, hidden_size, nclass, embedding_size,\n",
    "                 alignment_size, decoder_cell_state_shape=None, use_lstm=False, use_cuda=True):\n",
    "        super(Tree_to_Sequence_Attention_Model, self).__init__(encoder, decoder, hidden_size, nclass, embedding_size,\n",
    "                                                               use_lstm=use_lstm)\n",
    "        self.attention_hidden = nn.Linear(hidden_size, alignment_size)\n",
    "        self.attention_context = nn.Linear(hidden_size, alignment_size, bias=False)\n",
    "        self.attention_alignment_vector = nn.Linear(alignment_size, 1)\n",
    "\n",
    "    \"\"\"\n",
    "        input: The output of the encoder for the tree should have be a pair. The first \n",
    "               part of the pair should be the annotations and have dimensions, \n",
    "               number_of_nodes x hidden_size. The second part of the pair should be the hidden \n",
    "               representations of the root and should have dimensions, num_layers x hidden_size.\n",
    "        target: The target should have dimensions, seq_len, and should be a LongTensor.\n",
    "    \"\"\"\n",
    "    def forward_train(self, input, target, teacher_forcing=True):\n",
    "        annotations, decoder_hiddens = self.encoder(input)\n",
    "\n",
    "        attention_hidden_values = self.attention_hidden(annotations) # number_of_nodes x alignment_size\n",
    "        decoder_hiddens = self.tanh(self.extract_initial_hidden(decoder_hiddens)).unsqueeze(1) # num_layers x 1 x hidden_size\n",
    "\n",
    "        target_length, = target.size()\n",
    "        num_layers, _, _ = decoder_hiddens.size()\n",
    "        SOS_token = Variable(self.SOS_token)\n",
    "        \n",
    "        word_input = self.embedding(SOS_token).squeeze(0) # 1 x embedding_size\n",
    "        loss = 0\n",
    "\n",
    "        if self.use_lstm:\n",
    "            decoder_initial_cell_state = Variable(self.decoder_initial_cell_state)\n",
    "            decoder_cell_states = torch.stack([self.decoder_initial_cell_state for _ in range(num_layers)]).unsqueeze(1) # num_layers x 1 x hidden_size\n",
    "\n",
    "        for i in range(target_length):\n",
    "            attention_logits = self.attention_alignment_vector(self.tanh(self.attention_context(decoder_hiddens[0]) + attention_hidden_values))\n",
    "            attention_probs = self.softmax(attention_logits) # number_of_nodes x 1\n",
    "            context_vec = (attention_probs * annotations).sum(0).unsqueeze(0) # 1 x hidden_size\n",
    "            decoder_input = torch.cat((word_input, context_vec), dim=1) # 1 x embedding_size + hidden_size\n",
    "\n",
    "            if self.use_lstm:\n",
    "                decoder_hiddens, decoder_cell_states = self.decoder(decoder_input, (decoder_hiddens, decoder_cell_states))\n",
    "            else:\n",
    "                decoder_hiddens = self.decoder(decoder_input, decoder_hiddens)\n",
    "\n",
    "            decoder_hidden = decoder_hiddens[-1]\n",
    "            log_odds = self.output_log_odds(decoder_hidden)\n",
    "            loss += self.loss_func(log_odds, target[i])\n",
    "\n",
    "            if teacher_forcing:\n",
    "                next_input = target[i].unsqueeze(1)\n",
    "            else:\n",
    "                _, next_input = log_odds.topk(1)\n",
    "\n",
    "            word_input = self.embedding(next_input).squeeze(1) # 1 x embedding size\n",
    "    \n",
    "        return loss\n",
    "\n",
    "    \"\"\"\n",
    "        This is just an alias for point_wise_prediction, so that training code that assumes the presence\n",
    "        of a forward_train and forward_prediction works.\n",
    "    \"\"\"\n",
    "    def forward_prediction(self, input, maximum_length=20):\n",
    "        return self.point_wise_prediction(input, maximum_length)\n",
    "    \n",
    "    def point_wise_prediction(self, input, maximum_length=20):\n",
    "        annotations, decoder_hiddens = self.encoder(input)\n",
    "\n",
    "        attention_hidden_values = self.attention_hidden(annotations) # number_of_nodes x alignment_size\n",
    "        decoder_hiddens = self.tanh(self.extract_initial_hidden(decoder_hiddens)).unsqueeze(1) # num_layers x 1 x hidden_size\n",
    "\n",
    "        num_layers, _, _ = decoder_hiddens.size()\n",
    "        SOS_token = Variable(self.SOS_token)\n",
    "        \n",
    "        word_input = self.embedding(SOS_token).squeeze(0) # 1 x embedding_size\n",
    "        output_so_far = []\n",
    "        \n",
    "        if self.use_lstm:\n",
    "            decoder_initial_cell_state = Variable(self.decoder_initial_cell_state)\n",
    "            decoder_cell_states = torch.stack([self.decoder_initial_cell_state for _ in range(num_layers)]).unsqueeze(1) # num_layers x 1 x hidden_size\n",
    "\n",
    "        for i in range(maximum_length):\n",
    "            attention_logits = self.attention_alignment_vector(self.tanh(self.attention_context(decoder_hiddens[0]) + attention_hidden_values))\n",
    "            attention_probs = self.softmax(attention_logits) # number_of_nodes x 1\n",
    "            context_vec = (attention_probs * annotations).sum(0).unsqueeze(0) # 1 x hidden_size\n",
    "            decoder_input = torch.cat((word_input, context_vec), dim=1) # 1 x embedding_size + hidden_size\n",
    "\n",
    "            if self.use_lstm:\n",
    "                decoder_hiddens, decoder_cell_states = self.decoder(decoder_input, (decoder_hiddens, decoder_cell_states))\n",
    "            else:\n",
    "                decoder_hiddens = self.decoder(decoder_input, decoder_hiddens)\n",
    "\n",
    "            decoder_hidden = decoder_hiddens[-1]\n",
    "            log_odds = self.output_log_odds(decoder_hidden)\n",
    "            _, next_input = log_odds.topk(1)\n",
    "\n",
    "            if int(next_input) == self.EOS_value:\n",
    "                break\n",
    "\n",
    "            output_so_far.append(int(next_input))\n",
    "            word_input = self.embedding(next_input).squeeze(1) # 1 x embedding size\n",
    "\n",
    "        return output_so_far\n",
    "\n",
    "    def beam_search_prediction(self, input, maximum_length=20, beam_width=5):\n",
    "        annotations, decoder_hiddens = self.encoder(input)\n",
    "\n",
    "        attention_hidden_values = self.attention_hidden(annotations) # number_of_nodes x alignment_size\n",
    "        decoder_hiddens = self.tanh(self.extract_initial_hidden(decoder_hiddens)).unsqueeze(1) # num_layers x 1 x hidden_size\n",
    "\n",
    "        num_layers, _, _ = decoder_hiddens.size()\n",
    "        SOS_token = Variable(self.SOS_token)\n",
    "        \n",
    "        word_input = self.embedding(SOS_token).squeeze(0) # 1 x embedding_size\n",
    "\n",
    "        word_inputs = []\n",
    "\n",
    "        if self.use_lstm:\n",
    "            decoder_initial_cell_state = Variable(self.decoder_initial_cell_state)\n",
    "            decoder_cell_states = torch.stack([self.decoder_initial_cell_state for _ in range(num_layers)]).unsqueeze(1) # num_layers x 1 x hidden_size\n",
    "\n",
    "        for _ in range(beam_width):\n",
    "            if self.use_lstm:\n",
    "                word_inputs.append((0, [], True, [word_input, decoder_hiddens, decoder_cell_states]))\n",
    "            else:\n",
    "                word_inputs.append((0, [], True, [word_input, decoder_hiddens]))\n",
    "\n",
    "        for _ in range(maximum_length):\n",
    "            new_word_inputs = []\n",
    "\n",
    "            for i in range(beam_width):\n",
    "                if not word_inputs[i][2]:\n",
    "                    new_word_inputs.append(word_inputs[i])\n",
    "                    continue\n",
    "\n",
    "                if self.use_lstm:\n",
    "                    word_input, decoder_hiddens, decoder_cell_states = word_inputs[i][3]\n",
    "                else:\n",
    "                    word_input, decoder_hiddens = word_inputs[i][3]\n",
    "\n",
    "                attention_logits = self.attention_alignment_vector(self.tanh(self.attention_context(decoder_hiddens[0]) + attention_hidden_values))\n",
    "                attention_probs = self.softmax(attention_logits) # number_of_nodes x 1\n",
    "                context_vec = (attention_probs * annotations).sum(0).unsqueeze(0) # 1 x hidden_size\n",
    "                decoder_input = torch.cat((word_input, context_vec), dim=1) # 1 x embedding_size + hidden_size\n",
    "\n",
    "                if self.use_lstm:\n",
    "                    decoder_hiddens, decoder_cell_states = self.decoder(decoder_input, (decoder_hiddens, decoder_cell_states))\n",
    "                else:\n",
    "                    decoder_hiddens = self.decoder(decoder_input, decoder_hiddens)\n",
    "\n",
    "                decoder_hidden = decoder_hiddens[-1]\n",
    "                log_odds = self.output_log_odds(decoder_hidden).squeeze(0) # nclasses\n",
    "                log_probs = self.log_softmax(log_odds)\n",
    "\n",
    "                log_value, next_input = log_probs.topk(beam_width) # beam_width, beam_width\n",
    "                word_input = self.embedding(next_input.unsqueeze(1)) # beam_width x 1 x embedding size\n",
    "\n",
    "                if self.use_lstm:\n",
    "                    new_word_inputs.extend((word_inputs[i][0] + float(log_value[k]), word_inputs[i][1] + [int(next_input[k])],\n",
    "                                           int(next_input[k]) != self.EOS_value, [word_input[k], decoder_hiddens, decoder_cell_states])\n",
    "                                           for k in range(beam_width))\n",
    "                else:\n",
    "                    new_word_inputs.extend((word_inputs[i][0] + float(log_value[k]), word_inputs[i][1] + [int(next_input[k])],\n",
    "                                           int(next_input[k]) != self.EOS_value, [word_input[k], decoder_hiddens])\n",
    "                                           for k in range(beam_width))\n",
    "            word_inputs = sorted(new_word_inputs, key=lambda word_input: word_input[0])[-beam_width:]\n",
    "\n",
    "        return word_inputs[-1][1][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     19
    ]
   },
   "outputs": [],
   "source": [
    "class MultilayerLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_layers, bias=True):\n",
    "        super(MultilayerLSTMCell, self).__init__()\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        \n",
    "        if isinstance(hidden_sizes, int):\n",
    "            temp = []\n",
    "            \n",
    "            for _ in range(num_layers):\n",
    "                temp.append(hidden_sizes)\n",
    "            \n",
    "            hidden_sizes = temp\n",
    "            \n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            curr_lstm = nn.LSTMCell(hidden_sizes[i], hidden_sizes[i+1], bias=bias)\n",
    "            self.lstm_layers.append(curr_lstm)\n",
    "    \n",
    "    def forward(self, input, past_states):\n",
    "        hiddens, cell_states = past_states\n",
    "        result_hiddens, result_cell_states = [], []\n",
    "        curr_input = input\n",
    "        \n",
    "        for lstm_cell, curr_hidden, curr_cell_state in zip(self.lstm_layers, hiddens, cell_states):\n",
    "            curr_input, new_cell_state = lstm_cell(curr_input, (curr_hidden, curr_cell_state))\n",
    "            result_hiddens.append(curr_input)\n",
    "            result_cell_states.append(new_cell_state)\n",
    "        \n",
    "        return torch.stack(result_hiddens), torch.stack(result_cell_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     3,
     19
    ]
   },
   "outputs": [],
   "source": [
    "num_vars = 10\n",
    "num_ints = 11\n",
    "\n",
    "for_ops = {\n",
    "    \"Var\": 0,\n",
    "    \"Const\": 1,\n",
    "    \"Plus\": 2,\n",
    "    \"Minus\": 3,\n",
    "    \"EqualFor\": 4,\n",
    "    \"LeFor\": 5,\n",
    "    \"GeFor\": 6,\n",
    "    \"Assign\": 7,\n",
    "    \"If\": 8,\n",
    "    \"Seq\": 9,\n",
    "    \"For\": 10\n",
    "}\n",
    "\n",
    "for_ops = {\"<\" + k.upper() + \">\": v for k,v in for_ops.items()}\n",
    "\n",
    "lambda_ops = {\n",
    "    \"Var\": 0,\n",
    "    \"Const\": 1,\n",
    "    \"Plus\": 2,\n",
    "    \"Minus\": 3,\n",
    "    \"EqualFor\": 4,\n",
    "    \"LeFor\": 5,\n",
    "    \"GeFor\": 6,\n",
    "    \"If\": 7,\n",
    "    \"Let\": 8,\n",
    "    \"Unit\": 9,\n",
    "    \"Letrec\": 10,\n",
    "    \"App\": 11\n",
    "}\n",
    "\n",
    "lambda_ops = {\"<\" + k.upper() + \">\": v for k,v in lambda_ops.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 256\n",
    "nclass = num_vars + num_ints + len(lambda_ops.keys())\n",
    "num_layers = 1\n",
    "attention = True\n",
    "alignment_size = 50\n",
    "\n",
    "encoder = Encoder(num_vars + num_ints + len(for_ops.keys()), hidden_size, num_layers, [1, 2], attention=attention)\n",
    "\n",
    "if attention:\n",
    "    decoder = MultilayerLSTMCell(embedding_size + hidden_size, hidden_size, num_layers)\n",
    "    program_model = Tree_to_Sequence_Attention_Model(encoder, decoder, hidden_size, nclass, embedding_size, alignment_size, use_lstm=True)\n",
    "else:\n",
    "    decoder = MultilayerLSTMCell(embedding_size, hidden_size, num_layers)\n",
    "    program_model = Tree_to_Sequence_Model(encoder, decoder, hidden_size, nclass, embedding_size, use_lstm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (38) : no CUDA-capable device is detected at /pytorch/torch/lib/THC/THCGeneral.c:70",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8adc335ef913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprogram_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogram_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;31m# Variables stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    120\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (38) : no CUDA-capable device is detected at /pytorch/torch/lib/THC/THCGeneral.c:70"
     ]
    }
   ],
   "source": [
    "program_model = program_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_prediction(prediction, target):\n",
    "    return 1 if list(target.data) == prediction else 0\n",
    "\n",
    "optimizer = torch.optim.SGD(program_model.parameters(), lr=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "program_model, train_losses, validation_losses = \\\n",
    "    training.train_model_anc(program_model, for_lambda_dset, optimizer, \n",
    "                             lr_scheduler=partial(training.exp_lr_scheduler, init_lr=0.0005, lr_decay_epoch=1), \n",
    "                             num_epochs=10, validation_criterion=check_prediction, batch_size=100, \n",
    "                             use_cuda=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
