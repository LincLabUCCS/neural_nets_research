{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "# from queue import Queue\n",
    "from threading import Thread\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Process, Queue, current_process, freeze_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_val_set(i):\n",
    "    return i % 10 == 0\n",
    "\n",
    "def in_test_set(i):\n",
    "    return i % 10 in [1, 2]\n",
    "\n",
    "def in_train_set(i):\n",
    "    return i % 10 > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_phrases = [\"years ago\", \"min read\", \"(reuters) - \", \"min read*\"]\n",
    "end_phrases = [\"our standards:the thomson reuters trust principles\", \"reporting by\", \"editing by\", \"reporting and writing by\", \"additional reporting\", \"writing by\"]\n",
    "\n",
    "bad = 0\n",
    "\n",
    "# https://stackoverflow.com/questions/3472515/python-urllib2-urlopen-is-slow-need-a-better-way-to-read-several-urls\n",
    "\n",
    "# Fetch\n",
    "def fetch_parallel(articles):\n",
    "    dset = Queue()\n",
    "    print(\"started making threads\")\n",
    "    threads = [Thread(target=read_webpage, args = (article, dset)) for article in articles]\n",
    "    print(\"finished making threads\")\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    return dset\n",
    "\n",
    "def fetch_parallel2(articles):\n",
    "    dset = Queue()\n",
    "    threads = []\n",
    "    for article in articles:\n",
    "        thread = Thread(target=read_webpage, args = (article, dset))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    return dset\n",
    "\n",
    "def fetch_sequential(articles):\n",
    "    for article in articles:\n",
    "        dset = Queue()\n",
    "        read_webpage(article, dset)\n",
    "\n",
    "\n",
    "def read_webpage(article, queue):\n",
    "    print(\"Starting one!\")\n",
    "    title = article[\"title\"]\n",
    "    if title[:6] == \"TABLE-\":\n",
    "        return\n",
    "\n",
    "    link = article[\"href\"]\n",
    "    start_index = -1\n",
    "    try:\n",
    "        print(\"        reading \" + link)\n",
    "        with urllib.request.urlopen(link) as webpage:\n",
    "            print(\"        Read it raw\")\n",
    "            content = BeautifulSoup(webpage.read(), \"html.parser\")\n",
    "            for unnecessary_content in content([\"script\", \"style\", \"table\"]):\n",
    "                unnecessary_content.extract()    \n",
    "            text = content.get_text()\n",
    "            for phrase in start_phrases:\n",
    "                if phrase in text.lower():\n",
    "                    phrase_index = text.lower().index(phrase) + len(phrase)\n",
    "                    start_index = max(start_index, phrase_index)\n",
    "            end_index = 99999999\n",
    "            for phrase in end_phrases:\n",
    "                if phrase in text.lower():\n",
    "                    phrase_index = text.lower().index(phrase)\n",
    "                    if text[phrase_index - 1] == \"(\":\n",
    "                        phrase_index -= 1\n",
    "                    end_index = min(end_index, phrase_index)\n",
    "            text = text[start_index: end_index]\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            title = title.replace(\"\\n\", \" \")\n",
    "            queue.put((title, text))\n",
    "            print(\"     Finished parsing succeeded\")\n",
    "    except Exception as e:\n",
    "        print(\"        ERROR\", e)\n",
    "        global bad\n",
    "        bad += 1\n",
    "    \n",
    "def generate_dset(name, condition):\n",
    "    # Load data\n",
    "    articles = []\n",
    "    for ls in os.listdir('Reuters-full-data-set-master/data'):\n",
    "#         print(\"New set\", ls)\n",
    "        if ls.endswith('.pkl'):\n",
    "            with open('Reuters-full-data-set-master/data/' + ls, 'rb') as f:\n",
    "                data = pickle.load(f, encoding='latin1')\n",
    "                new_articles = [article for index, article in enumerate(data) if condition(index)]\n",
    "                articles += new_articles\n",
    "#                 print(\"total article count:\", len(articles))\n",
    "#                 print(\"bad so far\", bad)\n",
    "    print(\"LOADED ALL FILES!\")\n",
    "    with open(name + \"_articles.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(articles, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"done dumping\")\n",
    "    dset = fetch_parallel(articles)\n",
    "    write_data(dset, name)\n",
    "    \n",
    "    \n",
    "def write_data(dset, name):\n",
    "    print(\"writing data\")\n",
    "    with open(name + \"_source.txt\", \"a\") as file_source:\n",
    "        with open(name + \"_target.txt\", \"a\") as file_target:\n",
    "#             i = 0\n",
    "            while not dset.empty():\n",
    "                article = dset.get()\n",
    "#                 if not i % 1000: print(i)\n",
    "#                 print(\"START\", article[1], \"END\")\n",
    "                file_source.write(article[0] + \"\\n\")\n",
    "                file_target.write(article[1] + \"\\n\")\n",
    "    print(\"DONE!!!\")\n",
    "                \n",
    "                            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def generate_dset_old(name, condition):\n",
    "#     with open(name + \"_source.txt\", \"a\") as file_source:\n",
    "#         with open(name + \"_target.txt\", \"a\") as file_target:\n",
    "\n",
    "            # Load data\n",
    "            for ls in os.listdir('Reuters-full-data-set-master/data')[:5]:\n",
    "#                 print(\"New set\", ls)\n",
    "                if ls.endswith('.pkl'):\n",
    "                    with open('Reuters-full-data-set-master/data/' + ls, 'rb') as f:\n",
    "                        data = pickle.load(f, encoding='latin1')\n",
    "                        data = [article for index, article in enumerate(data) if condition(index)][:50]\n",
    "#                         print(\"total article count:\", len(data))\n",
    "                        for article in data:\n",
    "                            title = article[\"title\"]\n",
    "                            if title[:6] == \"TABLE-\":\n",
    "                                continue\n",
    "                            \n",
    "                            link = article[\"href\"]\n",
    "                            start_index = -1\n",
    "                            try:\n",
    "                                time1 = time.time()\n",
    "                                with urllib.request.urlopen(link) as webpage:\n",
    "                                    time2 = time.time()\n",
    "                                    content = BeautifulSoup(webpage.read(), \"html.parser\")\n",
    "                                    time3 = time.time()\n",
    "                                    for unnecessary_content in content([\"script\", \"style\", \"table\"]):\n",
    "                                        unnecessary_content.extract()    \n",
    "                                    text = content.get_text()\n",
    "                                    for phrase in start_phrases:\n",
    "                                        if phrase in text.lower():\n",
    "                                            phrase_index = text.lower().index(phrase) + len(phrase)\n",
    "                                            start_index = max(start_index, phrase_index)\n",
    "                                    end_index = 99999999\n",
    "                                    for phrase in end_phrases:\n",
    "                                        if phrase in text.lower():\n",
    "                                            phrase_index = text.lower().index(phrase)\n",
    "                                            if text[phrase_index - 1] == \"(\":\n",
    "                                                phrase_index -= 1\n",
    "                                            end_index = min(end_index, phrase_index)\n",
    "                                    text = text[start_index: end_index]\n",
    "                                    text = text.replace(\"\\n\", \" \")\n",
    "                                    title = title.replace(\"\\n\", \" \")\n",
    "                                    print(time3 - time2, time2 - time1)\n",
    "#                                     file_source.write(title + \"\\n\")\n",
    "#                                     file_target.write(text + \"\\n\")\n",
    "#                                     print(\"suceeded!!!\")\n",
    "                            except Exception as e:\n",
    "                                pass\n",
    "#                                 print(\"BAD ARTICLE!!!\")\n",
    "#                                 print(e)\n",
    "#                                 print(link)\n",
    "#     print(\"DONE!!!\")\n",
    "\n",
    "# Our Standards:The Thomson Reuters Trust Principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% val, 20% test, 70% train\n",
    "print(len(os.listdir('Reuters-full-data-set-master/data')))\n",
    "# print(\"Our Standards:The Thomson Reuters Trust Principles\".lower())\n",
    "# print(end_phrases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Web scrape -P1\n",
    "start = time.time()\n",
    "generate_dset(\"validation\", in_val_set)\n",
    "end = time.time()\n",
    "print(\"Total time: \", end - start)\n",
    "# generate_dset(\"train\", in_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scrape -P1\n",
    "start = time.time()\n",
    "generate_dset(\"test\", in_test_set)\n",
    "end = time.time()\n",
    "print(\"Total time: \", end - start)\n",
    "# generate_dset(\"train\", in_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scrape -P1\n",
    "start = time.time()\n",
    "generate_dset(\"train\", in_train_set)\n",
    "end = time.time()\n",
    "print(\"Total time: \", end - start)\n",
    "# generate_dset(\"train\", in_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "generate_dset_old(\"validation\", in_val_set)\n",
    "end = time.time()\n",
    "print(\"Total time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_articles.pkl\" , \"rb\") as file:\n",
    "    articles = pickle.load(file)\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "from multiprocessing import Process, Queue, current_process, freeze_support\n",
    "\n",
    "#\n",
    "# Function run by worker processes\n",
    "#\n",
    "\n",
    "def worker(input, output):\n",
    "    for func, args in iter(input.get, 'STOP'):\n",
    "        result = calculate(func, args)\n",
    "        output.put(result)\n",
    "\n",
    "#\n",
    "# Function used to calculate result\n",
    "#\n",
    "\n",
    "def calculate(func, args):\n",
    "    result = func(*args)\n",
    "    return '%s says that %s%s = %s' % \\\n",
    "        (current_process().name, func.__name__, args, result)\n",
    "\n",
    "#\n",
    "# Functions referenced by tasks\n",
    "#\n",
    "\n",
    "def mul(a, b):\n",
    "    time.sleep(0.5*random.random())\n",
    "    return a * b\n",
    "\n",
    "def plus(a, b):\n",
    "    time.sleep(0.5*random.random())\n",
    "    return a + b\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "def test():\n",
    "    NUMBER_OF_PROCESSES = 4\n",
    "    TASKS1 = [(mul, (i, 7)) for i in range(20)]\n",
    "    TASKS2 = [(plus, (i, 8)) for i in range(10)]\n",
    "\n",
    "    # Create queues\n",
    "    task_queue = Queue()\n",
    "    done_queue = Queue()\n",
    "\n",
    "    # Submit tasks\n",
    "    for task in TASKS1:\n",
    "        task_queue.put(task)\n",
    "\n",
    "    # Start worker processes\n",
    "    for i in range(NUMBER_OF_PROCESSES):\n",
    "        Process(target=worker, args=(task_queue, done_queue)).start()\n",
    "\n",
    "    # Get and print results\n",
    "    print('Unordered results:')\n",
    "    for i in range(len(TASKS1)):\n",
    "        print('\\t', done_queue.get())\n",
    "\n",
    "    # Add more tasks using `put()`\n",
    "    for task in TASKS2:\n",
    "        task_queue.put(task)\n",
    "\n",
    "    # Get and print some more results\n",
    "    for i in range(len(TASKS2)):\n",
    "        print('\\t', \"lol\", done_queue.get())\n",
    "\n",
    "    # Tell child processes to stop\n",
    "    for i in range(NUMBER_OF_PROCESSES):\n",
    "        task_queue.put('STOP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHANGE THIS ONE!!!\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from multiprocessing import Process, Queue, current_process, freeze_support\n",
    "\n",
    "def generate_dset(condition):\n",
    "    # Load data\n",
    "    articles = []\n",
    "    for ls in os.listdir('Reuters-full-data-set-master/data'):\n",
    "#         print(\"New set\", ls)\n",
    "        if ls.endswith('.pkl'):\n",
    "            with open('Reuters-full-data-set-master/data/' + ls, 'rb') as f:\n",
    "                data = pickle.load(f, encoding='latin1')\n",
    "                new_articles = [article for index, article in enumerate(data) if condition(index)]\n",
    "                articles += new_articles\n",
    "    print(\"LOADED ALL FILES!\")\n",
    "    return articles\n",
    "    \n",
    "\n",
    "#\n",
    "# Function run by worker processes\n",
    "#\n",
    "def read_webpage(article):\n",
    "    title = article[\"title\"]\n",
    "    if title[:6] == \"TABLE-\":\n",
    "        return None\n",
    "\n",
    "    link = article[\"href\"]\n",
    "    start_index = -1\n",
    "    try:\n",
    "        with urllib.request.urlopen(link) as webpage:\n",
    "            content = BeautifulSoup(webpage.read(), \"html.parser\")\n",
    "            for unnecessary_content in content([\"script\", \"style\", \"table\"]):\n",
    "                unnecessary_content.extract()    \n",
    "            text = content.get_text()\n",
    "            for phrase in start_phrases:\n",
    "                if phrase in text.lower():\n",
    "                    phrase_index = text.lower().index(phrase) + len(phrase)\n",
    "                    start_index = max(start_index, phrase_index)\n",
    "            end_index = 99999999\n",
    "            for phrase in end_phrases:\n",
    "                if phrase in text.lower():\n",
    "                    phrase_index = text.lower().index(phrase)\n",
    "                    if text[phrase_index - 1] == \"(\":\n",
    "                        phrase_index -= 1\n",
    "                    end_index = min(end_index, phrase_index)\n",
    "            text = text[start_index: end_index]\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            title = title.replace(\"\\n\", \" \")\n",
    "            return title, text\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def worker(input, output):\n",
    "    for article in iter(input.get, 'STOP'):\n",
    "        result = read_webpage(article)\n",
    "        output.put(result)\n",
    "\n",
    "def run_all(name, condition):\n",
    "    start = time.time()\n",
    "    NUMBER_OF_PROCESSES = 8\n",
    "    tasks = generate_dset(condition)\n",
    "    print(\"Tasks\", len(tasks))\n",
    "\n",
    "    # Create queues\n",
    "    task_queue = Queue()\n",
    "    done_queue = Queue()\n",
    "\n",
    "    # Submit tasks\n",
    "    for task in tasks:\n",
    "        task_queue.put(task)\n",
    "\n",
    "    # Start worker processes\n",
    "    for i in range(NUMBER_OF_PROCESSES):\n",
    "        Process(target=worker, args=(task_queue, done_queue)).start()\n",
    "\n",
    "    # Get and print results\n",
    "    print(\"writing data\")\n",
    "    with open(name + \"_source.txt\", \"a\") as file_source:\n",
    "        with open(name + \"_target.txt\", \"a\") as file_target:\n",
    "            for i in range(len(tasks)):\n",
    "                if not i % 10:\n",
    "                    print(\"got one\", i)\n",
    "                article = done_queue.get()\n",
    "                if article is not None:\n",
    "                    file_source.write(article[0] + \"\\n\")\n",
    "                    file_target.write(article[1] + \"\\n\")\n",
    "    print(\"DONE!!!\")\n",
    "\n",
    "    # Tell child processes to stop\n",
    "    for i in range(NUMBER_OF_PROCESSES):\n",
    "        task_queue.put('STOP')\n",
    "    end = time.time()\n",
    "    print(\"total time\", end - start)\n",
    "        \n",
    "#kangaroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(\"plz_work\", in_val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = multiprocessing.cpu_count()\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# class ActivePool(object):\n",
    "#     def __init__(self):\n",
    "#         super(ActivePool, self).__init__()\n",
    "#         self.mgr = multiprocessing.Manager()\n",
    "#         self.active = self.mgr.list()\n",
    "#         self.lock = multiprocessing.Lock()\n",
    "\n",
    "#     def make_active(self, name):\n",
    "#         with self.lock:\n",
    "#             self.active.append(name)\n",
    "\n",
    "#     def make_inactive(self, name):\n",
    "#         with self.lock:\n",
    "#             self.active.remove(name)\n",
    "\n",
    "#     def __str__(self):\n",
    "#         with self.lock:\n",
    "#             return str(self.active)\n",
    "\n",
    "    \n",
    "class DsetMaker:\n",
    "    \"\"\" Builds Reuters dataset \"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_phrases = [\"years ago\", \"min read\", \"(reuters) - \", \"min read*\"]\n",
    "        self.end_phrases = [\"our standards:the thomson reuters trust principles\", \"reporting by\", \"editing by\", \"reporting and writing by\", \"additional reporting\", \"writing by\"]\n",
    "            \n",
    "    def read_webpage(self, s, pool, results_queue, article):\n",
    "#         proc = psutil.Process()\n",
    "#         print(\"SECOND\", proc.open_files())\n",
    "        process_name = multiprocessing.current_process().name\n",
    "        with s:\n",
    "#             pool.make_active(process_name)\n",
    "            title = article[\"title\"]\n",
    "            if title[:6] == \"TABLE-\":\n",
    "                return\n",
    "\n",
    "            link = article[\"href\"]\n",
    "            start_index = -1\n",
    "            try:\n",
    "                time.sleep(.5)\n",
    "                results_queue.put((\"aaa\", \"BBB\"))\n",
    "#                 with urllib.request.urlopen(link) as webpage:\n",
    "#                     page = webpage.read()\n",
    "#                 content = BeautifulSoup(page, \"html.parser\")\n",
    "#                 for unnecessary_content in content([\"script\", \"style\", \"table\"]):\n",
    "#                     unnecessary_content.extract()    \n",
    "#                 text = content.get_text()\n",
    "#                 for phrase in start_phrases:\n",
    "#                     if phrase in text.lower():\n",
    "#                         phrase_index = text.lower().index(phrase) + len(phrase)\n",
    "#                         start_index = max(start_index, phrase_index)\n",
    "#                 end_index = 99999999\n",
    "#                 for phrase in end_phrases:\n",
    "#                     if phrase in text.lower():\n",
    "#                         phrase_index = text.lower().index(phrase)\n",
    "#                         if text[phrase_index - 1] == \"(\":\n",
    "#                             phrase_index -= 1\n",
    "#                         end_index = min(end_index, phrase_index)\n",
    "#                 text = text[start_index: end_index]\n",
    "#                 text = text.replace(\"\\n\", \" \")\n",
    "#                 title = title.replace(\"\\n\", \" \")\n",
    "#                 results_queue.put((title, text))\n",
    "            except:\n",
    "                results_queue.put(None)\n",
    "#             pool.make_inactive(process_name)\n",
    "    \n",
    "    def generate_dset(self, name, condition):\n",
    "        # Load data\n",
    "        articles = []\n",
    "        for ls in os.listdir('Reuters-full-data-set-master/data')[:5]:\n",
    "            print(\"New set\", ls)\n",
    "            if ls.endswith('.pkl'):\n",
    "                with open('Reuters-full-data-set-master/data/' + ls, 'rb') as f:\n",
    "                    data = pickle.load(f, encoding='latin1')\n",
    "                    new_articles = [article for index, article in enumerate(data) if condition(index)][:50]\n",
    "                    articles += new_articles\n",
    "                    print(\"total article count:\", len(articles))\n",
    "        print(\"LOADED ALL FILES!\")\n",
    "        with open(name + \"_articles.pkl\", \"wb\") as outfile:\n",
    "            pickle.dump(articles, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"done dumping\")\n",
    "        self.build_dset(articles, name)\n",
    "\n",
    "    def build_dset(self, articles, name):\n",
    "        \"\"\"\n",
    "        Building dset\n",
    "        \"\"\"\n",
    "        with multiprocessing.Pool(nprocess) as pool:\n",
    "        \n",
    "#         pool = ActivePool()\n",
    "        pool = None\n",
    "        s = multiprocessing.Semaphore(50)\n",
    "#         results_queue = multiprocessing.Queue()\n",
    "        results_queue = Queue()\n",
    "        jobs = [\n",
    "            multiprocessing.Process(\n",
    "                target = self.read_webpage,\n",
    "                name = article[\"href\"],\n",
    "                args = (\n",
    "                    s,\n",
    "                    pool,\n",
    "                    results_queue,\n",
    "                    article\n",
    "                )\n",
    "            )\n",
    "            for article in articles\n",
    "        ]\n",
    "        load_start = time.time()\n",
    "        proc = psutil.Process()\n",
    "        print(\"FIRST TIME\")\n",
    "        print(proc.open_files())\n",
    "        [j.start() for j in jobs]# yep\n",
    "\n",
    "        # pull results from queue before joining threads to avoid pipe deadlock.\n",
    "        # https://tinyurl.com/yda2aa6k\n",
    "#         with open(name + \"_source.txt\", \"a\") as file_source:\n",
    "#             with open(name + \"_target.txt\", \"a\") as file_target:\n",
    "#                 pulled_results = 0\n",
    "#                 print(\"writing data\")\n",
    "#                 write_time = time.time()\n",
    "#                 while(pulled_results < len(articles)):\n",
    "#                     try:\n",
    "#                         while(not results_queue.empty()):\n",
    "#                             article = results_queue.get()\n",
    "#                             pulled_results += 1\n",
    "#                             if not article is None:\n",
    "#                                 pass\n",
    "#         #                         file_source.write(article[0] + \"\\n\")\n",
    "#         #                         file_target.write(article[1] + \"\\n\")\n",
    "#                             print(\"DONE!!!\")\n",
    "#                     except Queue.empty:\n",
    "#                         pass\n",
    "#                     print(\"sleeping\")\n",
    "#                     time.sleep(1)\n",
    "#                 after_write = time.time()\n",
    "#                 print(\"writing (and waiting for things to write) took\", after_write - write_time)\n",
    "\n",
    "        [j.join() for j in jobs] \n",
    "        load_end = time.time()\n",
    "        print(\"Load time is \", load_end - load_start)\n",
    "    \n",
    "#         with open(name + \"_source.txt\", \"a\") as file_source:\n",
    "#             with open(name + \"_target.txt\", \"a\") as file_target:\n",
    "#                 file_source.write(\"HI1\")\n",
    "#                 file_target.write(\"BYE!\")\n",
    "#                 print(\"writing data\")\n",
    "#         write_end = time.time()\n",
    "#         print(\"writing took\", write_end - load_end)\n",
    "                \n",
    "#                 pulled_results = 0\n",
    "#                 print(\"writing data\")\n",
    "#                 write_time = time.time()\n",
    "#                 while(pulled_results < len(articles)):\n",
    "#                     try:\n",
    "#                         while(not results_queue.empty()):\n",
    "#                             article = results_queue.get()\n",
    "#                             pulled_results += 1\n",
    "#                             if not article is None:\n",
    "#                                 pass\n",
    "#                                 file_source.write(article[0] + \"\\n\")\n",
    "#                                 file_target.write(article[1] + \"\\n\")\n",
    "#                             print(\"DONE!!!\")\n",
    "#                     except Queue.empty:\n",
    "#                         pass\n",
    "#                     print(\"sleeping\")\n",
    "#                     time.sleep(1)\n",
    "#                 after_write = time.time()\n",
    "#                 print(\"writing (and waiting for things to write) took\", after_write - write_time)\n",
    "        \n",
    "        print(\"ALL DONE!!!\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overall_start = time.time()\n",
    "d = DsetMaker()\n",
    "d.generate_dset(\"isthisworking\", in_val_set)\n",
    "overall_end = time.time()\n",
    "print(\"Overall Time: \", overall_end - overall_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T CHANGE THIS!!!\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "from activePool import ActivePool\n",
    "from celex import Celex\n",
    "from config import GAConfig\n",
    "\n",
    "class ActivePool(object):\n",
    "    def __init__(self):\n",
    "        super(ActivePool, self).__init__()\n",
    "        self.mgr = multiprocessing.Manager()\n",
    "        self.active = self.mgr.list()\n",
    "        self.lock = multiprocessing.Lock()\n",
    "\n",
    "    def make_active(self, name):\n",
    "        with self.lock:\n",
    "            self.active.append(name)\n",
    "\n",
    "    def make_inactive(self, name):\n",
    "        with self.lock:\n",
    "            self.active.remove(name)\n",
    "\n",
    "    def __str__(self):\n",
    "        with self.lock:\n",
    "            return str(self.active)\n",
    "\n",
    "class ComputeFitness:\n",
    "    \"\"\" Computes the fitness of chromosomes in a population concurrently \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.celex = Celex()#B\n",
    "\n",
    "    def compute(self, population):\n",
    "        \"\"\"\n",
    "        Compute the fitness of all chromosomes in the population.\n",
    "        Updates the fitness value of all chromosomes.\n",
    "        Chromosome fitness calculation is done in separate processes.\n",
    "        \"\"\"\n",
    "        sizes = (GAConfig[\"training_size_hmm\"], GAConfig[\"testing_size_hmm\"])#B\n",
    "        self.celex.load_sets(sizes[0], sizes[1])#B\n",
    "\n",
    "        pool = ActivePool()\n",
    "        s = multiprocessing.Semaphore(GAConfig[\"max_thread_count\"])#MAYBE\n",
    "        results_queue = multiprocessing.Queue(len(population) + 1)#yep!\n",
    "        jobs = [ #yep... read here\n",
    "            multiprocessing.Process(\n",
    "                target = self._compute_single_fitness,\n",
    "                name = str(i),\n",
    "                args = (\n",
    "                    i,\n",
    "                    s,\n",
    "                    pool,\n",
    "                    results_queue,\n",
    "                    population[i].get_genes()\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(population))\n",
    "        ]\n",
    "        [j.start() for j in jobs]# yep\n",
    "\n",
    "        # pull results from queue before joining threads to avoid pipe deadlock.\n",
    "        # https://tinyurl.com/yda2aa6k\n",
    "        pulled_results = 0\n",
    "        while(pulled_results < len(population)): # while not done\n",
    "            try:\n",
    "                while(not results_queue.empty()): # Write to file here\n",
    "                    result = results_queue.get()\n",
    "                    index = result[0]\n",
    "                    fitness = result[1]\n",
    "                    results_list = result[2]\n",
    "                    population[index].set_fitness(fitness)\n",
    "                    population[index].set_results(results_list)\n",
    "                    pulled_results += 1\n",
    "\n",
    "            except Queue.empty:\n",
    "                pass\n",
    "            time.sleep(1)\n",
    "\n",
    "        [j.join() for j in jobs] # yep\n",
    "        return population\n",
    "\n",
    "    def _compute_single_fitness(self, i, s, pool, results_queue, genes):\n",
    "        \"\"\"\n",
    "        Calculates and puts updated fitness on the results_queue.\n",
    "        Args:\n",
    "            i (int): process and population index\n",
    "            s (multiprocessing.Semaphore)\n",
    "            pool (ActivePool): manager of the pool and locks\n",
    "            results_queue (multiprocessing.Queue): communication\n",
    "                        between processes.\n",
    "            genes (list<list<char>>): genes of a Chromosome\n",
    "        \"\"\"\n",
    "        process_name = multiprocessing.current_process().name\n",
    "        with s:\n",
    "            pool.make_active(process_name)\n",
    "            ps_model = self.celex.train_hmm(genes)\n",
    "            fitness, all_results = self.celex.test_hmm(ps_model)\n",
    "            ps_model = None\n",
    "            results_queue.put((i, fitness, all_results))\n",
    "            pool.make_inactive(process_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url1 = \"http://www.reuters.com/article/companyNewsAndPR/idUSTP13157220070102\"\n",
    "url2 = \"http://www.reuters.com/article/2011/09/09/indian-stocks-open-idUSI8E7J302G20110909\"\n",
    "url3 = \"http://www.reuters.com/article/companyNewsAndPR/idUSL1136380220070111\"\n",
    "url4 = \"http://www.reuters.com/article/india-cenbank-refinance-idUSB8N12101C20151015\"\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "start_phrase = \"years ago\"\n",
    "end_phrase = \"Our Standards:The Thomson Reuters Trust Principles.\"\n",
    "\n",
    "with urllib.request.urlopen(url3) as webpage:\n",
    "    content = BeautifulSoup(webpage.read(), \"html.parser\")\n",
    "    for script in content([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    print(content)\n",
    "#     text = content.get_text()\n",
    "#     start_index = text.index(start_phrase) + len(start_phrase)\n",
    "#     end_index = text.index(end_phrase)\n",
    "#     text = text[start_index: end_index]\n",
    "#     lines = (line.strip() for line in text.splitlines() if not line.strip() == \"\")\n",
    "#     for i, line in enumerate(lines):\n",
    "# #         print(\"Line \" + str(i) + \": \" + line)\n",
    "#         tokens = tokenizer.tokenize(line)\n",
    "#         for token in tokens:\n",
    "#             print(\"\\n\")\n",
    "#             print(token)\n",
    "# #         print(tokens)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "class ActivePool(object):\n",
    "    def __init__(self):\n",
    "        super(ActivePool, self).__init__()\n",
    "        self.mgr = multiprocessing.Manager()\n",
    "        self.active = self.mgr.list()\n",
    "        self.lock = multiprocessing.Lock()\n",
    "\n",
    "    def make_active(self, name):\n",
    "        with self.lock:\n",
    "            self.active.append(name)\n",
    "\n",
    "    def make_inactive(self, name):\n",
    "        with self.lock:\n",
    "            self.active.remove(name)\n",
    "\n",
    "    def __str__(self):\n",
    "        with self.lock:\n",
    "            return str(self.active)\n",
    "\n",
    "    \n",
    "class DsetMaker:\n",
    "    \"\"\" Builds Reuters dataset \"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_phrases = [\"years ago\", \"min read\", \"(reuters) - \", \"min read*\"]\n",
    "        self.end_phrases = [\"our standards:the thomson reuters trust principles\", \"reporting by\", \"editing by\", \"reporting and writing by\", \"additional reporting\", \"writing by\"]\n",
    "            \n",
    "    def read_webpage(self, s, pool, results_queue, article):\n",
    "        proc = psutil.Process()\n",
    "        print(\"SECOND\", proc.open_files())\n",
    "        process_name = multiprocessing.current_process().name\n",
    "        with s:\n",
    "            pool.make_active(process_name)\n",
    "            title = article[\"title\"]\n",
    "            if title[:6] == \"TABLE-\":\n",
    "                return\n",
    "\n",
    "            link = article[\"href\"]\n",
    "            start_index = -1\n",
    "            try:\n",
    "                time.sleep(.5)\n",
    "                results_queue.put((\"aaa\", \"BBB\"))\n",
    "#                 with urllib.request.urlopen(link) as webpage:\n",
    "#                     page = webpage.read()\n",
    "#                 content = BeautifulSoup(page, \"html.parser\")\n",
    "#                 for unnecessary_content in content([\"script\", \"style\", \"table\"]):\n",
    "#                     unnecessary_content.extract()    \n",
    "#                 text = content.get_text()\n",
    "#                 for phrase in start_phrases:\n",
    "#                     if phrase in text.lower():\n",
    "#                         phrase_index = text.lower().index(phrase) + len(phrase)\n",
    "#                         start_index = max(start_index, phrase_index)\n",
    "#                 end_index = 99999999\n",
    "#                 for phrase in end_phrases:\n",
    "#                     if phrase in text.lower():\n",
    "#                         phrase_index = text.lower().index(phrase)\n",
    "#                         if text[phrase_index - 1] == \"(\":\n",
    "#                             phrase_index -= 1\n",
    "#                         end_index = min(end_index, phrase_index)\n",
    "#                 text = text[start_index: end_index]\n",
    "#                 text = text.replace(\"\\n\", \" \")\n",
    "#                 title = title.replace(\"\\n\", \" \")\n",
    "#                 results_queue.put((title, text))\n",
    "            except:\n",
    "                results_queue.put(None)\n",
    "            pool.make_inactive(process_name)\n",
    "    \n",
    "    def generate_dset(self, name, condition):\n",
    "        # Load data\n",
    "        articles = []\n",
    "        for ls in os.listdir('Reuters-full-data-set-master/data')[:5]:\n",
    "            print(\"New set\", ls)\n",
    "            if ls.endswith('.pkl'):\n",
    "                with open('Reuters-full-data-set-master/data/' + ls, 'rb') as f:\n",
    "                    data = pickle.load(f, encoding='latin1')\n",
    "                    new_articles = [article for index, article in enumerate(data) if condition(index)][:50]\n",
    "                    articles += new_articles\n",
    "                    print(\"total article count:\", len(articles))\n",
    "        print(\"LOADED ALL FILES!\")\n",
    "        with open(name + \"_articles.pkl\", \"wb\") as outfile:\n",
    "            pickle.dump(articles, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"done dumping\")\n",
    "        self.build_dset(articles, name)\n",
    "\n",
    "    def build_dset(self, articles, name):\n",
    "        \"\"\"\n",
    "        Building dset\n",
    "        \"\"\"\n",
    "        \n",
    "        pool = ActivePool()\n",
    "        s = multiprocessing.Semaphore(50)\n",
    "#         results_queue = multiprocessing.Queue()\n",
    "        results_queue = Queue()\n",
    "        jobs = [\n",
    "            multiprocessing.Process(\n",
    "                target = self.read_webpage,\n",
    "                name = article[\"href\"],\n",
    "                args = (\n",
    "                    s,\n",
    "                    pool,\n",
    "                    results_queue,\n",
    "                    article\n",
    "                )\n",
    "            )\n",
    "            for article in articles\n",
    "        ]\n",
    "        load_start = time.time()\n",
    "        proc = psutil.Process()\n",
    "        print(\"FIRST TIME\")\n",
    "        print(proc.open_files())\n",
    "        [j.start() for j in jobs]# yep\n",
    "\n",
    "        # pull results from queue before joining threads to avoid pipe deadlock.\n",
    "        # https://tinyurl.com/yda2aa6k\n",
    "#         with open(name + \"_source.txt\", \"a\") as file_source:\n",
    "#             with open(name + \"_target.txt\", \"a\") as file_target:\n",
    "#                 pulled_results = 0\n",
    "#                 print(\"writing data\")\n",
    "#                 write_time = time.time()\n",
    "#                 while(pulled_results < len(articles)):\n",
    "#                     try:\n",
    "#                         while(not results_queue.empty()):\n",
    "#                             article = results_queue.get()\n",
    "#                             pulled_results += 1\n",
    "#                             if not article is None:\n",
    "#                                 pass\n",
    "#         #                         file_source.write(article[0] + \"\\n\")\n",
    "#         #                         file_target.write(article[1] + \"\\n\")\n",
    "#                             print(\"DONE!!!\")\n",
    "#                     except Queue.empty:\n",
    "#                         pass\n",
    "#                     print(\"sleeping\")\n",
    "#                     time.sleep(1)\n",
    "#                 after_write = time.time()\n",
    "#                 print(\"writing (and waiting for things to write) took\", after_write - write_time)\n",
    "\n",
    "        [j.join() for j in jobs] \n",
    "        load_end = time.time()\n",
    "        print(\"Load time is \", load_end - load_start)\n",
    "    \n",
    "#         with open(name + \"_source.txt\", \"a\") as file_source:\n",
    "#             with open(name + \"_target.txt\", \"a\") as file_target:\n",
    "#                 file_source.write(\"HI1\")\n",
    "#                 file_target.write(\"BYE!\")\n",
    "#                 print(\"writing data\")\n",
    "#         write_end = time.time()\n",
    "#         print(\"writing took\", write_end - load_end)\n",
    "                \n",
    "#                 pulled_results = 0\n",
    "#                 print(\"writing data\")\n",
    "#                 write_time = time.time()\n",
    "#                 while(pulled_results < len(articles)):\n",
    "#                     try:\n",
    "#                         while(not results_queue.empty()):\n",
    "#                             article = results_queue.get()\n",
    "#                             pulled_results += 1\n",
    "#                             if not article is None:\n",
    "#                                 pass\n",
    "#                                 file_source.write(article[0] + \"\\n\")\n",
    "#                                 file_target.write(article[1] + \"\\n\")\n",
    "#                             print(\"DONE!!!\")\n",
    "#                     except Queue.empty:\n",
    "#                         pass\n",
    "#                     print(\"sleeping\")\n",
    "#                     time.sleep(1)\n",
    "#                 after_write = time.time()\n",
    "#                 print(\"writing (and waiting for things to write) took\", after_write - write_time)\n",
    "        \n",
    "        print(\"ALL DONE!!!\")\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
