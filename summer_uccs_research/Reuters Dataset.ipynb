{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_val_set(i):\n",
    "    return i % 10 == 0\n",
    "\n",
    "def in_test_set(i):\n",
    "    return i % 10 in [1, 2]\n",
    "\n",
    "def in_train_set(i):\n",
    "    return i % 10 > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_phrases = [\"years ago\", \"min read\", \"(reuters) - \", \"min read*\"]\n",
    "end_phrases = [\"our standards:the thomson reuters trust principles\", \"reporting by\", \"editing by\", \"reporting and writing by\", \"additional reporting\", \"writing by\"]\n",
    "\n",
    "bad = 0\n",
    "\n",
    "# https://stackoverflow.com/questions/3472515/python-urllib2-urlopen-is-slow-need-a-better-way-to-read-several-urls\n",
    "\n",
    "# Fetch\n",
    "def fetch_parallel(articles):\n",
    "    dset = Queue()\n",
    "    print(\"started making threads\")\n",
    "    threads = [Thread(target=read_webpage, args = (article, dset)) for article in articles]\n",
    "    print(\"finished making threads\")\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    return dset\n",
    "\n",
    "def fetch_parallel2(articles):\n",
    "    dset = Queue()\n",
    "    threads = []\n",
    "    for article in articles:\n",
    "        thread = Thread(target=read_webpage, args = (article, dset))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    return dset\n",
    "\n",
    "\n",
    "def read_webpage(article, queue):\n",
    "    title = article[\"title\"]\n",
    "    if title[:6] == \"TABLE-\":\n",
    "        return\n",
    "\n",
    "    link = article[\"href\"]\n",
    "    start_index = -1\n",
    "    try:\n",
    "        with urllib.request.urlopen(link) as webpage:\n",
    "            content = BeautifulSoup(webpage.read(), \"html.parser\")\n",
    "            for unnecessary_content in content([\"script\", \"style\", \"table\"]):\n",
    "                unnecessary_content.extract()    \n",
    "            text = content.get_text()\n",
    "            for phrase in start_phrases:\n",
    "                if phrase in text.lower():\n",
    "                    phrase_index = text.lower().index(phrase) + len(phrase)\n",
    "                    start_index = max(start_index, phrase_index)\n",
    "            end_index = 99999999\n",
    "            for phrase in end_phrases:\n",
    "                if phrase in text.lower():\n",
    "                    phrase_index = text.lower().index(phrase)\n",
    "                    if text[phrase_index - 1] == \"(\":\n",
    "                        phrase_index -= 1\n",
    "                    end_index = min(end_index, phrase_index)\n",
    "            text = text[start_index: end_index]\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            title = title.replace(\"\\n\", \" \")\n",
    "            queue.put((title, text))\n",
    "    except:\n",
    "        global bad\n",
    "        bad += 1\n",
    "    \n",
    "def generate_dset(name, condition):\n",
    "    # Load data\n",
    "    articles = []\n",
    "    for ls in os.listdir('Reuters-full-data-set-master/data')[:5]:\n",
    "        print(\"New set\", ls)\n",
    "        if ls.endswith('.pkl'):\n",
    "            with open('Reuters-full-data-set-master/data/' + ls, 'rb') as f:\n",
    "                data = pickle.load(f, encoding='latin1')\n",
    "                new_articles = [article for index, article in enumerate(data) if condition(index)][:50]\n",
    "                articles += new_articles\n",
    "                print(\"total article count:\", len(articles))\n",
    "#                 print(\"bad so far\", bad)\n",
    "    print(\"LOADED ALL FILES!\")\n",
    "    with open(name + \"_articles.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(articles, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"done dumping\")\n",
    "    dset = fetch_parallel2(articles)\n",
    "    write_data(dset, name)\n",
    "    \n",
    "    \n",
    "def write_data(dset, name):\n",
    "    print(\"writing data\")\n",
    "    with open(name + \"_source.txt\", \"a\") as file_source:\n",
    "        with open(name + \"_target.txt\", \"a\") as file_target:\n",
    "#             i = 0\n",
    "            while not dset.empty():\n",
    "                article = dset.get()\n",
    "#                 if not i % 1000: print(i)\n",
    "#                 print(\"START\", article[1], \"END\")\n",
    "                file_source.write(article[0] + \"\\n\")\n",
    "                file_target.write(article[1] + \"\\n\")\n",
    "    print(\"DONE!!!\")\n",
    "                \n",
    "                            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def generate_dset_old(name, condition):\n",
    "#     with open(name + \"_source.txt\", \"a\") as file_source:\n",
    "#         with open(name + \"_target.txt\", \"a\") as file_target:\n",
    "\n",
    "            # Load data\n",
    "            for ls in os.listdir('Reuters-full-data-set-master/data')[:1]:\n",
    "                print(\"New set\", ls)\n",
    "                if ls.endswith('.pkl'):\n",
    "                    with open('Reuters-full-data-set-master/data/' + ls, 'rb') as f:\n",
    "                        data = pickle.load(f, encoding='latin1')\n",
    "                        data = [article for index, article in enumerate(data) if condition(index)][:50]\n",
    "                        print(\"total article count:\", len(data))\n",
    "                        for article in data:\n",
    "                            title = article[\"title\"]\n",
    "                            if title[:6] == \"TABLE-\":\n",
    "                                continue\n",
    "                            \n",
    "                            link = article[\"href\"]\n",
    "                            start_index = -1\n",
    "                            try:\n",
    "                                time1 = time.time()\n",
    "                                with urllib.request.urlopen(link) as webpage:\n",
    "                                    time2 = time.time()\n",
    "                                    content = BeautifulSoup(webpage.read(), \"html.parser\")\n",
    "                                    time3 = time.time()\n",
    "                                    for unnecessary_content in content([\"script\", \"style\", \"table\"]):\n",
    "                                        unnecessary_content.extract()    \n",
    "                                    text = content.get_text()\n",
    "                                    for phrase in start_phrases:\n",
    "                                        if phrase in text.lower():\n",
    "                                            phrase_index = text.lower().index(phrase) + len(phrase)\n",
    "                                            start_index = max(start_index, phrase_index)\n",
    "                                    end_index = 99999999\n",
    "                                    for phrase in end_phrases:\n",
    "                                        if phrase in text.lower():\n",
    "                                            phrase_index = text.lower().index(phrase)\n",
    "                                            if text[phrase_index - 1] == \"(\":\n",
    "                                                phrase_index -= 1\n",
    "                                            end_index = min(end_index, phrase_index)\n",
    "                                    text = text[start_index: end_index]\n",
    "                                    text = text.replace(\"\\n\", \" \")\n",
    "                                    title = title.replace(\"\\n\", \" \")\n",
    "                                    print(time3 - time2, time2 - time1)\n",
    "#                                     file_source.write(title + \"\\n\")\n",
    "#                                     file_target.write(text + \"\\n\")\n",
    "#                                     print(\"suceeded!!!\")\n",
    "                            except Exception as e:\n",
    "                                pass\n",
    "#                                 print(\"BAD ARTICLE!!!\")\n",
    "#                                 print(e)\n",
    "#                                 print(link)\n",
    "#     print(\"DONE!!!\")\n",
    "\n",
    "# Our Standards:The Thomson Reuters Trust Principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% val, 20% test, 70% train\n",
    "print(len(os.listdir('Reuters-full-data-set-master/data')))\n",
    "# print(\"Our Standards:The Thomson Reuters Trust Principles\".lower())\n",
    "# print(end_phrases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Web scrape -P1\n",
    "start = time.time()\n",
    "generate_dset(\"validation\", in_val_set)\n",
    "end = time.time()\n",
    "print(\"Total time: \", end - start)\n",
    "# generate_dset(\"train\", in_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scrape -P2\n",
    "start = time.time()\n",
    "generate_dset(\"validation\", in_val_set)\n",
    "end = time.time()\n",
    "print(\"Total time: \", end - start)\n",
    "# generate_dset(\"train\", in_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "generate_dset_old(\"validation\", in_val_set)\n",
    "end = time.time()\n",
    "print(\"Total time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset\n",
    "\n",
    "https://stackoverflow.com/questions/328356/extracting-text-from-html-file-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url1 = \"http://www.reuters.com/article/companyNewsAndPR/idUSTP13157220070102\"\n",
    "url2 = \"http://www.reuters.com/article/2011/09/09/indian-stocks-open-idUSI8E7J302G20110909\"\n",
    "url3 = \"http://www.reuters.com/article/companyNewsAndPR/idUSL1136380220070111\"\n",
    "url4 = \"http://www.reuters.com/article/india-cenbank-refinance-idUSB8N12101C20151015\"\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "start_phrase = \"years ago\"\n",
    "end_phrase = \"Our Standards:The Thomson Reuters Trust Principles.\"\n",
    "\n",
    "with urllib.request.urlopen(url3) as webpage:\n",
    "    content = BeautifulSoup(webpage.read(), \"html.parser\")\n",
    "    for script in content([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    print(content)\n",
    "#     text = content.get_text()\n",
    "#     start_index = text.index(start_phrase) + len(start_phrase)\n",
    "#     end_index = text.index(end_phrase)\n",
    "#     text = text[start_index: end_index]\n",
    "#     lines = (line.strip() for line in text.splitlines() if not line.strip() == \"\")\n",
    "#     for i, line in enumerate(lines):\n",
    "# #         print(\"Line \" + str(i) + \": \" + line)\n",
    "#         tokens = tokenizer.tokenize(line)\n",
    "#         for token in tokens:\n",
    "#             print(\"\\n\")\n",
    "#             print(token)\n",
    "# #         print(tokens)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
