{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text, TextCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main idea behind generating stories and simplified stories is that you can do preprocessing on\n",
    "# simplified_stories to help you choose sentences, but still include the original sentences in the\n",
    "# summary (if you want).\n",
    "\n",
    "\n",
    "# Read in data from target, breaking each story into sentences\n",
    "def load_data_as_sentences(file, stem=False, remove_stop_words=False, lowercase=False):\n",
    "    stories = []\n",
    "    simplified_stories = []\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Load stories from file\n",
    "    with open(file) as f:\n",
    "        stories_raw = f.readlines()[:10] #TODO: After testing, remove [:10]\n",
    "        \n",
    "    for story in stories_raw:\n",
    "        # Split story into a list of sentences\n",
    "        story = re.sub(\"< newLine >\", \"\\n\", story)\n",
    "        sentences = nltk.sent_tokenize(story)\n",
    "        stories.append(sentences)\n",
    "        \n",
    "        # Extra preprocessing\n",
    "        story_tokenized = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "        if remove_stop_words:\n",
    "            story_tokenized = [[word for word in sentence if word not in stop_words] for sentence in story_tokenized]\n",
    "        if stem:\n",
    "            story_tokenized = [[stemmer.stem(word) for word in sentence] for sentence in story_tokenized]\n",
    "        if lowercase:\n",
    "            story_tokenized = [[word.lower() for word in sentence] for sentence in story_tokenized]\n",
    "        \n",
    "        simplified_stories.append(story_tokenized)\n",
    "    return stories, simplified_stories\n",
    "\n",
    "# Read in data from target, breaking each story into paragraphs (and then sentences)\n",
    "def load_data_as_paragraphs(file, stem=False, remove_stop_words=False, lowercase=False):\n",
    "    simplified_stories = []\n",
    "    stories = []\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Load stories from file\n",
    "    with open(file) as f:\n",
    "        stories_raw = f.readlines()[:10] #TODO: After testing, remove [:10]\n",
    "    for story in stories_raw:\n",
    "        # Split into a list of paragraphs\n",
    "        paragraphs = story.split(\"< newLine > < newLine >\")\n",
    "        simplified_paragraphs = []\n",
    "        untokenized_paragraphs = []\n",
    "        for paragraph in paragraphs:\n",
    "            # SPlit paragraph into a list of sentences\n",
    "            paragraph = re.sub(\"< newLine >\", \"\\n\", paragraph)\n",
    "            sentences = nltk.sent_tokenize(paragraph)\n",
    "            untokenized_paragraphs.append(sentences)\n",
    "            # For the tokenized version, split each sentence int oa list of words\n",
    "            paragraph_tokenized = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "            # Extra preprocessing\n",
    "            if remove_stop_words:\n",
    "                paragraph_tokenized = [[word for word in sentence if word not in stop_words] for sentence in paragraph_tokenized]\n",
    "            if stem:\n",
    "                paragraph_tokenized = [[stemmer.stem(word) for word in sentence] for sentence in paragraph_tokenized]\n",
    "            if lowercase:\n",
    "                paragraph_tokenized = [[word.lower() for word in sentence] for sentence in paragraph_tokenized]\n",
    "                \n",
    "            simplified_paragraphs.append(paragraph_tokenized)\n",
    "        stories.append(untokenized_paragraphs)\n",
    "        simplified_stories.append(simplified_paragraphs)\n",
    "    return stories, simplified_stories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SumBasic algorithm\n",
    "\n",
    "\n",
    "# Compute word probablities\n",
    "def get_probs(data, tfidf=False, text=None):\n",
    "    probs = {}\n",
    "    if not tfidf:\n",
    "        # Loop through each word\n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                # Build a dictionary of word frequency counts\n",
    "                if word in probs:\n",
    "                    probs[word] += 1\n",
    "                else:\n",
    "                    probs[word] = 1\n",
    "        N = sum([len(sentence) for sentence in data]) * 1.0\n",
    "        \n",
    "        # Each word's score is word_count/total_word_count\n",
    "        for key in probs.keys():\n",
    "            probs[key] /= N\n",
    "    else:\n",
    "        # Otherwise each word's score is its tf_idf\n",
    "        # NOTE: tf_idf was calculated over all stories, not just this one.\n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                probs[word] = all_tokens.tf_idf(word, text)\n",
    "    return probs          \n",
    "\n",
    "# Pick the best scoring sentence that contains the highest probability word.\n",
    "def get_best_sentence(data, probs):\n",
    "    highest_prob_word = max(probs, key=probs.get) \n",
    "    \n",
    "    # Comment this out later, but for testing it's really useful to see what\n",
    "    # words are being marked as most important\n",
    "    print(highest_prob_word)\n",
    "    best_sentence_index = -1\n",
    "    best_score = 0.0\n",
    "    for index, sentence in enumerate(data):\n",
    "        if highest_prob_word in sentence:\n",
    "            score = sum([probs[word] for word in sentence])/len(sentence)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_sentence_index = index\n",
    "    return best_sentence_index\n",
    "\n",
    "# Square the probablity of each word in the chosen sentence\n",
    "def update_probs(probs, sentence):\n",
    "    for word in set(sentence):\n",
    "        probs[word] = probs[word] ** 2\n",
    "    return probs\n",
    "\n",
    "\n",
    "# Count sentences in text\n",
    "def get_length(text):\n",
    "    return len(text)\n",
    "\n",
    "# Get tf_idf across all stories\n",
    "def get_tfidf(data):\n",
    "    text_list = [Text([word for sentence in story for word in sentence]) for story in data]\n",
    "    story_tokens = TextCollection(text_list)\n",
    "    return text_list, story_tokens\n",
    "\n",
    "# Get tf_idf across all stories\n",
    "def get_tfidf_paragraphs(data):\n",
    "    text_list = [Text([word for paragraph in story for sentence in paragraph for word in sentence]) for story in data]\n",
    "    story_tokens = TextCollection(text_list)\n",
    "    return text_list, story_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually run everything\n",
    "stem = True\n",
    "remove_stop_words = True\n",
    "lowercase = True\n",
    "\n",
    "# stories is a triply nested lists (first broken by story, then by paragraph, then by sentences)\n",
    "# simplified_stories is a quadruply nested list (broken by story, paragraph, sentence, word)\n",
    "stories, simplified_stories = load_data_as_paragraphs(\"writingPromptsData/train.wp_target\", stem, remove_stop_words, lowercase)\n",
    "\n",
    "#TODO: If necessary, introduce other cleaning things:\n",
    "# - Clean up quotes in tetokenized story (or better yet, never tokenize at all)\n",
    "# - Deal with parens unmatched\n",
    "# - remove punctuation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summaries = []\n",
    "detokenizer = MosesDetokenizer()\n",
    "tfidf = True\n",
    "\n",
    "texts, all_tokens = get_tfidf_paragraphs(simplified_stories)\n",
    "\n",
    "# Loop through stories (assumes each story is a list of paragraphs, each of which are lists of sentences)\n",
    "for story, simplified_story, text in zip(stories, simplified_stories, texts):\n",
    "    summary = []\n",
    "    # Loop through paragraphs, adding one sentence per paragraph to the summary.\n",
    "    for index, paragraph, simplified_paragraph in zip(range(len(story)), story, simplified_story):\n",
    "        # Get word probabilities\n",
    "        probs = get_probs(simplified_paragraph, tfidf, text) \n",
    "        # Choose sentence with best score\n",
    "        next_sentence_index = get_best_sentence(simplified_paragraph, probs)\n",
    "        # Add it to summary\n",
    "        summary.append(paragraph[next_sentence_index])\n",
    "        # Update probabilities to downweight the words we just used.\n",
    "        probs = update_probs(probs, simplified_paragraph[next_sentence_index])\n",
    "    # Join sentences into a summary\n",
    "    summary_string = \" \".join(summary)\n",
    "    print(summary_string)\n",
    "    print(\" ===== \")\n",
    "    summaries.append(summary_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 5\n",
    "\n",
    "texts, all_tokens = get_tfidf(simplified_stories)\n",
    "tfidf = False\n",
    "\n",
    "# Note - this currently chooses sentences regardless of their order in the original text.  We could save sentence\n",
    "# indexes instead, then use those to chop out the summary sentences in order.\n",
    "\n",
    "# Original sumBasic (assumes stories are lists of sentences, not lists of paragraphs of sentences)\n",
    "# to use this one, run the load_data_as_sentences function\n",
    "for index, story, simplified_story, text in zip(range(len(stories)), stories, simplified_stories, texts):\n",
    "    summary = []\n",
    "    # Get word probabilities\n",
    "    probs = get_probs(simplified_story, tfidf, text)\n",
    "    # Keep adding sentences until we have enough\n",
    "    while get_length(summary) <  max_length:\n",
    "        # Choose sentence with best score\n",
    "        next_sentence_index = get_best_sentence(simplified_story, probs)\n",
    "        # Add it to summary\n",
    "        summary.append(story[next_sentence_index])\n",
    "        # Update probablities to downweight the words we just used\n",
    "        probs = update_probs(probs, simplified_story[next_sentence_index])\n",
    "    # Join sentences into a summary\n",
    "    summary_string = \" \".join(summary)\n",
    "    print(summary_string)\n",
    "    print(\" ======= \")\n",
    "    summaries.append(summary_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
