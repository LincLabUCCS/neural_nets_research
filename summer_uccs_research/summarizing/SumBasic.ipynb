{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text, TextCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main idea behind generating stories and simplified stories is that you can do preprocessing on\n",
    "# simplified_stories to help you choose sentences, but still include the original sentences in the\n",
    "# summary (if you want).\n",
    "\n",
    "# Read in data from target, breaking each story into paragraphs (and then sentences)\n",
    "def load_data_as_paragraphs(file, stem=False, remove_stop_words=False, lowercase=False, metaparagraph_size=1):\n",
    "    simplified_stories = []\n",
    "    stories = []\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Load stories from file\n",
    "    with open(file) as f:\n",
    "        stories_raw = f.readlines()[:10] #TODO: After testing, remove [:10]\n",
    "    for story in stories_raw:\n",
    "        \n",
    "        # Split into a list of paragraphs\n",
    "        paragraphs = story.split(\"<newline>\")\n",
    "        simplified_paragraphs = []\n",
    "        untokenized_paragraphs = []\n",
    "        par_index = 0\n",
    "        \n",
    "        # Loop through paragraphs\n",
    "        while par_index < len(paragraphs):\n",
    "            meta_paragraph = []\n",
    "            \n",
    "            # Combine \n",
    "            while par_index < len(paragraphs) and len(meta_paragraph) < metaparagraph_size:\n",
    "                paragraph = paragraphs[par_index]\n",
    "                \n",
    "                # Split paragraph into a list of sentences\n",
    "                sentences = nltk.sent_tokenize(paragraph)\n",
    "                meta_paragraph += sentences\n",
    "                par_index += 1\n",
    "            \n",
    "            untokenized_paragraphs.append(meta_paragraph)\n",
    "            # For the tokenized version, split each sentence into a list of words\n",
    "            paragraph_tokenized = [nltk.word_tokenize(sentence) for sentence in meta_paragraph]\n",
    "            # Extra preprocessing\n",
    "            if remove_stop_words:\n",
    "                paragraph_tokenized = [[word for word in sentence if word not in stop_words] for sentence in paragraph_tokenized]\n",
    "            if stem:\n",
    "                paragraph_tokenized = [[stemmer.stem(word) for word in sentence] for sentence in paragraph_tokenized]\n",
    "            if lowercase:\n",
    "                paragraph_tokenized = [[word.lower() for word in sentence] for sentence in paragraph_tokenized]\n",
    "                \n",
    "            simplified_paragraphs.append(paragraph_tokenized)\n",
    "        stories.append(untokenized_paragraphs)\n",
    "        simplified_stories.append(simplified_paragraphs)\n",
    "    return stories, simplified_stories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SumBasic algorithm\n",
    "\n",
    "\n",
    "# Compute word probablities\n",
    "def get_probs(data, tfidf=False, text=None):\n",
    "    probs = {}\n",
    "    if not tfidf:\n",
    "        # Loop through each word\n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                # Build a dictionary of word frequency counts\n",
    "                if word in probs:\n",
    "                    probs[word] += 1\n",
    "                else:\n",
    "                    probs[word] = 1\n",
    "        N = sum([len(sentence) for sentence in data]) * 1.0\n",
    "        \n",
    "        # Each word's score is word_count/total_word_count\n",
    "        for key in probs.keys():\n",
    "            probs[key] /= N\n",
    "    else:\n",
    "        # Otherwise each word's score is its tf_idf\n",
    "        # NOTE: tf_idf was calculated over all stories, not just this one.\n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                probs[word] = all_tokens.tf_idf(word, text)\n",
    "    return probs \n",
    "\n",
    "# Pick the best scoring sentence that contains the highest probability word.\n",
    "def get_best_sentence(data, probs, include_best=True):\n",
    "    highest_prob_word = max(probs, key=probs.get) \n",
    "    \n",
    "    # Comment this out later, but for testing it's really useful to see what\n",
    "    # words are being marked as most important\n",
    "    print(highest_prob_word)\n",
    "    best_sentence_index = -1\n",
    "    best_score = 0.0\n",
    "    for index, sentence in enumerate(data):\n",
    "        if (highest_prob_word in sentence) or not include_best:\n",
    "            score = sum([probs[word] for word in sentence])/len(sentence)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_sentence_index = index  \n",
    "    return best_sentence_index\n",
    "\n",
    "\n",
    "# Square the probablity of each word in the chosen sentence\n",
    "def update_probs(probs, sentence):\n",
    "    for word in set(sentence):\n",
    "        probs[word] = probs[word] ** 2\n",
    "    return probs\n",
    "\n",
    "\n",
    "# Count sentences in text\n",
    "def get_length(text):\n",
    "    return len(text)\n",
    "\n",
    "# Get tf_idf across all stories\n",
    "def get_tfidf(data):\n",
    "    text_list = [Text([word for sentence in story for word in sentence]) for story in data]\n",
    "    story_tokens = TextCollection(text_list)\n",
    "    return text_list, story_tokens\n",
    "\n",
    "# Get tf_idf across all stories\n",
    "def get_tfidf(data, by_paragraph=False):\n",
    "    # If get by paragraph, each text refers to 1 paragraph\n",
    "    if by_paragraph:\n",
    "        text_list = [Text([word for sentence in paragraph for word in sentence]) for story in data for paragraph in story]\n",
    "    # Otherwise each text is 1 story\n",
    "    else:\n",
    "        text_list = [Text([word for paragraph in story for sentence in paragraph for word in sentence]) for story in data]\n",
    "    story_tokens = TextCollection(text_list)\n",
    "    return text_list, story_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually run everything\n",
    "stem = True\n",
    "remove_stop_words = True\n",
    "lowercase = True\n",
    "metaparagraph_size = 5\n",
    "\n",
    "# stories is a triply nested lists (first broken by story, then by paragraph, then by sentences)\n",
    "# simplified_stories is a quadruply nested list (broken by story, paragraph, sentence, word)\n",
    "stories, simplified_stories = load_data_as_paragraphs(\"writingPromptsData/examples.wp_target\", stem, remove_stop_words, lowercase, metaparagraph_size)\n",
    "\n",
    "#TODO: If necessary, introduce other cleaning things:\n",
    "# - Clean up quotes\n",
    "# - Deal with parens unmatched\n",
    "# - remove punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summaries = []\n",
    "detokenizer = MosesDetokenizer()\n",
    "tfidf = True\n",
    "by_paragraph = True\n",
    "include_best_word = False\n",
    "\n",
    "texts, all_tokens = get_tfidf(simplified_stories, by_paragraph)\n",
    "\n",
    "# Loop through stories (assumes each story is a list of paragraphs, each of which are lists of sentences)\n",
    "for story, simplified_story, text in zip(stories, simplified_stories, texts):\n",
    "    summary = []\n",
    "    # Loop through paragraphs, adding one sentence per paragraph to the summary.\n",
    "    for index, paragraph, simplified_paragraph in zip(range(len(story)), story, simplified_story):\n",
    "        # Get word probabilities\n",
    "        probs = get_probs(simplified_paragraph, tfidf, text) \n",
    "        # Choose sentence with best score\n",
    "        next_sentence_index = get_best_sentence(simplified_paragraph, probs, include_best=include_best_word)\n",
    "        # Add it to summary\n",
    "        summary.append(paragraph[next_sentence_index])\n",
    "    # Join sentences into a summary\n",
    "    summary_string = \"<newline>\".join(summary)\n",
    "    print(summary_string)\n",
    "    print(\" ===== \")\n",
    "    summaries.append(summary_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
