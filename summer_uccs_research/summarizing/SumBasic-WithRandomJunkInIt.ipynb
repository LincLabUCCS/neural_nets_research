{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import regex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main idea behind generating stories and simplified stories is that you can do preprocessing on\n",
    "# simplified_stories to help you choose sentences, but still include the original sentences in the\n",
    "# summary (if you want).\n",
    "\n",
    "# Read in data from target, breaking each story into paragraphs (and then sentences)\n",
    "def load_data_as_paragraphs(file, stem=True, remove_stop_words=True, \n",
    "                            remove_punctuation=True, metaparagraph_size=5):\n",
    "    simplified_stories = []\n",
    "    stories = []\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    # Load stories from file\n",
    "    with open(file) as f:\n",
    "        stories_raw = f.readlines()[0:2000]\n",
    "        \n",
    "    for story in stories_raw:\n",
    "        # Split into a list of paragraphs\n",
    "        paragraphs = story.split(\"<newline>\")\n",
    "        simplified_paragraphs = []\n",
    "        untokenized_paragraphs = []\n",
    "        par_index = 0\n",
    "        \n",
    "        # Loop through paragraphs\n",
    "        while par_index < len(paragraphs):\n",
    "            meta_paragraph = []\n",
    "            \n",
    "            # Combine small paragraphs into meta_paragraphs with at least some minimum number of sentences\n",
    "            while par_index < len(paragraphs) and len(meta_paragraph) < metaparagraph_size:\n",
    "                paragraph = paragraphs[par_index]\n",
    "                \n",
    "                # Split paragraph into a list of sentences\n",
    "                sentences = nltk.sent_tokenize(paragraph)\n",
    "                meta_paragraph += sentences\n",
    "                par_index += 1\n",
    "            \n",
    "\n",
    "            \n",
    "            meta_paragraph_unprocessed = meta_paragraph\n",
    "            \n",
    "            if remove_stop_words:\n",
    "                meta_paragraph = [sentence.replace(\"<num>\",\" \") for sentence in meta_paragraph]\n",
    "            \n",
    "            # For the tokenized version, split each sentence into a list of words\n",
    "            paragraph_tokenized = [nltk.word_tokenize(sentence) for sentence in meta_paragraph]\n",
    "            # Extra preprocessing\n",
    "            if remove_stop_words:\n",
    "                paragraph_tokenized = [[word for word in sentence if word not in stop_words] for sentence in paragraph_tokenized]\n",
    "            if remove_punctuation:\n",
    "                paragraph_tokenized = [[regex.sub('[\\p{P}\\p{Sm}`]+', '', word) for word in sentence] for sentence in paragraph_tokenized]\n",
    "                paragraph_tokenized = [[word for word in sentence if word != \"\"] for sentence in paragraph_tokenized]\n",
    "            if stem:\n",
    "                paragraph_tokenized = [[stemmer.stem(word) for word in sentence] for sentence in paragraph_tokenized]\n",
    "\n",
    "            if len(meta_paragraph) < metaparagraph_size and len(untokenized_paragraphs) > 0:\n",
    "                untokenized_paragraphs[-1] += meta_paragraph_unprocessed\n",
    "                simplified_paragraphs[-1] += paragraph_tokenized\n",
    "            else:\n",
    "                untokenized_paragraphs.append(meta_paragraph_unprocessed)\n",
    "                simplified_paragraphs.append(paragraph_tokenized)\n",
    "                \n",
    "        stories.append(untokenized_paragraphs)\n",
    "        simplified_stories.append(simplified_paragraphs)\n",
    "    return stories, simplified_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# SumBasic algorithm\n",
    "\n",
    "# Pick the best scoring sentence that optionally contains the highest probability word.\n",
    "def get_best_sentence(data, document_scores, document_index, vocab, inverse_vocab, sentence_vectorizer, include_best=True):\n",
    "    \n",
    "    # Concatenate tokens into strings\n",
    "    strings = [\" \".join(sentence) for sentence in data]\n",
    "    \n",
    "    # Create a bag-of-words-style sentence vector\n",
    "    vector_sentences = sentence_vectorizer.transform(strings)\n",
    "    \n",
    "    # Dot the sentence vector with the document tf_idf vector\n",
    "    curr_doc_scores = document_scores[document_index].transpose()\n",
    "    scores = vector_sentences * curr_doc_scores\n",
    "    \n",
    "    # Divide each sentence's score by its length\n",
    "    lengths = 1.0 / vector_sentences.sum(axis=1)\n",
    "    scores = scores.multiply(lengths)\n",
    "    \n",
    "    # If we have to include the best word, mask invalid sentences\n",
    "    # If we aren't including best, might as well leave this commmented out.\n",
    "#     if include_best:\n",
    "#         highest_score_word = inverse_vocab[document_scores[document_index].argmax()]\n",
    "#         highest_score_mask = [1 if highest_score_word in sentence else 0 for sentence in data]\n",
    "#         highest_sparse = csr_matrix(highest_score_mask).transpose()\n",
    "#         scores = scores.multiply(highest_sparse)\n",
    "    \n",
    "    if scores.count_nonzero() == 0:\n",
    "        return 0\n",
    "        \n",
    "    # Return the index of the best-scoring sentence\n",
    "    best = scores.argmax(axis=0)     \n",
    "    return best[0,0]\n",
    "\n",
    "def get_best_sentence2(data, document_scores, document_index, vocab, inverse_vocab, whatever, include_best=True):\n",
    "    highest_score_word = inverse_vocab[document_scores[document_index].argmax()]\n",
    "    \n",
    "    best_sentence_index = 0\n",
    "    best_score = -1\n",
    "    \n",
    "    for index, sentence in enumerate(data):\n",
    "        if not include_best or (highest_score_word in sentence):            \n",
    "            if len(sentence) == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = sum([document_scores[document_index, vocab[word]] for word in sentence])/len(sentence)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_sentence_index = index \n",
    "                \n",
    "    return best_sentence_index\n",
    "\n",
    "# Square the score of each word in the chosen sentence. Not currently used, but could be in the future.\n",
    "def update_probs(document_scores, vocab, sentence):    \n",
    "    for word in set(sentence):\n",
    "        document_scores[vocab[word]] **= 2\n",
    "    return document_scores\n",
    "\n",
    "def construct_text_collection(simplified_stories, by_paragraph=False):\n",
    "    # If get by paragraph, each element refers to 1 paragraph\n",
    "    if by_paragraph:\n",
    "        texts = [[word for sentence in paragraph for word in sentence] for story in simplified_stories for paragraph in story]\n",
    "    # Otherwise each element is 1 story\n",
    "    else:\n",
    "        texts = [[word for paragraph in story for sentence in paragraph for word in sentence] for story in simplified_stories]\n",
    "    \n",
    "    return texts\n",
    "\n",
    "def compute_all_probs(texts):\n",
    "    tfidf = TfidfVectorizer(analyzer='word', tokenizer=lambda x: x,\n",
    "                            preprocessor=lambda x: x,\n",
    "                            norm='l1', use_idf=False, token_pattern=r\"(?u)\\b[^\\s]+\\b\")\n",
    "    scores = tfidf.fit_transform(texts)\n",
    "    return tfidf, scores\n",
    "    \n",
    "\n",
    "def compute_all_tfidfs(texts):\n",
    "    probs = TfidfVectorizer(analyzer='word', tokenizer=lambda x: x,\n",
    "                            preprocessor=lambda x: x, \n",
    "                            token_pattern=r\"(?u)\\b[^\\s]+\\b\")\n",
    "    scores = probs.fit_transform(texts)\n",
    "    return probs, scores\n",
    "    \n",
    "def compute_all_scores(texts, tfidf=True):\n",
    "    if tfidf:\n",
    "        return compute_all_tfidfs(texts)\n",
    "    else:\n",
    "        return compute_all_probs(texts)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually run everything\n",
    "stem = True\n",
    "remove_stop_words = True\n",
    "remove_punctuation = True\n",
    "metaparagraph_size = 5\n",
    "\n",
    "# stories is a triply nested lists (first broken by story, then by paragraph, then by sentences)\n",
    "# simplified_stories is a quadruply nested list (broken by story, paragraph, sentence, word)\n",
    "stories, simplified_stories = load_data_as_paragraphs(\"../datasets/writing_prompts/valid.wp_target\", stem, remove_stop_words, \n",
    "                                                      remove_punctuation, metaparagraph_size)\n",
    "\n",
    "#TODO: If necessary, introduce other cleaning things:\n",
    "# - Deal with parens unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlines():\n",
    "    summaries = []\n",
    "    tfidf = True\n",
    "    by_paragraph = True\n",
    "    include_best_word = False\n",
    "\n",
    "    texts = construct_text_collection(simplified_stories, by_paragraph=by_paragraph)\n",
    "    vectorizer, scores = compute_all_scores(texts, tfidf=tfidf)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    sentence_vectorizer = CountVectorizer(input='content', vocabulary=feature_names, token_pattern=r\"(?u)\\b[^\\s]+\\b\") #r\"(?u)\\b[^\\s]+\\b\"  #r\"(?u)\\b\\w+\\b\"\n",
    "    \n",
    "    paragraph_index = 0\n",
    "\n",
    "    # Loop through stories (assumes each story is a list of paragraphs, each of which are lists of sentences)\n",
    "    for story_index, (story, simplified_story) in enumerate(zip(stories, simplified_stories)):\n",
    "        summary = []\n",
    "\n",
    "        # Loop through paragraphs, adding one sentence per paragraph to the summary.\n",
    "        for paragraph, simplified_paragraph in zip(story, simplified_story):\n",
    "            # indexing is done in a bit of a stupid way because csr matrices don't support indexing like\n",
    "            # A[x][y] and instead require A[x,y].\n",
    "            document_index = paragraph_index if by_paragraph else story_index\n",
    "\n",
    "            # Choose sentence with best score\n",
    "#             next_sentence_index = get_best_sentence(simplified_paragraph, scores, document_index, vectorizer.vocabulary_, feature_names, sentence_vectorizer, include_best=include_best_word)\n",
    "            next_sentence_index = get_best_sentence2(simplified_paragraph, scores, document_index, vectorizer.vocabulary_, feature_names, sentence_vectorizer, include_best=include_best_word)\n",
    "            # Add it to summary\n",
    "            summary.append(paragraph[next_sentence_index])\n",
    "            paragraph_index += 1\n",
    "        # Join sentences into a summary\n",
    "        summary_string = \" <newline> \".join(summary)\n",
    "        summaries.append(summary_string)\n",
    "        \n",
    "    with open('summaries_old.txt', 'w') as f:\n",
    "        for summary in summaries:\n",
    "            f.write(summary + \"\\n\")\n",
    "        \n",
    "#     print(\"done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with include_Best\n",
    "# orig: 67 ms\n",
    "# new: 161 ms\n",
    "\n",
    "# WITHOUT include_best\n",
    "# orig: 127 ms\n",
    "# new: 92 ms\n",
    "\n",
    "# Original (before any improvements):  229ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13.531407639966346\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "outlines()\n",
    "end = timer()\n",
    "print(\"total\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.43 s ± 1.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit outlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "lp = LineProfiler()\n",
    "lp.add_function(get_best_sentence)\n",
    "lp_wrapper = lp(outlines)\n",
    "lp_wrapper()\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
