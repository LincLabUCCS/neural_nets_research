{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text, TextCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in data, source and target\n",
    "\n",
    "# def load_data(file_source, file_target):\n",
    "#     source_stories = []\n",
    "#     target_stories = []\n",
    "\n",
    "#     with open(file_source) as f:\n",
    "#         source_raw = f.readlines()\n",
    "#     with open(file_target) as f:\n",
    "#         target_raw = f.readlines()\n",
    "\n",
    "#     for source, target in zip(source_raw, target_raw):\n",
    "#         # If the story starts with [ CONTEST ] remove it all together\n",
    "#         if source[:11] == \"[ CONTEST ]\":\n",
    "#             print(source)\n",
    "#             print(target)\n",
    "#             pass\n",
    "\n",
    "#         # Remove prefix [ XX ] and ( XX ) from the phrase (this is typically at the beginning or end of the string)\n",
    "#         source = re.sub(\"(\\[|\\() [A-Za-z][A-Za-z] (\\]|\\))\", '', source)\n",
    "#         source_stories.append(nltk.word_tokenize(source))\n",
    "#         target_stories.append(nltk.word_tokenize(target))\n",
    "#     print(len(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in data from source\n",
    "\n",
    "# def load_data(file_source):\n",
    "#     stories = []\n",
    "\n",
    "#     with open(file_source) as f:\n",
    "#         stories_raw = f.readlines()\n",
    "\n",
    "#     for story in stories_raw:\n",
    "#         # If the story starts with [ CONTEST ] remove it all together\n",
    "#         if story[:11] == \"[ CONTEST ]\":\n",
    "#             pass\n",
    "\n",
    "#         # Remove prefix [ XX ] and ( XX ) from the phrase (this is typically at the beginning or end of the string)\n",
    "#         story = re.sub(\"(\\[|\\() [A-Za-z][A-Za-z] (\\]|\\))\", '', story)\n",
    "#         stories.append(nltk.word_tokenize(story))\n",
    "#     return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in data from target\n",
    "\n",
    "# def load_data(file, stem=False, remove_stop_words=False):\n",
    "#     stories = []\n",
    "#     stemmer = PorterStemmer()\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "#     with open(file) as f:\n",
    "#         stories_raw = f.readlines()[:10]\n",
    "#     for story in stories_raw:\n",
    "#         story = re.sub(\"< newLine >\", \"\\n\", story)\n",
    "#         sentences = nltk.sent_tokenize(story)\n",
    "#         story_tokenized = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "#         if remove_stop_words:\n",
    "#             story_tokenized = [[word for word in sentence if word not in stop_words] for sentence in story_tokenized]\n",
    "#         if stem:\n",
    "#             story_tokenized = [[stemmer.stem(word) for word in sentence] for sentence in story_tokenized]\n",
    "        \n",
    "#         stories.append(story_tokenized)\n",
    "#     return stories\n",
    "\n",
    "# Read in data from target\n",
    "\n",
    "def load_data(file, stem=False, remove_stop_words=False):\n",
    "    simplified_stories = []\n",
    "    stories = []\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    with open(file) as f:\n",
    "        stories_raw = f.readlines()[:10]\n",
    "    for story in stories_raw:\n",
    "        paragraphs = story.split(\"< newLine > < newLine >\")\n",
    "        simplified_paragraphs = []\n",
    "        untokenized_paragraphs = []\n",
    "        for paragraph in paragraphs:\n",
    "            paragraph = re.sub(\"< newLine >\", \"\\n\", paragraph)\n",
    "            sentences = nltk.sent_tokenize(paragraph)\n",
    "            untokenized_paragraphs.append(sentences)\n",
    "            paragraph_tokenized = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "            if remove_stop_words:\n",
    "                paragraph_tokenized = [[word for word in sentence if word not in stop_words] for sentence in paragraph_tokenized]\n",
    "            if stem:\n",
    "                paragraph_tokenized = [[stemmer.stem(word) for word in sentence] for sentence in paragraph_tokenized]\n",
    "\n",
    "            simplified_paragraphs.append(paragraph_tokenized)\n",
    "        stories.append(untokenized_paragraphs)\n",
    "        simplified_stories.append(simplified_paragraphs)\n",
    "    return stories, simplified_stories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "s = load_data(\"writingPromptsData/train.wp_target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SumBasic algorithm\n",
    "def get_probs(data, tfidf=False, text=None):\n",
    "    probs = {}\n",
    "    if not tfidf:\n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                if word in probs:\n",
    "                    probs[word] += 1\n",
    "                else:\n",
    "                    probs[word] = 1\n",
    "        N = sum([len(sentence) for sentence in data]) * 1.0\n",
    "        for key in probs.keys():\n",
    "            probs[key] /= N\n",
    "    else:\n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                probs[word] = all_tokens.tf_idf(word, text)\n",
    "    return probs\n",
    "            \n",
    "\n",
    "def get_best_sentence(data, probs):\n",
    "    best_sentence_index = -1\n",
    "    best_score = 0.0\n",
    "    for index, sentence in enumerate(data):\n",
    "        score = sum([probs[word] for word in sentence])/len(sentence)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_sentence_index = index\n",
    "    return best_sentence_index\n",
    "\n",
    "\n",
    "def get_best_word():\n",
    "    pass\n",
    "\n",
    "def update_probs(probs, sentence):\n",
    "    for word in set(sentence):\n",
    "        probs[word] = probs[word] ** 2\n",
    "    return probs\n",
    "\n",
    "# Count words in text\n",
    "# def get_length(text):\n",
    "#     return sum([len(sentence) for sentence in text])\n",
    "\n",
    "# Count sentences in text\n",
    "def get_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def get_tfidf(data):\n",
    "    text_list = [Text([word for sentence in story for word in sentence]) for story in data]\n",
    "    story_tokens = TextCollection(text_list)\n",
    "    return text_list, story_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually run everything\n",
    "stem = True\n",
    "remove_stop_words = True\n",
    "stories, simplified_stories = load_data(\"writingPromptsData/train.wp_target\", stem, remove_stop_words)\n",
    "\n",
    "#TODO: If necessary, introduce other cleaning things:\n",
    "# - Clean up quotes in tetokenized story (or better yet, never tokenize at all)\n",
    "# - Deal with parens unmatched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stories[0])\n",
    "print(simplified_stories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "detokenizer = MosesDetokenizer()\n",
    "tfidf = True\n",
    "max_length = 1\n",
    "\n",
    "texts, all_tokens = get_tfidf(simplified_stories)\n",
    "    \n",
    "\n",
    "\n",
    "for story, simplified_story, text in zip(stories, simplified_stories, texts):\n",
    "    print(\"NEW STORY\", len(story))\n",
    "    summary = []\n",
    "    for index, paragraph, simplified_paragraph in zip(range(len(story)), story, simplified_story):\n",
    "        probs = get_probs(simplified_paragraph, tfidf, text)     \n",
    "        next_sentence_index = get_best_sentence(simplified_paragraph, probs)\n",
    "        summary.append(paragraph[next_sentence_index])\n",
    "        probs = update_probs(probs, simplified_paragraph[next_sentence_index])\n",
    "    summary_string = \" \".join(summary)\n",
    "    print(summary_string)\n",
    "    print(\" \")\n",
    "    summaries.append(summary_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original sumBasic (assumes stories are lists of sentences, not lists of paragraphs of sentences)\n",
    "for index, story, simplified_story, text in zip(range(len(stories)), stories, simplified_stories, texts):\n",
    "    summary = []\n",
    "    probs = get_probs(simplified_story, tfidf, text)\n",
    "    while get_length(summary) <  max_length:\n",
    "        next_sentence_index = get_best_sentence(simplified_story, probs)\n",
    "        summary.append(story[next_sentence_index])\n",
    "        probs = update_probs(probs, simplified_story[next_sentence_index])\n",
    "#     summary_string = \" \".join([detokenizer.detokenize(sentence, return_str=True) for sentence in summary])\n",
    "    print(summary)\n",
    "    summary_string = \" \".join(summary)\n",
    "    print(summary_string)\n",
    "    print(\" \")\n",
    "    summaries.append(summary_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
